{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Vectors-Input (InputLayer)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Vectors-Input-Reshaped (Resh (None, 64, 8, 1)          0         \n",
      "_________________________________________________________________\n",
      "Deconvolution1 (Conv2DTransp (None, 73, 9, 64)         1344      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 73, 9, 64)         256       \n",
      "_________________________________________________________________\n",
      "Deconvolution2 (Conv2DTransp (None, 84, 11, 64)        147520    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 84, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution3 (Conv2DTransp (None, 95, 14, 60)        184380    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 95, 14, 60)        240       \n",
      "_________________________________________________________________\n",
      "Deconvolution4 (Conv2DTransp (None, 106, 17, 60)       172860    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 106, 17, 60)       240       \n",
      "_________________________________________________________________\n",
      "Deconvolution5 (Conv2DTransp (None, 117, 21, 55)       198055    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 117, 21, 55)       220       \n",
      "_________________________________________________________________\n",
      "Deconvolution6 (Conv2DTransp (None, 128, 25, 55)       181555    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128, 25, 55)       220       \n",
      "_________________________________________________________________\n",
      "Deconvolution7 (Conv2DTransp (None, 256, 50, 32)       63392     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256, 50, 32)       128       \n",
      "_________________________________________________________________\n",
      "Deconvolution8 (Conv2DTransp (None, 512, 100, 1)       2049      \n",
      "_________________________________________________________________\n",
      "Output-Reshaped (Reshape)    (None, 512, 100)          0         \n",
      "=================================================================\n",
      "Total params: 952,715\n",
      "Trainable params: 951,935\n",
      "Non-trainable params: 780\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Concatenate,Lambda,Reshape,Dropout, BatchNormalization, Conv2DTranspose, UpSampling2D\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vector = Input(shape=(512,), name='Vectors-Input',dtype='float32')\n",
    "vectorReshaped = Reshape((64,8,1),name='Vectors-Input-Reshaped')(vector)\n",
    "deconv1 = Conv2DTranspose(64, (10,2), strides=(1,1),name='Deconvolution1')(vectorReshaped)\n",
    "norm_deconv1=BatchNormalization()(deconv1)\n",
    "deconv2 = Conv2DTranspose(64, (12,3), strides=(1,1), name='Deconvolution2')(norm_deconv1)\n",
    "norm_deconv2=BatchNormalization()(deconv2)\n",
    "deconv3 = Conv2DTranspose(60, (12,4), strides=(1,1),name='Deconvolution3')(norm_deconv2)\n",
    "norm_deconv3=BatchNormalization()(deconv3)\n",
    "deconv4 = Conv2DTranspose(60, (12,4), strides=(1,1),name='Deconvolution4')(norm_deconv3)\n",
    "norm_deconv4=BatchNormalization()(deconv4)\n",
    "deconv5 = Conv2DTranspose(55, (12,5), strides=(1,1),name='Deconvolution5')(norm_deconv4)\n",
    "norm_deconv5=BatchNormalization()(deconv5)\n",
    "deconv6 = Conv2DTranspose(55, (12,5), strides=(1,1),name='Deconvolution6')(norm_deconv5)\n",
    "norm_deconv6=BatchNormalization()(deconv6)\n",
    "deconv7 = Conv2DTranspose(32, (6,6), strides=(2,2),padding='same',name='Deconvolution7')(norm_deconv6)\n",
    "norm_deconv7=BatchNormalization()(deconv7)\n",
    "deconv8 = Conv2DTranspose(1, (8,8), strides=(2,2),padding='same',name='Deconvolution8')(norm_deconv7)\n",
    "output = Reshape((512,100),name='Output-Reshaped')(deconv8)\n",
    "\n",
    "\n",
    "model = Model(inputs=vector,outputs=output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "from collections import Iterable\n",
    "import numpy as np\n",
    "\n",
    "bpemb_ru = BPEmb(lang=\"ru\", dim=100, vs=3000)\n",
    "\n",
    "def textreader(st,limit):\n",
    "    texts = []\n",
    "    with open(\"D:/112/decoder/data/data3000/np_rev2.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        i=0\n",
    "        for line in fr:\n",
    "            i+=1\n",
    "            if i<st:      \n",
    "                continue\n",
    "            texts.append(line.replace('\\n',''))\n",
    "            if i==st+limit-1:\n",
    "                break\n",
    "    return texts\n",
    "\n",
    "def word_vectors_creator(data):\n",
    "    l=len(data)\n",
    "    kol= np.zeros(shape=(l))\n",
    "    pieces=[]\n",
    "    for i in range(l):\n",
    "        s = bpemb_ru.encode_ids(data[i])\n",
    "        kol[i]+=len(s)\n",
    "        pieces.append(s)\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(l,512,100))\n",
    "    for g in range(l):\n",
    "        out[g][kol[g]] = np.ones(100)\n",
    "        for k in range(1,kol[g]-1):\n",
    "            out[g][k+1]+=bpemb_ru.emb.vectors[pieces[g][k]]\n",
    "        out[g][kol[g]+2:]-= np.ones(shape=(512-kol[g]-2,100))\n",
    "        out[g][kol[g]+2:]-= np.ones(shape=(512-kol[g]-2,100))    \n",
    "    return out, kol       #, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "**********************************\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1/1\n",
      "475/475 [==============================] - 8s 17ms/step - loss: 2.1809 - val_loss: 0.9535\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 21/21\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1452 - val_loss: 0.1232\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 41/41\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1688 - val_loss: 0.1674\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 61/61\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1828 - val_loss: 0.1512\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 81/81\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.1159\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 101/101\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1575 - val_loss: 0.2285\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 121/121\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1545 - val_loss: 0.1271\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 141/141\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1184 - val_loss: 0.1636\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 161/161\n",
      "474/474 [==============================] - 3s 5ms/step - loss: 0.1635 - val_loss: 0.1677\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 181/181\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1534 - val_loss: 0.2344\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 201/201\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1716 - val_loss: 0.1384\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 221/221\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.1351\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 241/241\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1785 - val_loss: 0.2665\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 261/261\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1761 - val_loss: 0.1505\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 281/281\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1900 - val_loss: 0.2164\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 301/301\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1484 - val_loss: 0.1649\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 321/321\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1962 - val_loss: 0.1141\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 341/341\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1711 - val_loss: 0.2209\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 361/361\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1764 - val_loss: 0.1338\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 381/381\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1590 - val_loss: 0.1456\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 401/401\n",
      "474/474 [==============================] - 3s 5ms/step - loss: 0.1803 - val_loss: 0.1857\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 421/421\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1753 - val_loss: 0.2034\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 441/441\n",
      "474/474 [==============================] - 3s 5ms/step - loss: 0.1579 - val_loss: 0.1585\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 461/461\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1650 - val_loss: 0.1798\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 481/481\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1566 - val_loss: 0.1369\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 501/501\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.1229\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 521/521\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1640 - val_loss: 0.1619\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 541/541\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1684 - val_loss: 0.1082\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 561/561\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1600 - val_loss: 0.1472\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 581/581\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1527 - val_loss: 0.0906\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 601/601\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1600 - val_loss: 0.1268\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 621/621\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.1527\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 641/641\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1492 - val_loss: 0.1148\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 661/661\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1591 - val_loss: 0.1384\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 681/681\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1863 - val_loss: 0.1274\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 701/701\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1561 - val_loss: 0.1397\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 721/721\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1822 - val_loss: 0.2139\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 741/741\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1599 - val_loss: 0.1253\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 761/761\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1339 - val_loss: 0.1477\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 781/781\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1440 - val_loss: 0.1902\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "**********************************\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 801/801\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1674 - val_loss: 0.2015\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 821/821\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1403 - val_loss: 0.1128\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 841/841\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1626 - val_loss: 0.1176\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 861/861\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1812 - val_loss: 0.1499\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 881/881\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1644 - val_loss: 0.1123\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 901/901\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1558 - val_loss: 0.2168\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 921/921\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1529 - val_loss: 0.1168\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 941/941\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1166 - val_loss: 0.1467\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 961/961\n",
      "474/474 [==============================] - 2s 5ms/step - loss: 0.1628 - val_loss: 0.1600\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 981/981\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1531 - val_loss: 0.2440\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1001/1001\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1695 - val_loss: 0.1353\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1021/1021\n",
      "475/475 [==============================] - 3s 5ms/step - loss: 0.1617 - val_loss: 0.1309\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1041/1041\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1768 - val_loss: 0.2271\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1061/1061\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1744 - val_loss: 0.1318\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1081/1081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1886 - val_loss: 0.2301\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1101/1101\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1476 - val_loss: 0.1659\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1121/1121\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1943 - val_loss: 0.1167\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1141/1141\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1694 - val_loss: 0.1925\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1161/1161\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1753 - val_loss: 0.1477\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1181/1181\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1585 - val_loss: 0.1476\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 1201/1201\n",
      "474/474 [==============================] - 2s 5ms/step - loss: 0.1787 - val_loss: 0.1817\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1221/1221\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1692 - val_loss: 0.2016\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 1241/1241\n",
      "474/474 [==============================] - 2s 5ms/step - loss: 0.1566 - val_loss: 0.1562\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1261/1261\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1658 - val_loss: 0.1792\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1281/1281\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1562 - val_loss: 0.1194\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1301/1301\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1670 - val_loss: 0.1265\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1321/1321\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1636 - val_loss: 0.1664\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1341/1341\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1676 - val_loss: 0.1168\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1361/1361\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1594 - val_loss: 0.1404\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1381/1381\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1518 - val_loss: 0.0911\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1401/1401\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1589 - val_loss: 0.1223\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1421/1421\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1721 - val_loss: 0.1510\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1441/1441\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1477 - val_loss: 0.1123\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1461/1461\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1589 - val_loss: 0.1381\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1481/1481\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1852 - val_loss: 0.1259\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1501/1501\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1557 - val_loss: 0.1334\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1521/1521\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1824 - val_loss: 0.2172\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1541/1541\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1600 - val_loss: 0.1247\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1561/1561\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1335 - val_loss: 0.1512\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1581/1581\n",
      "475/475 [==============================] - 2s 5ms/step - loss: 0.1433 - val_loss: 0.1942\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "n=0\n",
    "e=1\n",
    "for d in range(2):\n",
    "    start=1\n",
    "    print('Epoch: '+str(d)+'\\n'+\"**********************************\\n\")\n",
    "    for i in range(800):\n",
    "        vectors_train = np.load('D:/112/decoder/data/data3000/news_vectors'+str(999-i)+'.npy')\n",
    "        #print(start)\n",
    "        texts = textreader(start,vectors_train.shape[0])\n",
    "        start+=vectors_train.shape[0]\n",
    "        #print(len(texts))\n",
    "\n",
    "        #for j in range(5):\n",
    "        tt, kol  = word_vectors_creator(texts)\n",
    "        #vtt = vectors_train[0+j*100:(1+j)*100]\n",
    "        tt, vectors_train = shuffle(tt, vectors_train, random_state=0)\n",
    "\n",
    "        if n%20==0:\n",
    "            model.fit(\n",
    "                x=vectors_train,\n",
    "                y =tt,\n",
    "                epochs=e, initial_epoch=n,\n",
    "                validation_split=0.05,\n",
    "                verbose=1,\n",
    "                batch_size=8\n",
    "            )\n",
    "            if n%500==0:\n",
    "                model.save('D:/112/decoder/models/conv_mse_sen_to_word'+str(d)+'_'+str(i)+'.h5')\n",
    "                print('\\n')\n",
    "        else:\n",
    "            model.fit(\n",
    "                x=vectors_train,\n",
    "                y =tt,\n",
    "                epochs=e, initial_epoch=n,\n",
    "                validation_split=0.05,\n",
    "                verbose=0,\n",
    "                batch_size=8\n",
    "            )\n",
    "        n+=1\n",
    "        e+=1\n",
    "    model.save('D:/112/decoder/models/conv_mse_sen_to_word'+str(d)+'.h5')\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
