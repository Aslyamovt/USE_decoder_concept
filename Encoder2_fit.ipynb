{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y):\n",
    "    from keras import backend as k\n",
    "    return k.one_hot(y, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Concatenate,Lambda,Reshape,Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "vector = Input(shape=(512,), name='Vectors-Input',dtype='float32')\n",
    "vector_shiter0 = Dense(512)(vector)\n",
    "norm_vector_shiter0=BatchNormalization()(vector_shiter0)\n",
    "vector_shiter = Dense(512)(norm_vector_shiter0)\n",
    "norm_vector_shiter=BatchNormalization()(vector_shiter)\n",
    "position = Input(shape=(1,), name='Positions-Input',dtype='int32')\n",
    "categorical_position=Lambda(to_categorical, name='Positional_encoding')(position)\n",
    "reshaped_categorical_position=Reshape((512,))(categorical_position)\n",
    "concatenated_encoder_input=Concatenate()([norm_vector_shiter,reshaped_categorical_position])\n",
    "encoder_input_divider1 = Dense(1024, name='Encoder-Output-Divider-1',activation='selu')(concatenated_encoder_input)\n",
    "norm_encoder_input1=BatchNormalization()(encoder_input_divider1)\n",
    "\n",
    "encoder_input_divider2 = Dense(512, name='Encoder-Output-Divider-2',activation='selu')(norm_encoder_input1)\n",
    "norm_encoder_input2=BatchNormalization()(encoder_input_divider2)\n",
    "\n",
    "output_vector = Dense(512, name='Encoder-Garbedge',activation='selu')(norm_encoder_input2)\n",
    "encoder_input_divider3 = Dense(256, name='Encoder-Output-Divider-3',activation='selu')(norm_encoder_input2)\n",
    "norm_encoder_input3=BatchNormalization()(encoder_input_divider3)\n",
    "\n",
    "output_vector2 = Dense(100, name='Encoder-Output',activation='selu')(norm_encoder_input3)\n",
    "\n",
    "model = Model(inputs=[vector,position],outputs=[output_vector,output_vector2])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_absolute_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "from collections import Iterable\n",
    "import numpy as np\n",
    "\n",
    "bpemb_ru = BPEmb(lang=\"ru\", dim=100, vs=3000)\n",
    "\n",
    "def textreader(st):\n",
    "    texts = []\n",
    "    with open(\"D:/decoder/data/data3000/np_rev2.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        i=0\n",
    "        for line in fr:\n",
    "            i+=1\n",
    "            if i<st:      \n",
    "                continue\n",
    "            texts.append(line.replace('\\n',''))\n",
    "            if i==st+499:\n",
    "                break\n",
    "    return texts\n",
    "\n",
    "def word_vectors_creator(data):\n",
    "    l=len(data)\n",
    "    kol= np.zeros(shape=(l))\n",
    "    pieces=[]\n",
    "    for i in range(l):\n",
    "        s = bpemb_ru.encode_ids(data[i])\n",
    "        kol[i]+=len(s)\n",
    "        pieces.append(s)\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(np.sum(kol),100))\n",
    "    #out2 = np.zeros(shape=(np.sum(kol),100))\n",
    "    buf = 0\n",
    "    for g in range(l):        \n",
    "        for k in range(0,kol[g]):\n",
    "            out[buf]+=bpemb_ru.emb.vectors[pieces[g][k]]\n",
    "            '''\n",
    "            for m in range(kol[g]):\n",
    "                if m!=k:\n",
    "                    out2[buf]+=(bpemb_ru.emb.vectors[pieces[g][m]]/kol[g])\n",
    "            buf+=1\n",
    "            '''\n",
    "    return out, kol       #, out2\n",
    "\n",
    "def vectorsextender(data,kol):\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    out2 = np.zeros(shape=(np.sum(kol)))\n",
    "    buf=0\n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(kol[g]):\n",
    "            out[buf]+=data[g]\n",
    "            #out2[buf]+=np.sum(data[g])/512\n",
    "            buf+=1\n",
    "    return out#, out2\n",
    "\n",
    "def positioncreator(kol):\n",
    "    out = np.zeros(shape=(np.sum(kol)))\n",
    "    buf=0\n",
    "    for g in range(len(kol)):\n",
    "        out[buf:buf+kol[g]]+=np.arange(1,kol[g]+1)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model ('D:/112/decoder/models/mse_sen_to_word0_100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4578 samples, validate on 241 samples\n",
      "Epoch 521/521\n",
      "4578/4578 [==============================] - 17s 4ms/step - loss: 7.8072 - Encoder-Garbedge_loss: 0.0132 - Encoder-Output_loss: 7.7940 - val_loss: 1.9301 - val_Encoder-Garbedge_loss: 0.0138 - val_Encoder-Output_loss: 1.9163\n",
      "Train on 5409 samples, validate on 285 samples\n",
      "Epoch 541/541\n",
      "5409/5409 [==============================] - 21s 4ms/step - loss: 7.8029 - Encoder-Garbedge_loss: 0.0131 - Encoder-Output_loss: 7.7898 - val_loss: 1.8076 - val_Encoder-Garbedge_loss: 0.0128 - val_Encoder-Output_loss: 1.7947\n",
      "Train on 3873 samples, validate on 204 samples\n",
      "Epoch 561/561\n",
      "3873/3873 [==============================] - 14s 4ms/step - loss: 6.8133 - Encoder-Garbedge_loss: 0.0130 - Encoder-Output_loss: 6.8003 - val_loss: 1.7637 - val_Encoder-Garbedge_loss: 0.0120 - val_Encoder-Output_loss: 1.7517\n",
      "Train on 6058 samples, validate on 319 samples\n",
      "Epoch 581/581\n",
      "6058/6058 [==============================] - 22s 4ms/step - loss: 8.3074 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 8.2941 - val_loss: 1.8066 - val_Encoder-Garbedge_loss: 0.0139 - val_Encoder-Output_loss: 1.7927\n",
      "Train on 4282 samples, validate on 226 samples\n",
      "Epoch 601/601\n",
      "4282/4282 [==============================] - 16s 4ms/step - loss: 8.9110 - Encoder-Garbedge_loss: 0.0134 - Encoder-Output_loss: 8.8976 - val_loss: 1.9907 - val_Encoder-Garbedge_loss: 0.0112 - val_Encoder-Output_loss: 1.9795\n",
      "Train on 4404 samples, validate on 232 samples\n",
      "Epoch 621/621\n",
      "4404/4404 [==============================] - 16s 4ms/step - loss: 8.1762 - Encoder-Garbedge_loss: 0.0137 - Encoder-Output_loss: 8.1626 - val_loss: 1.1598 - val_Encoder-Garbedge_loss: 0.0121 - val_Encoder-Output_loss: 1.1477\n",
      "Train on 4845 samples, validate on 256 samples\n",
      "Epoch 641/641\n",
      "4845/4845 [==============================] - 18s 4ms/step - loss: 12.5204 - Encoder-Garbedge_loss: 0.0135 - Encoder-Output_loss: 12.5070 - val_loss: 1.7150 - val_Encoder-Garbedge_loss: 0.0148 - val_Encoder-Output_loss: 1.7002\n",
      "Train on 4103 samples, validate on 216 samples\n",
      "Epoch 661/661\n",
      "4103/4103 [==============================] - 15s 4ms/step - loss: 7.1542 - Encoder-Garbedge_loss: 0.0136 - Encoder-Output_loss: 7.1407 - val_loss: 1.7109 - val_Encoder-Garbedge_loss: 0.0109 - val_Encoder-Output_loss: 1.7000\n",
      "Train on 4078 samples, validate on 215 samples\n",
      "Epoch 681/681\n",
      "4078/4078 [==============================] - 15s 4ms/step - loss: 8.6154 - Encoder-Garbedge_loss: 0.0134 - Encoder-Output_loss: 8.6020 - val_loss: 1.7729 - val_Encoder-Garbedge_loss: 0.0111 - val_Encoder-Output_loss: 1.7618\n",
      "Train on 5072 samples, validate on 267 samples\n",
      "Epoch 701/701\n",
      "5072/5072 [==============================] - 19s 4ms/step - loss: 7.7463 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 7.7330 - val_loss: 1.6511 - val_Encoder-Garbedge_loss: 0.0129 - val_Encoder-Output_loss: 1.6382\n",
      "Train on 3608 samples, validate on 190 samples\n",
      "Epoch 721/721\n",
      "3608/3608 [==============================] - 13s 4ms/step - loss: 7.7871 - Encoder-Garbedge_loss: 0.0134 - Encoder-Output_loss: 7.7737 - val_loss: 1.8648 - val_Encoder-Garbedge_loss: 0.0103 - val_Encoder-Output_loss: 1.8545\n",
      "Train on 4101 samples, validate on 216 samples\n",
      "Epoch 741/741\n",
      "4101/4101 [==============================] - 16s 4ms/step - loss: 5.5386 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 5.5253 - val_loss: 1.6848 - val_Encoder-Garbedge_loss: 0.0157 - val_Encoder-Output_loss: 1.6691\n",
      "Train on 3724 samples, validate on 196 samples\n",
      "Epoch 761/761\n",
      "3724/3724 [==============================] - 13s 4ms/step - loss: 7.1886 - Encoder-Garbedge_loss: 0.0135 - Encoder-Output_loss: 7.1751 - val_loss: 2.6791 - val_Encoder-Garbedge_loss: 0.0121 - val_Encoder-Output_loss: 2.6670\n",
      "Train on 5636 samples, validate on 297 samples\n",
      "Epoch 781/781\n",
      "5636/5636 [==============================] - 20s 4ms/step - loss: 7.7704 - Encoder-Garbedge_loss: 0.0134 - Encoder-Output_loss: 7.7570 - val_loss: 1.7317 - val_Encoder-Garbedge_loss: 0.0134 - val_Encoder-Output_loss: 1.7183\n",
      "Train on 4170 samples, validate on 220 samples\n",
      "Epoch 801/801\n",
      "4170/4170 [==============================] - 15s 4ms/step - loss: 6.8398 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 6.8265 - val_loss: 1.8078 - val_Encoder-Garbedge_loss: 0.0147 - val_Encoder-Output_loss: 1.7930\n",
      "Train on 4520 samples, validate on 238 samples\n",
      "Epoch 821/821\n",
      "4520/4520 [==============================] - 17s 4ms/step - loss: 7.7271 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 7.7138 - val_loss: 1.8315 - val_Encoder-Garbedge_loss: 0.0079 - val_Encoder-Output_loss: 1.8236\n",
      "Train on 4821 samples, validate on 254 samples\n",
      "Epoch 841/841\n",
      "4821/4821 [==============================] - 17s 4ms/step - loss: 8.2267 - Encoder-Garbedge_loss: 0.0132 - Encoder-Output_loss: 8.2136 - val_loss: 1.6843 - val_Encoder-Garbedge_loss: 0.0152 - val_Encoder-Output_loss: 1.6691\n",
      "Train on 4787 samples, validate on 252 samples\n",
      "Epoch 861/861\n",
      "4787/4787 [==============================] - 17s 4ms/step - loss: 8.6598 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 8.6464 - val_loss: 2.0211 - val_Encoder-Garbedge_loss: 0.0120 - val_Encoder-Output_loss: 2.0091\n",
      "Train on 5714 samples, validate on 301 samples\n",
      "Epoch 881/881\n",
      "5714/5714 [==============================] - 25s 4ms/step - loss: 7.2849 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 7.2716 - val_loss: 11.5077 - val_Encoder-Garbedge_loss: 0.0124 - val_Encoder-Output_loss: 11.4953\n",
      "Train on 3662 samples, validate on 193 samples\n",
      "Epoch 901/901\n",
      "3662/3662 [==============================] - 14s 4ms/step - loss: 7.5939 - Encoder-Garbedge_loss: 0.0135 - Encoder-Output_loss: 7.5805 - val_loss: 7.9658 - val_Encoder-Garbedge_loss: 0.0104 - val_Encoder-Output_loss: 7.9554\n",
      "Train on 5016 samples, validate on 264 samples\n",
      "Epoch 921/921\n",
      "5016/5016 [==============================] - 19s 4ms/step - loss: 7.2696 - Encoder-Garbedge_loss: 0.0133 - Encoder-Output_loss: 7.2563 - val_loss: 1.7108 - val_Encoder-Garbedge_loss: 0.0108 - val_Encoder-Output_loss: 1.7000\n",
      "Train on 4517 samples, validate on 238 samples\n",
      "Epoch 941/941\n",
      "4517/4517 [==============================] - 18s 4ms/step - loss: 8.2673 - Encoder-Garbedge_loss: 0.0132 - Encoder-Output_loss: 8.2541 - val_loss: 1.8404 - val_Encoder-Garbedge_loss: 0.0169 - val_Encoder-Output_loss: 1.8235\n",
      "Train on 4483 samples, validate on 236 samples\n",
      "Epoch 961/961\n",
      "4483/4483 [==============================] - 18s 4ms/step - loss: 6.7033 - Encoder-Garbedge_loss: 0.0139 - Encoder-Output_loss: 6.6894 - val_loss: 1.6192 - val_Encoder-Garbedge_loss: 0.0118 - val_Encoder-Output_loss: 1.6074\n",
      "Train on 4614 samples, validate on 243 samples\n",
      "Epoch 981/981\n",
      "4614/4614 [==============================] - 18s 4ms/step - loss: 8.7447 - Encoder-Garbedge_loss: 0.0138 - Encoder-Output_loss: 8.7309 - val_loss: 1.6822 - val_Encoder-Garbedge_loss: 0.0128 - val_Encoder-Output_loss: 1.6694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "start=1\n",
    "n=501\n",
    "e=502\n",
    "for i in range(101,200):\n",
    "    vectors_train = np.load('D:/decoder/data/data3000/news_vectors'+str(999-i)+'.npy')\n",
    "    #print(start)\n",
    "    texts = textreader(start)\n",
    "    start+=500\n",
    "    #print(len(texts))\n",
    "\n",
    "    for j in range(5):\n",
    "        tt, kol  = word_vectors_creator(texts[0+j*100:(1+j)*100])\n",
    "        vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "        pos = positioncreator(kol)\n",
    "        tt, vtt,pos = shuffle(tt, vtt,pos, random_state=0)\n",
    "\n",
    "        if n%20==0:\n",
    "            model.fit(\n",
    "                x=[vtt,\n",
    "                   pos],\n",
    "                y = [vtt,\n",
    "                    tt],\n",
    "                epochs=e, initial_epoch=n,\n",
    "                validation_split=0.05,\n",
    "                verbose=1,\n",
    "                batch_size=4\n",
    "            )\n",
    "            if n%500==0:\n",
    "                model.save('D:/decoder/models/mse_sen_to_word'+str(d)+'_'+str(i)+'.h5')\n",
    "                print('\\n')\n",
    "        else:\n",
    "            model.fit(\n",
    "                x=[vtt,\n",
    "                   pos],\n",
    "                y = [vtt,\n",
    "                    tt],\n",
    "                epochs=e, initial_epoch=n,\n",
    "                validation_split=0.05,\n",
    "                verbose=0,\n",
    "                batch_size=4\n",
    "            )\n",
    "        n+=1\n",
    "        e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "**********************************\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 4308 samples, validate on 227 samples\n",
      "Epoch 1/1\n",
      "4308/4308 [==============================] - 27s 6ms/step - loss: 0.2586 - Encoder-Garbedge_loss: 0.1298 - Encoder-Output_loss: 0.1288 - val_loss: 0.0991 - val_Encoder-Garbedge_loss: 0.0637 - val_Encoder-Output_loss: 0.0354\n",
      "\n",
      "\n",
      "Train on 4538 samples, validate on 239 samples\n",
      "Epoch 21/21\n",
      "4538/4538 [==============================] - 10s 2ms/step - loss: 0.0753 - Encoder-Garbedge_loss: 0.0322 - Encoder-Output_loss: 0.0431 - val_loss: 0.0407 - val_Encoder-Garbedge_loss: 0.0286 - val_Encoder-Output_loss: 0.0121\n",
      "Train on 4973 samples, validate on 262 samples\n",
      "Epoch 41/41\n",
      "4973/4973 [==============================] - 11s 2ms/step - loss: 0.0622 - Encoder-Garbedge_loss: 0.0264 - Encoder-Output_loss: 0.0358 - val_loss: 0.0341 - val_Encoder-Garbedge_loss: 0.0218 - val_Encoder-Output_loss: 0.0124\n",
      "Train on 4933 samples, validate on 260 samples\n",
      "Epoch 61/61\n",
      "4933/4933 [==============================] - 11s 2ms/step - loss: 0.0602 - Encoder-Garbedge_loss: 0.0261 - Encoder-Output_loss: 0.0342 - val_loss: 0.0513 - val_Encoder-Garbedge_loss: 0.0315 - val_Encoder-Output_loss: 0.0198\n",
      "Train on 6069 samples, validate on 320 samples\n",
      "Epoch 81/81\n",
      "6069/6069 [==============================] - 13s 2ms/step - loss: 0.0531 - Encoder-Garbedge_loss: 0.0251 - Encoder-Output_loss: 0.0280 - val_loss: 0.0270 - val_Encoder-Garbedge_loss: 0.0213 - val_Encoder-Output_loss: 0.0057\n",
      "Train on 4304 samples, validate on 227 samples\n",
      "Epoch 101/101\n",
      "4304/4304 [==============================] - 8s 2ms/step - loss: 0.0550 - Encoder-Garbedge_loss: 0.0243 - Encoder-Output_loss: 0.0307 - val_loss: 0.4126 - val_Encoder-Garbedge_loss: 0.2903 - val_Encoder-Output_loss: 0.1223\n",
      "Train on 5041 samples, validate on 266 samples\n",
      "Epoch 121/121\n",
      "5041/5041 [==============================] - 9s 2ms/step - loss: 0.0542 - Encoder-Garbedge_loss: 0.0230 - Encoder-Output_loss: 0.0312 - val_loss: 0.0163 - val_Encoder-Garbedge_loss: 0.0161 - val_Encoder-Output_loss: 1.7832e-04\n",
      "Train on 4783 samples, validate on 252 samples\n",
      "Epoch 141/141\n",
      "4783/4783 [==============================] - 10s 2ms/step - loss: 0.0464 - Encoder-Garbedge_loss: 0.0225 - Encoder-Output_loss: 0.0240 - val_loss: 0.0162 - val_Encoder-Garbedge_loss: 0.0161 - val_Encoder-Output_loss: 1.4729e-04\n",
      "Train on 3027 samples, validate on 160 samples\n",
      "Epoch 161/161\n",
      "3027/3027 [==============================] - 5s 2ms/step - loss: 0.0626 - Encoder-Garbedge_loss: 0.0235 - Encoder-Output_loss: 0.0392 - val_loss: 0.0159 - val_Encoder-Garbedge_loss: 0.0157 - val_Encoder-Output_loss: 1.5475e-04\n",
      "Train on 4394 samples, validate on 232 samples\n",
      "Epoch 181/181\n",
      "4394/4394 [==============================] - 8s 2ms/step - loss: 0.0539 - Encoder-Garbedge_loss: 0.0223 - Encoder-Output_loss: 0.0316 - val_loss: 0.0156 - val_Encoder-Garbedge_loss: 0.0154 - val_Encoder-Output_loss: 1.8254e-04\n",
      "Train on 4900 samples, validate on 258 samples\n",
      "Epoch 201/201\n",
      "4900/4900 [==============================] - 10s 2ms/step - loss: 0.0508 - Encoder-Garbedge_loss: 0.0222 - Encoder-Output_loss: 0.0286 - val_loss: 0.0154 - val_Encoder-Garbedge_loss: 0.0152 - val_Encoder-Output_loss: 1.7863e-04\n",
      "Train on 4313 samples, validate on 228 samples\n",
      "Epoch 221/221\n",
      "4313/4313 [==============================] - 8s 2ms/step - loss: 0.0499 - Encoder-Garbedge_loss: 0.0216 - Encoder-Output_loss: 0.0283 - val_loss: 0.0920 - val_Encoder-Garbedge_loss: 0.0750 - val_Encoder-Output_loss: 0.0170\n",
      "Train on 4843 samples, validate on 255 samples\n",
      "Epoch 241/241\n",
      "4843/4843 [==============================] - 9s 2ms/step - loss: 0.0487 - Encoder-Garbedge_loss: 0.0212 - Encoder-Output_loss: 0.0275 - val_loss: 0.0145 - val_Encoder-Garbedge_loss: 0.0142 - val_Encoder-Output_loss: 2.1766e-04\n",
      "Train on 4794 samples, validate on 253 samples\n",
      "Epoch 261/261\n",
      "4794/4794 [==============================] - 9s 2ms/step - loss: 0.0487 - Encoder-Garbedge_loss: 0.0218 - Encoder-Output_loss: 0.0270 - val_loss: 0.0148 - val_Encoder-Garbedge_loss: 0.0146 - val_Encoder-Output_loss: 1.8415e-04\n",
      "Train on 4329 samples, validate on 228 samples\n",
      "Epoch 281/281\n",
      "4329/4329 [==============================] - 8s 2ms/step - loss: 0.0514 - Encoder-Garbedge_loss: 0.0219 - Encoder-Output_loss: 0.0295 - val_loss: 0.0145 - val_Encoder-Garbedge_loss: 0.0144 - val_Encoder-Output_loss: 1.7971e-04\n",
      "Train on 6166 samples, validate on 325 samples\n",
      "Epoch 301/301\n",
      "6166/6166 [==============================] - 11s 2ms/step - loss: 0.0455 - Encoder-Garbedge_loss: 0.0207 - Encoder-Output_loss: 0.0247 - val_loss: 0.0132 - val_Encoder-Garbedge_loss: 0.0130 - val_Encoder-Output_loss: 1.7718e-04\n",
      "Train on 4424 samples, validate on 233 samples\n",
      "Epoch 321/321\n",
      "4424/4424 [==============================] - 8s 2ms/step - loss: 0.0494 - Encoder-Garbedge_loss: 0.0208 - Encoder-Output_loss: 0.0287 - val_loss: 0.0134 - val_Encoder-Garbedge_loss: 0.0132 - val_Encoder-Output_loss: 1.6670e-04\n",
      "Train on 5870 samples, validate on 309 samples\n",
      "Epoch 341/341\n",
      "5870/5870 [==============================] - 11s 2ms/step - loss: 0.0507 - Encoder-Garbedge_loss: 0.0211 - Encoder-Output_loss: 0.0296 - val_loss: 0.0129 - val_Encoder-Garbedge_loss: 0.0127 - val_Encoder-Output_loss: 1.9210e-04\n",
      "Train on 3971 samples, validate on 209 samples\n",
      "Epoch 361/361\n",
      "3971/3971 [==============================] - 7s 2ms/step - loss: 0.0464 - Encoder-Garbedge_loss: 0.0216 - Encoder-Output_loss: 0.0248 - val_loss: 0.0144 - val_Encoder-Garbedge_loss: 0.0142 - val_Encoder-Output_loss: 2.0153e-04\n",
      "Train on 5811 samples, validate on 306 samples\n",
      "Epoch 381/381\n",
      "5811/5811 [==============================] - 11s 2ms/step - loss: 0.0441 - Encoder-Garbedge_loss: 0.0204 - Encoder-Output_loss: 0.0237 - val_loss: 0.0138 - val_Encoder-Garbedge_loss: 0.0136 - val_Encoder-Output_loss: 2.0826e-04\n",
      "Train on 3744 samples, validate on 198 samples\n",
      "Epoch 401/401\n",
      "3744/3744 [==============================] - 7s 2ms/step - loss: 0.0478 - Encoder-Garbedge_loss: 0.0217 - Encoder-Output_loss: 0.0261 - val_loss: 0.0196 - val_Encoder-Garbedge_loss: 0.0193 - val_Encoder-Output_loss: 2.6059e-04\n",
      "Train on 4595 samples, validate on 242 samples\n",
      "Epoch 421/421\n",
      "4595/4595 [==============================] - 8s 2ms/step - loss: 0.0503 - Encoder-Garbedge_loss: 0.0215 - Encoder-Output_loss: 0.0288 - val_loss: 0.0146 - val_Encoder-Garbedge_loss: 0.0144 - val_Encoder-Output_loss: 1.9441e-04\n",
      "Train on 3677 samples, validate on 194 samples\n",
      "Epoch 441/441\n",
      "3677/3677 [==============================] - 7s 2ms/step - loss: 0.0523 - Encoder-Garbedge_loss: 0.0209 - Encoder-Output_loss: 0.0314 - val_loss: 0.0152 - val_Encoder-Garbedge_loss: 0.0150 - val_Encoder-Output_loss: 1.9693e-04\n",
      "Train on 4137 samples, validate on 218 samples\n",
      "Epoch 461/461\n",
      "4137/4137 [==============================] - 7s 2ms/step - loss: 0.0467 - Encoder-Garbedge_loss: 0.0199 - Encoder-Output_loss: 0.0268 - val_loss: 0.0157 - val_Encoder-Garbedge_loss: 0.0154 - val_Encoder-Output_loss: 3.3804e-04\n",
      "Train on 3463 samples, validate on 183 samples\n",
      "Epoch 481/481\n",
      "3463/3463 [==============================] - 7s 2ms/step - loss: 0.0550 - Encoder-Garbedge_loss: 0.0212 - Encoder-Output_loss: 0.0338 - val_loss: 0.0152 - val_Encoder-Garbedge_loss: 0.0150 - val_Encoder-Output_loss: 1.6751e-04\n",
      "Train on 4751 samples, validate on 251 samples\n",
      "Epoch 501/501\n",
      "4751/4751 [==============================] - 9s 2ms/step - loss: 0.0466 - Encoder-Garbedge_loss: 0.0207 - Encoder-Output_loss: 0.0259 - val_loss: 0.0135 - val_Encoder-Garbedge_loss: 0.0134 - val_Encoder-Output_loss: 1.7757e-04\n",
      "\n",
      "\n",
      "Train on 4129 samples, validate on 218 samples\n",
      "Epoch 521/521\n",
      "4129/4129 [==============================] - 8s 2ms/step - loss: 0.0502 - Encoder-Garbedge_loss: 0.0218 - Encoder-Output_loss: 0.0283 - val_loss: 0.0148 - val_Encoder-Garbedge_loss: 0.0146 - val_Encoder-Output_loss: 1.9246e-04\n",
      "Train on 5168 samples, validate on 273 samples\n",
      "Epoch 541/541\n",
      "5168/5168 [==============================] - 9s 2ms/step - loss: 0.0479 - Encoder-Garbedge_loss: 0.0208 - Encoder-Output_loss: 0.0271 - val_loss: 0.0138 - val_Encoder-Garbedge_loss: 0.0136 - val_Encoder-Output_loss: 1.9403e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4759 samples, validate on 251 samples\n",
      "Epoch 561/561\n",
      "4759/4759 [==============================] - 9s 2ms/step - loss: 0.0489 - Encoder-Garbedge_loss: 0.0211 - Encoder-Output_loss: 0.0278 - val_loss: 0.0139 - val_Encoder-Garbedge_loss: 0.0137 - val_Encoder-Output_loss: 2.0658e-04\n",
      "Train on 3977 samples, validate on 210 samples\n",
      "Epoch 581/581\n",
      "3977/3977 [==============================] - 7s 2ms/step - loss: 0.0494 - Encoder-Garbedge_loss: 0.0212 - Encoder-Output_loss: 0.0282 - val_loss: 0.0141 - val_Encoder-Garbedge_loss: 0.0139 - val_Encoder-Output_loss: 2.0803e-04\n",
      "Train on 4747 samples, validate on 250 samples\n",
      "Epoch 601/601\n",
      "4747/4747 [==============================] - 8s 2ms/step - loss: 0.0508 - Encoder-Garbedge_loss: 0.0214 - Encoder-Output_loss: 0.0293 - val_loss: 0.0143 - val_Encoder-Garbedge_loss: 0.0141 - val_Encoder-Output_loss: 2.1480e-04\n",
      "Train on 3945 samples, validate on 208 samples\n",
      "Epoch 621/621\n",
      "3945/3945 [==============================] - 7s 2ms/step - loss: 0.0506 - Encoder-Garbedge_loss: 0.0211 - Encoder-Output_loss: 0.0295 - val_loss: 0.0147 - val_Encoder-Garbedge_loss: 0.0145 - val_Encoder-Output_loss: 1.9366e-04\n",
      "Train on 3979 samples, validate on 210 samples\n",
      "Epoch 641/641\n",
      "3979/3979 [==============================] - 7s 2ms/step - loss: 0.0502 - Encoder-Garbedge_loss: 0.0200 - Encoder-Output_loss: 0.0302 - val_loss: 0.0130 - val_Encoder-Garbedge_loss: 0.0127 - val_Encoder-Output_loss: 2.4600e-04\n",
      "Train on 3924 samples, validate on 207 samples\n",
      "Epoch 661/661\n",
      "3924/3924 [==============================] - 7s 2ms/step - loss: 0.0500 - Encoder-Garbedge_loss: 0.0213 - Encoder-Output_loss: 0.0288 - val_loss: 0.0147 - val_Encoder-Garbedge_loss: 0.0145 - val_Encoder-Output_loss: 2.0294e-04\n",
      "Train on 2609 samples, validate on 138 samples\n",
      "Epoch 681/681\n",
      "2609/2609 [==============================] - 5s 2ms/step - loss: 0.0536 - Encoder-Garbedge_loss: 0.0217 - Encoder-Output_loss: 0.0319 - val_loss: 0.0154 - val_Encoder-Garbedge_loss: 0.0152 - val_Encoder-Output_loss: 1.9045e-04\n",
      "Train on 4256 samples, validate on 225 samples\n",
      "Epoch 701/701\n",
      "4256/4256 [==============================] - 8s 2ms/step - loss: 0.0473 - Encoder-Garbedge_loss: 0.0208 - Encoder-Output_loss: 0.0265 - val_loss: 0.0188 - val_Encoder-Garbedge_loss: 0.0177 - val_Encoder-Output_loss: 0.0011\n",
      "Train on 3404 samples, validate on 180 samples\n",
      "Epoch 721/721\n",
      "3404/3404 [==============================] - 6s 2ms/step - loss: 0.0553 - Encoder-Garbedge_loss: 0.0209 - Encoder-Output_loss: 0.0343 - val_loss: 0.0141 - val_Encoder-Garbedge_loss: 0.0140 - val_Encoder-Output_loss: 1.8650e-04\n",
      "Train on 4147 samples, validate on 219 samples\n",
      "Epoch 741/741\n",
      "4147/4147 [==============================] - 11s 3ms/step - loss: 0.0464 - Encoder-Garbedge_loss: 0.0211 - Encoder-Output_loss: 0.0252 - val_loss: 0.0140 - val_Encoder-Garbedge_loss: 0.0138 - val_Encoder-Output_loss: 1.8653e-04\n",
      "Train on 4631 samples, validate on 244 samples\n",
      "Epoch 761/761\n",
      "4631/4631 [==============================] - 11s 2ms/step - loss: 0.0485 - Encoder-Garbedge_loss: 0.0206 - Encoder-Output_loss: 0.0279 - val_loss: 0.0139 - val_Encoder-Garbedge_loss: 0.0138 - val_Encoder-Output_loss: 1.6335e-04\n",
      "Train on 4602 samples, validate on 243 samples\n",
      "Epoch 781/781\n",
      "4602/4602 [==============================] - 10s 2ms/step - loss: 0.0465 - Encoder-Garbedge_loss: 0.0199 - Encoder-Output_loss: 0.0266 - val_loss: 0.0140 - val_Encoder-Garbedge_loss: 0.0138 - val_Encoder-Output_loss: 2.7828e-04\n",
      "Train on 4606 samples, validate on 243 samples\n",
      "Epoch 801/801\n",
      "4606/4606 [==============================] - 10s 2ms/step - loss: 0.0446 - Encoder-Garbedge_loss: 0.0199 - Encoder-Output_loss: 0.0247 - val_loss: 0.0133 - val_Encoder-Garbedge_loss: 0.0131 - val_Encoder-Output_loss: 2.1143e-04\n",
      "Train on 3764 samples, validate on 199 samples\n",
      "Epoch 821/821\n",
      "3764/3764 [==============================] - 9s 2ms/step - loss: 0.0492 - Encoder-Garbedge_loss: 0.0210 - Encoder-Output_loss: 0.0282 - val_loss: 0.0154 - val_Encoder-Garbedge_loss: 0.0152 - val_Encoder-Output_loss: 1.9794e-04\n",
      "Train on 4921 samples, validate on 259 samples\n",
      "Epoch 841/841\n",
      "4921/4921 [==============================] - 11s 2ms/step - loss: 0.0479 - Encoder-Garbedge_loss: 0.0209 - Encoder-Output_loss: 0.0269 - val_loss: 0.0295 - val_Encoder-Garbedge_loss: 0.0274 - val_Encoder-Output_loss: 0.0021\n",
      "Train on 3509 samples, validate on 185 samples\n",
      "Epoch 861/861\n",
      "3509/3509 [==============================] - 8s 2ms/step - loss: 0.0508 - Encoder-Garbedge_loss: 0.0216 - Encoder-Output_loss: 0.0292 - val_loss: 0.0142 - val_Encoder-Garbedge_loss: 0.0141 - val_Encoder-Output_loss: 1.7734e-04\n",
      "Train on 5059 samples, validate on 267 samples\n",
      "Epoch 881/881\n",
      "5059/5059 [==============================] - 11s 2ms/step - loss: 0.0452 - Encoder-Garbedge_loss: 0.0199 - Encoder-Output_loss: 0.0253 - val_loss: 0.0186 - val_Encoder-Garbedge_loss: 0.0182 - val_Encoder-Output_loss: 3.5988e-04\n",
      "Train on 3413 samples, validate on 180 samples\n",
      "Epoch 901/901\n",
      "3413/3413 [==============================] - 7s 2ms/step - loss: 0.0463 - Encoder-Garbedge_loss: 0.0212 - Encoder-Output_loss: 0.0251 - val_loss: 0.0149 - val_Encoder-Garbedge_loss: 0.0147 - val_Encoder-Output_loss: 1.9437e-04\n",
      "Train on 5419 samples, validate on 286 samples\n",
      "Epoch 921/921\n",
      "5419/5419 [==============================] - 12s 2ms/step - loss: 0.0471 - Encoder-Garbedge_loss: 0.0199 - Encoder-Output_loss: 0.0273 - val_loss: 0.0132 - val_Encoder-Garbedge_loss: 0.0131 - val_Encoder-Output_loss: 1.7766e-04\n",
      "Train on 3084 samples, validate on 163 samples\n",
      "Epoch 941/941\n",
      "3084/3084 [==============================] - 7s 2ms/step - loss: 0.0579 - Encoder-Garbedge_loss: 0.0229 - Encoder-Output_loss: 0.0349 - val_loss: 0.0153 - val_Encoder-Garbedge_loss: 0.0151 - val_Encoder-Output_loss: 2.0598e-04\n",
      "Train on 5675 samples, validate on 299 samples\n",
      "Epoch 961/961\n",
      "5675/5675 [==============================] - 12s 2ms/step - loss: 0.0540 - Encoder-Garbedge_loss: 0.0199 - Encoder-Output_loss: 0.0341 - val_loss: 0.0142 - val_Encoder-Garbedge_loss: 0.0140 - val_Encoder-Output_loss: 1.9044e-04\n",
      "Train on 5128 samples, validate on 270 samples\n",
      "Epoch 981/981\n",
      "5128/5128 [==============================] - 12s 2ms/step - loss: 0.0413 - Encoder-Garbedge_loss: 0.0203 - Encoder-Output_loss: 0.0210 - val_loss: 0.0347 - val_Encoder-Garbedge_loss: 0.0333 - val_Encoder-Output_loss: 0.0014\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "**********************************\n",
      "\n",
      "Train on 4308 samples, validate on 227 samples\n",
      "Epoch 1001/1001\n",
      "1152/4308 [=======>......................] - ETA: 8s - loss: 0.1116 - Encoder-Garbedge_loss: 0.0247 - Encoder-Output_loss: 0.0869- ETA: 8s - loss: 0.1149 - Encoder-Garbedge_loss: 0.0248 - Encoder-Output_loss: 0.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-07616699f316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 )\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    207\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mprev_total_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_width\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\b'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "n=0\n",
    "e=1\n",
    "for d in range(2):\n",
    "    start=1\n",
    "    print('Epoch: '+str(d)+'\\n'+\"**********************************\\n\")\n",
    "    for i in range(200):\n",
    "        vectors_train = np.load('D:/decoder/data/data3000/news_vectors'+str(999-i)+'.npy')\n",
    "        #print(start)\n",
    "        texts = textreader(start)\n",
    "        start+=500\n",
    "        #print(len(texts))\n",
    "\n",
    "        for j in range(5):\n",
    "            tt, kol  = word_vectors_creator(texts[0+j*100:(1+j)*100])\n",
    "            vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "            pos = positioncreator(kol)\n",
    "            tt, vtt,pos = shuffle(tt, vtt,pos, random_state=0)\n",
    "\n",
    "            if n%20==0:\n",
    "                model.fit(\n",
    "                    x=[vtt,\n",
    "                       pos],\n",
    "                    y = [vtt,\n",
    "                        tt],\n",
    "                    epochs=e, initial_epoch=n,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=1,\n",
    "                    batch_size=8\n",
    "                )\n",
    "                if n%500==0:\n",
    "                    model.save('D:/decoder/models/mae_sen_to_word'+str(d)+'_'+str(i)+'.h5')\n",
    "                    print('\\n')\n",
    "            else:\n",
    "                model.fit(\n",
    "                    x=[vtt,\n",
    "                       pos],\n",
    "                    y = [vtt,\n",
    "                        tt],\n",
    "                    epochs=e, initial_epoch=n,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=0,\n",
    "                    batch_size=8\n",
    "                )\n",
    "            n+=1\n",
    "            e+=1\n",
    "    model.save('D:/decoder/models/mae_sen_to_word'+str(d)+'.h5')\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/decoder/models/sen_to_word'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
