{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras_transformer import *\n",
    "\n",
    "import numpy as np\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Input, Lambda,RepeatVector,Dense,Reshape,Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "def get_m(token_num,\n",
    "          embed_dim,\n",
    "          encoder_num,\n",
    "          decoder_num,\n",
    "          head_num,\n",
    "          hidden_dim,\n",
    "          attention_activation=None,\n",
    "          feed_forward_activation='relu',\n",
    "          dropout_rate=0.0,\n",
    "          embed_weights =None,\n",
    "          embed_trainable=None,\n",
    "          trainable=True,\n",
    "          use_adapter=False,\n",
    "          adapter_units=None,\n",
    "          adapter_activation='relu'):\n",
    "\n",
    "    decoder_token_num = token_num\n",
    "\n",
    "    decoder_embed_weights = embed_weights\n",
    "\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    "\n",
    "    decoder_embed_trainable = embed_trainable\n",
    "\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    "\n",
    "\n",
    "    decoder_embed_layer = EmbeddingRet(\n",
    "        input_dim=decoder_token_num,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        weights=decoder_embed_weights,\n",
    "        trainable=decoder_embed_trainable,\n",
    "        name='Decoder-Token-Embedding',\n",
    "    )\n",
    "\n",
    "    encoder_input = keras.layers.Input(shape=(None,100), name='Encoder-Input')\n",
    "    pos_wised_encoder = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-Embedding',\n",
    "    )(encoder_input)\n",
    "    #pos_wised_encoder = Lambda(get_position_encoding, name='Encoder_With_Positions')(encoder_input)\n",
    "\n",
    "    encoded_layer = get_encoders(\n",
    "        encoder_num=encoder_num,\n",
    "        input_layer=pos_wised_encoder,#encoder_input,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "\n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input') \n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=encoded_layer,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    dense_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    return keras.models.Model(inputs=[encoder_input,decoder_input], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def piecesextender(data):\n",
    "    kol= np.zeros(shape=(data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        kol[i]=np.count_nonzero(data[i])\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(np.sum(kol)-2*kol.shape[0],512))\n",
    "    buf = 0\n",
    "    for g in range(data.shape[0]):        \n",
    "        for k in range(1,kol[g]-1):\n",
    "            out[buf][0]=3000\n",
    "            out[buf][1:513-k]+=data[g][k:]\n",
    "            buf+=1\n",
    "    return out, kol\n",
    "\n",
    "def positionencoder(kol):\n",
    "    out = np.zeros(shape=(np.sum(kol)-2*kol.shape[0],1))\n",
    "    out = out.astype(np.int)\n",
    "    buf=0\n",
    "    for g in range(len(kol)):\n",
    "        for k in range(1,kol[g]-1):\n",
    "            out[buf]=k\n",
    "            buf+=1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 100)\n",
      "(3003, 100)\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bpemb import BPEmb\n",
    "bpemb_ru = BPEmb(lang=\"ru\", dim=100, vs=3000)\n",
    "embedding_weights=bpemb_ru.emb.vectors\n",
    "embedding_weights=np.concatenate((embedding_weights,(np.zeros(shape=(1,100))-np.ones(shape=(1,100))-np.ones(shape=(1,100)))))\n",
    "embedding_weights=np.concatenate((embedding_weights,np.zeros(shape=(1,100))))\n",
    "embedding_weights=np.concatenate((embedding_weights,np.ones(shape=(1,100))))\n",
    "\n",
    "model = get_m(\n",
    "    token_num=3003,\n",
    "    embed_dim=100,\n",
    "    encoder_num=3,\n",
    "    decoder_num=6,\n",
    "    head_num=4,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=embedding_weights,\n",
    "    use_adapter=False,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model ('D:/112/decoder/models/m16.h5', custom_objects = get_custom_objects ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed\n",
      "Converted\n",
      "Train on 23548 samples, validate on 1240 samples\n",
      "Epoch 1/1\n",
      "23548/23548 [==============================] - 1324s 56ms/step - loss: 4.3661 - val_loss: 5.2460\n",
      "Readed\n",
      "Converted\n",
      "Train on 24654 samples, validate on 1298 samples\n",
      "Epoch 2/2\n",
      "24654/24654 [==============================] - 1369s 56ms/step - loss: 4.3670 - val_loss: 5.4071\n",
      "Readed\n",
      "Converted\n",
      "Train on 22035 samples, validate on 1160 samples\n",
      "Epoch 3/3\n",
      "22035/22035 [==============================] - 1227s 56ms/step - loss: 4.2867 - val_loss: 4.4019\n",
      "Readed\n",
      "Converted\n",
      "Train on 13135 samples, validate on 692 samples\n",
      "Epoch 4/4\n",
      "13135/13135 [==============================] - 727s 55ms/step - loss: 4.1814 - val_loss: 5.6172\n",
      "Readed\n",
      "Converted\n",
      "Train on 25193 samples, validate on 1326 samples\n",
      "Epoch 5/5\n",
      "25193/25193 [==============================] - 1400s 56ms/step - loss: 4.3580 - val_loss: 5.1899\n",
      "Readed\n",
      "Converted\n",
      "Train on 25993 samples, validate on 1369 samples\n",
      "Epoch 6/6\n",
      "25993/25993 [==============================] - 1453s 56ms/step - loss: 4.2867 - val_loss: 4.7067\n",
      "Readed\n",
      "Converted\n",
      "Train on 21755 samples, validate on 1146 samples\n",
      "Epoch 7/7\n",
      "21755/21755 [==============================] - 1204s 55ms/step - loss: 4.3861 - val_loss: 4.8312\n",
      "Readed\n",
      "Converted\n",
      "Train on 24488 samples, validate on 1289 samples\n",
      "Epoch 8/8\n",
      "24488/24488 [==============================] - 1365s 56ms/step - loss: 4.4518 - val_loss: 4.3066\n",
      "Readed\n",
      "Converted\n",
      "Train on 23444 samples, validate on 1234 samples\n",
      "Epoch 9/9\n",
      "23444/23444 [==============================] - 1311s 56ms/step - loss: 4.4148 - val_loss: 5.1126\n",
      "Readed\n",
      "Converted\n",
      "Train on 25720 samples, validate on 1354 samples\n",
      "Epoch 10/10\n",
      "25720/25720 [==============================] - 1425s 55ms/step - loss: 4.4761 - val_loss: 5.0394\n",
      "Readed\n",
      "Converted\n",
      "Train on 20987 samples, validate on 1105 samples\n",
      "Epoch 11/11\n",
      "20987/20987 [==============================] - 1161s 55ms/step - loss: 4.4212 - val_loss: 4.8596\n",
      "Readed\n",
      "Converted\n",
      "Train on 22472 samples, validate on 1183 samples\n",
      "Epoch 12/12\n",
      "22472/22472 [==============================] - 1240s 55ms/step - loss: 4.3410 - val_loss: 5.3755\n",
      "Readed\n",
      "Converted\n",
      "Train on 21451 samples, validate on 1130 samples\n",
      "Epoch 13/13\n",
      "21451/21451 [==============================] - 1185s 55ms/step - loss: 4.4200 - val_loss: 5.3950\n",
      "Readed\n",
      "Converted\n",
      "Train on 23025 samples, validate on 1212 samples\n",
      "Epoch 14/14\n",
      "23025/23025 [==============================] - 1273s 55ms/step - loss: 4.3371 - val_loss: 4.9688\n",
      "Readed\n",
      "Converted\n",
      "Train on 25616 samples, validate on 1349 samples\n",
      "Epoch 15/15\n",
      "25616/25616 [==============================] - 1436s 56ms/step - loss: 4.2980 - val_loss: 5.6832\n",
      "Readed\n",
      "Converted\n",
      "Train on 24297 samples, validate on 1279 samples\n",
      "Epoch 16/16\n",
      "24297/24297 [==============================] - 1340s 55ms/step - loss: 4.4328 - val_loss: 5.0230\n",
      "Readed\n",
      "Converted\n",
      "Train on 23105 samples, validate on 1217 samples\n",
      "Epoch 17/17\n",
      "23105/23105 [==============================] - 1277s 55ms/step - loss: 4.4308 - val_loss: 5.2654\n",
      "Readed\n",
      "Converted\n",
      "Train on 24183 samples, validate on 1273 samples\n",
      "Epoch 18/18\n",
      "24183/24183 [==============================] - 1338s 55ms/step - loss: 4.3579 - val_loss: 5.4005\n",
      "Readed\n",
      "Converted\n",
      "Train on 22949 samples, validate on 1208 samples\n",
      "Epoch 19/19\n",
      "22949/22949 [==============================] - 1269s 55ms/step - loss: 4.3441 - val_loss: 5.2592\n",
      "Readed\n",
      "Converted\n",
      "Train on 22726 samples, validate on 1197 samples\n",
      "Epoch 20/20\n",
      "22726/22726 [==============================] - 1267s 56ms/step - loss: 4.3124 - val_loss: 5.4340\n",
      "Readed\n",
      "Converted\n",
      "Train on 25515 samples, validate on 1343 samples\n",
      "Epoch 21/21\n",
      "25515/25515 [==============================] - 1409s 55ms/step - loss: 4.3965 - val_loss: 5.0567\n",
      "Readed\n",
      "Converted\n",
      "Train on 22493 samples, validate on 1184 samples\n",
      "Epoch 22/22\n",
      "22493/22493 [==============================] - 1251s 56ms/step - loss: 4.3031 - val_loss: 5.3899\n",
      "Readed\n",
      "Converted\n",
      "Train on 23548 samples, validate on 1240 samples\n",
      "Epoch 1/1\n",
      "23548/23548 [==============================] - 1313s 56ms/step - loss: 4.3604 - val_loss: 5.6372\n",
      "Readed\n",
      "Converted\n",
      "Train on 24654 samples, validate on 1298 samples\n",
      "Epoch 2/2\n",
      "24654/24654 [==============================] - 1364s 55ms/step - loss: 4.3559 - val_loss: 5.6506\n",
      "Readed\n",
      "Converted\n",
      "Train on 22035 samples, validate on 1160 samples\n",
      "Epoch 3/3\n",
      "22035/22035 [==============================] - 1215s 55ms/step - loss: 4.2910 - val_loss: 4.8988\n",
      "Readed\n",
      "Converted\n",
      "Train on 13135 samples, validate on 692 samples\n",
      "Epoch 4/4\n",
      "13135/13135 [==============================] - 727s 55ms/step - loss: 4.1840 - val_loss: 5.9074\n",
      "Readed\n",
      "Converted\n",
      "Train on 25193 samples, validate on 1326 samples\n",
      "Epoch 5/5\n",
      "25193/25193 [==============================] - 1396s 55ms/step - loss: 4.3443 - val_loss: 5.2400\n",
      "Readed\n",
      "Converted\n",
      "Train on 25993 samples, validate on 1369 samples\n",
      "Epoch 6/6\n",
      "25993/25993 [==============================] - 1434s 55ms/step - loss: 4.2764 - val_loss: 5.1337\n",
      "Readed\n",
      "Converted\n",
      "Train on 21755 samples, validate on 1146 samples\n",
      "Epoch 7/7\n",
      "15764/21755 [====================>.........] - ETA: 5:27 - loss: 4.3727"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "for j in range(1,7):\n",
    "    for i in range(22):\n",
    "        pieces_train = np.load('D:/112/decoder/data/data3000/news_pieces'+str(i)+'.npy')\n",
    "        vpwt=np.load('D:/112/decoder/data/data3000/embedings'+str(i)+'.npy')\n",
    "        shifting = np.ones(shape=(pieces_train.shape[0],512))\n",
    "        pieces_train = pieces_train+shifting\n",
    "\n",
    "        print('Readed')  \n",
    "\n",
    "\n",
    "        ptt, kol = piecesextender(pieces_train)\n",
    "        pot = np.copy(ptt)\n",
    "        pot = shift(pot, (0,-1))\n",
    "        pot = pot.reshape(pot.shape[0],512,1)\n",
    "        pos = positionencoder(kol)\n",
    "        ptt, vpwt,pos = shuffle(ptt, vpwt,pos, random_state=0)\n",
    "        print('Converted')\n",
    "\n",
    "        model.fit(\n",
    "            x=[vpwt,\n",
    "               ptt],\n",
    "            y = pot,\n",
    "            epochs=i+1, initial_epoch=i,\n",
    "            validation_split=0.05,\n",
    "            batch_size=4\n",
    "        )\n",
    "        if(i%5==0):\n",
    "            model.save('D:/112/decoder/models/m'+str(j)+'_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
