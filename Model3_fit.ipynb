{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Lambda,RepeatVector,Dense,Reshape,Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "def dimention_extender(arg, n=512):\n",
    "    from keras import backend as K\n",
    "    a= K.reshape(arg,(K.shape(arg)[0],1))\n",
    "    return K.repeat_elements(a, n, 1)\n",
    "\n",
    "def words_extender(args):\n",
    "    from keras import backend as K\n",
    "    a=RepeatVector(K.shape(args[0])[0])(args[1])\n",
    "    #print(K.shape(a))\n",
    "    #print(K.shape(args[1]))\n",
    "    b = K.cast(args[0], dtype='float32')\n",
    "    return K.dot(a,b)\n",
    "\n",
    "def padding_extender(arg):\n",
    "    from keras import backend as K\n",
    "    a = K.zeros_like(arg) - K.ones_like(arg) - K.ones_like(arg)\n",
    "    return K.dot(K.cast(arg,dtype='float32'),a)\n",
    "\n",
    "def kols_extender(arg):\n",
    "    from keras import backend as K\n",
    "    a = K.arange(K.shape(arg)[0])\n",
    "    a = K.reshape(a,(K.shape(a)[0],1))\n",
    "    out = K.dot(a,arg)#return K.dot(a,arg) #\n",
    "    return K.expand_dims(out, axis=2) #K.reshape(out, (K.shape(out)[0], K.shape(out)[1],1))\n",
    "\n",
    "def output_filter(args):\n",
    "    from keras import backend as K\n",
    "    a = dimention_extender(arg[0], n=100)\n",
    "    return K.dot(a,args[1])\n",
    "\n",
    "def output_concatenator(args):\n",
    "    from keras import backend as K\n",
    "    a = K.zeros_like(args[0])\n",
    "    a += args[0]+args[1]+args[2]\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_position_encoding(arg, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    import math\n",
    "    from keras import backend as K\n",
    "    \n",
    "    length=K.shape(arg)\n",
    "    hidden_size=100\n",
    "    \n",
    "    position = K.cast(K.range(length), K.float32)\n",
    "    num_timescales = hidden_size // 2\n",
    "    log_timescale_increment = (\n",
    "          math.log(float(max_timescale) / float(min_timescale)) /\n",
    "          (K.cast(num_timescales, tf.float32) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "          tf.cast(K.range(num_timescales), K.float32) * -log_timescale_increment)\n",
    "    scaled_time = K.expand_dims(position, 1) * K.expand_dims(inv_timescales, 0)\n",
    "    signal = K.concat([K.sin(scaled_time), K.cos(scaled_time)], axis=1)\n",
    "    return signal + arg\n",
    "\n",
    "def reshape(arg):\n",
    "    from keras import backend as K\n",
    "    return K.reshape(arg,(K.shape(arg)[0]*K.shape(arg)[1],K.shape(arg)[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_transformer import *\n",
    "\n",
    "import numpy as np\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def get_m(token_num,\n",
    "              embed_dim,\n",
    "              encoder_num,\n",
    "              decoder_num,\n",
    "              head_num,\n",
    "              hidden_dim,\n",
    "              attention_activation=None,\n",
    "              feed_forward_activation='relu',\n",
    "              dropout_rate=0.0,\n",
    "              embed_weights =None,\n",
    "              embed_trainable=None,\n",
    "              trainable=True,\n",
    "              use_adapter=False,\n",
    "              adapter_units=None,\n",
    "              adapter_activation='relu'):\n",
    "    \"\"\"Get full model without compilation.\n",
    "    :param token_num: Number of distinct tokens.\n",
    "    :param embed_dim: Dimension of token embedding.\n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n",
    "                           `embed_trainable` should be lists of two elements if it is False.\n",
    "    :param embed_weights: Initial weights of token embedding.\n",
    "    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n",
    "                            value is None when embedding weights has been provided.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :param use_adapter: Whether to use feed-forward adapters before each residual connections.\n",
    "    :param adapter_units: The dimension of the first transformation in feed-forward adapter.\n",
    "    :param adapter_activation: The activation after the first transformation in feed-forward adapter.\n",
    "    :return: Keras model.\n",
    "    \"\"\"\n",
    "    decoder_token_num = token_num\n",
    "\n",
    "    decoder_embed_weights = embed_weights\n",
    "\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    "\n",
    "    decoder_embed_trainable = embed_trainable\n",
    "\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    "\n",
    "\n",
    "    decoder_embed_layer = EmbeddingRet(\n",
    "        input_dim=decoder_token_num,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        weights=decoder_embed_weights,\n",
    "        trainable=decoder_embed_trainable,\n",
    "        name='Decoder-Token-Embedding',\n",
    "    )\n",
    "    \n",
    "    \n",
    "    vector = Input(shape=(512,), name='Vectors-Input',dtype='float32')\n",
    "    pos_words = Input(shape=(None,), name='Position-Words',dtype='int32')\n",
    "    pos_pads = Input(shape=(None,), name='Position-Paddings',dtype='int32')\n",
    "    pos_end = Input(shape=(None,), name='Position-End',dtype='int32')\n",
    "    \n",
    "    pos_words_dim_ex = Lambda(dimention_extender, name='Position-Words-Dimentions-Extender')(pos_words)\n",
    "    pos_pads_dim_ex = Lambda(dimention_extender, name='Position-Paddings-Dimentions-Extender',arguments={'n':100})(pos_pads)\n",
    "    pos_ens_ex = Lambda(dimention_extender, name='Position-End-Dimentions-Extender',arguments={'n':100})(pos_end)    \n",
    "    \n",
    "    pos_pads_ex = Lambda(padding_extender, name='Padding-Extender')(pos_pads_dim_ex)\n",
    "    pos_vec_ex = Lambda(words_extender, name='Words-Extender')([pos_words_dim_ex,vector])\n",
    "    pos_word_ex = Lambda(kols_extender, name='Numbers-Extender')(pos_words)\n",
    "    \n",
    "    pos_vec_ex_r = Lambda(reshape, name='Reshape_')(pos_vec_ex)\n",
    "    pos_word_ex_r = Lambda(reshape, name='Reshape')(pos_word_ex)\n",
    "    print(K.shape(pos_vec_ex))\n",
    "    print(K.shape(pos_vec_ex_r))\n",
    "    encoder_shifter_model = load_model('D:/112/decoder/models/sen_to_word2_0.h5') \n",
    "    garbedge, encoder_shifter_layer  = encoder_shifter_model([pos_vec_ex_r,pos_word_ex_r])\n",
    "    #encoder_shifter_layer.trainable = False\n",
    "    encoded_words= encoder_shifter_layer.outputs[1]\n",
    "    \n",
    "    encoded_words_filter = Lambda(output_filter, name='Encoded_Words_Filter')([pos_words,encoded_words])\n",
    "    encoder_concatenator = Lambda(output_concatenator, name='Encoder_Concatenator')([encoded_words_filter,pos_pads_ex,pos_ens_ex])\n",
    "    pos_wised_encoder = Lambda(get_position_encoding, name='Encoder_Concatenator')(encoder_concatenator)\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoded_layer = get_encoders(\n",
    "        encoder_num=encoder_num,\n",
    "        input_layer=pos_wised_encoder,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input') \n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=encoded_layer,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    dense_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    return keras.models.Model(inputs=[vector,n_words,t_words,decoder_input], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def piecesextender(data):\n",
    "    kol= np.zeros(shape=(data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        kol[i]=np.count_nonzero(data[i])\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    buf = 0\n",
    "    for g in range(data.shape[0]):        \n",
    "        for k in range(1,kol[g]-1):\n",
    "            out[buf][0]=3000\n",
    "            out[buf][1:513-k]+=data[g][k:]\n",
    "            buf+=1\n",
    "    return out, kol\n",
    "\n",
    "def vectorsextender(data,kol):\n",
    "    out = np.zeros(shape=(np.sum(kol)-2*data.shape[0],512))\n",
    "    buf=0\n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(kol[g]-2):\n",
    "            out[buf]+=data[g]\n",
    "            buf+=1\n",
    "    return out\n",
    "\n",
    "def kolextender(kol):\n",
    "    out = np.zeros(shape=(np.sum(kol)-2*kol.shape[0],512))\n",
    "    out2 = np.zeros(shape=(np.sum(kol)-2*kol.shape[0],512))\n",
    "    out3 = np.zeros(shape=(np.sum(kol)-2*kol.shape[0],512))\n",
    "    buf = 0\n",
    "    for g in range(kol.shape[0]):\n",
    "        for k in range(1, kol[g]-1):\n",
    "            out[buf][1:k+1]=np.ones(shape=(k))\n",
    "            out2[buf][k+3:]=np.ones(shape=(512-k-2))\n",
    "            out3[buf]=to_catigorical(k+1,512)\n",
    "            buf+=1\n",
    "    return out, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Tensor(\"Shape:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f10fe04b51ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0membed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3003\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0muse_adapter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m model.compile(\n",
      "\u001b[1;32m<ipython-input-2-84bfdddbd600>\u001b[0m in \u001b[0;36mget_m\u001b[1;34m(token_num, embed_dim, encoder_num, decoder_num, head_num, hidden_dim, attention_activation, feed_forward_activation, dropout_rate, embed_weights, embed_trainable, trainable, use_adapter, adapter_units, adapter_activation)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_vec_ex_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mencoder_shifter_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/112/decoder/models/sen_to_word2_0.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mgarbedge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_shifter_layer\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mencoder_shifter_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos_vec_ex_r\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_word_ex_r\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;31m#encoder_shifter_layer.trainable = False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mencoded_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mencoder_shifter_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0moutput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[1;34m(self, inputs, masks)\u001b[0m\n\u001b[0;32m    783\u001b[0m                         input_shapes = unpack_singleton(\n\u001b[0;32m    784\u001b[0m                             [x._keras_shape for x in computed_tensors])\n\u001b[1;32m--> 785\u001b[1;33m                         \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m                         uses_learning_phase = any(\n\u001b[0;32m    787\u001b[0m                             [x._uses_learning_phase for x in computed_tensors])\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m         \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_m(\n",
    "    token_num=3003,\n",
    "    embed_dim=100,\n",
    "    encoder_num=3,\n",
    "    decoder_num=6,\n",
    "    head_num=4,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((3003, 100)),\n",
    "    use_adapter=True,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "model.summary()\n",
    "model.save('D:/decoder/models/dec_buf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "0.001\n",
      "0.0002\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model ('D:/decoder/models/m5.h5' , custom_objects = get_custom_objects ())\n",
    "print(K.get_value(model.optimizer.lr))\n",
    "K.set_value(model.optimizer.lr, .0002)\n",
    "print(K.get_value(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed\n",
      "Train on 5453 samples, validate on 287 samples\n",
      "Epoch 1/1\n",
      "5453/5453 [==============================] - 433s 79ms/step - loss: 7.1631 - val_loss: 3.8508\n",
      "Train on 4615 samples, validate on 243 samples\n",
      "Epoch 2/2\n",
      "4615/4615 [==============================] - 355s 77ms/step - loss: 5.7817 - val_loss: 3.7864\n",
      "Train on 4569 samples, validate on 241 samples\n",
      "Epoch 3/3\n",
      "4569/4569 [==============================] - 351s 77ms/step - loss: 5.5983 - val_loss: 3.4127\n",
      "Train on 5488 samples, validate on 289 samples\n",
      "Epoch 4/4\n",
      "5488/5488 [==============================] - 423s 77ms/step - loss: 5.6053 - val_loss: 3.6657\n",
      "Train on 4372 samples, validate on 231 samples\n",
      "Epoch 5/5\n",
      "4372/4372 [==============================] - 337s 77ms/step - loss: 5.2134 - val_loss: 3.1306\n",
      "Readed\n",
      "Train on 5344 samples, validate on 282 samples\n",
      "Epoch 6/6\n",
      "5344/5344 [==============================] - 412s 77ms/step - loss: 5.5773 - val_loss: 4.2312\n",
      "Train on 5092 samples, validate on 268 samples\n",
      "Epoch 7/7\n",
      "5092/5092 [==============================] - 391s 77ms/step - loss: 5.5280 - val_loss: 3.1595\n",
      "Train on 5563 samples, validate on 293 samples\n",
      "Epoch 8/8\n",
      "5563/5563 [==============================] - 429s 77ms/step - loss: 5.4457 - val_loss: 3.3203\n",
      "Train on 4247 samples, validate on 224 samples\n",
      "Epoch 9/9\n",
      "4247/4247 [==============================] - 326s 77ms/step - loss: 5.3103 - val_loss: 3.7233\n",
      "Train on 5357 samples, validate on 282 samples\n",
      "Epoch 10/10\n",
      "5357/5357 [==============================] - 411s 77ms/step - loss: 5.4983 - val_loss: 4.7454\n",
      "Readed\n",
      "Train on 5131 samples, validate on 271 samples\n",
      "Epoch 11/11\n",
      "5131/5131 [==============================] - 401s 78ms/step - loss: 5.5298 - val_loss: 3.0636\n",
      "Train on 5918 samples, validate on 312 samples\n",
      "Epoch 12/12\n",
      "5918/5918 [==============================] - 455s 77ms/step - loss: 5.8007 - val_loss: 3.9049\n",
      "Train on 4452 samples, validate on 235 samples\n",
      "Epoch 13/13\n",
      "4452/4452 [==============================] - 343s 77ms/step - loss: 5.3438 - val_loss: 3.2720\n",
      "Train on 4645 samples, validate on 245 samples\n",
      "Epoch 14/14\n",
      "4645/4645 [==============================] - 357s 77ms/step - loss: 5.1749 - val_loss: 2.5946\n",
      "Train on 2836 samples, validate on 150 samples\n",
      "Epoch 15/15\n",
      "2836/2836 [==============================] - 219s 77ms/step - loss: 5.2063 - val_loss: 1.2184\n",
      "Readed\n",
      "Train on 2452 samples, validate on 130 samples\n",
      "Epoch 16/16\n",
      "2452/2452 [==============================] - 189s 77ms/step - loss: 4.9928 - val_loss: 0.8456\n",
      "Train on 2401 samples, validate on 127 samples\n",
      "Epoch 17/17\n",
      "2401/2401 [==============================] - 185s 77ms/step - loss: 4.9948 - val_loss: 0.9626\n",
      "Train on 2326 samples, validate on 123 samples\n",
      "Epoch 18/18\n",
      "2326/2326 [==============================] - 179s 77ms/step - loss: 4.6987 - val_loss: 0.7286\n",
      "Train on 2794 samples, validate on 148 samples\n",
      "Epoch 19/19\n",
      "2794/2794 [==============================] - 216s 77ms/step - loss: 5.0200 - val_loss: 1.2616\n",
      "Train on 4109 samples, validate on 217 samples\n",
      "Epoch 20/20\n",
      "4109/4109 [==============================] - 317s 77ms/step - loss: 5.5679 - val_loss: 3.1079\n",
      "Readed\n",
      "Train on 5033 samples, validate on 265 samples\n",
      "Epoch 21/21\n",
      "5033/5033 [==============================] - 392s 78ms/step - loss: 5.5645 - val_loss: 3.3683\n",
      "Train on 4720 samples, validate on 249 samples\n",
      "Epoch 22/22\n",
      "4720/4720 [==============================] - 363s 77ms/step - loss: 5.3993 - val_loss: 3.2118\n",
      "Train on 5108 samples, validate on 269 samples\n",
      "Epoch 23/23\n",
      "5108/5108 [==============================] - 395s 77ms/step - loss: 5.3485 - val_loss: 3.6247\n",
      "Train on 5851 samples, validate on 308 samples\n",
      "Epoch 24/24\n",
      "5851/5851 [==============================] - 452s 77ms/step - loss: 5.7880 - val_loss: 4.1121\n",
      "Train on 5430 samples, validate on 286 samples\n",
      "Epoch 25/25\n",
      "5430/5430 [==============================] - 419s 77ms/step - loss: 5.5839 - val_loss: 3.6536\n",
      "Readed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2f299f7ad13d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mpot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mvtt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorsextender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositionencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-428e74472fe8>\u001b[0m in \u001b[0;36mpositionencoder\u001b[1;34m(kol)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m#out[buf]+=binarycalculator(k+1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mbinarycalculator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mbuf\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-428e74472fe8>\u001b[0m in \u001b[0;36mbinarycalculator\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbinarycalculator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mbuf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumbertoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "n=0\n",
    "e=1\n",
    "bo = True\n",
    "\n",
    "for i in range(1000):\n",
    "    vectors_train = np.load('D:/decoder/data/data3000/news_vectors'+str(i)+'.npy')\n",
    "    pieces_train = np.load('D:/decoder/data/data3000/news_pieces'+str(i)+'.npy')\n",
    "    shifting = np.ones(shape=(pieces_train.shape[0],512))\n",
    "    pieces_train = pieces_train+shifting\n",
    "    \n",
    "    print('Readed')  \n",
    "        \n",
    "    for j in range(5):\n",
    "        ptt, kol = piecesextender(pieces_train[0+j*100:(1+j)*100])\n",
    "        pot = np.copy(ptt)\n",
    "        pot = shift(pot, (0,-1))\n",
    "        pot = pot.reshape(pot.shape[0],512,1)\n",
    "        vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "        nt, tt = kolextender(kol)\n",
    "        \n",
    "        \n",
    "        model.fit(\n",
    "            x=[vtt,\n",
    "               nt,\n",
    "               tt,\n",
    "               ptt],\n",
    "            y = pot,\n",
    "            epochs=e, initial_epoch=n,\n",
    "            validation_split=0.05,\n",
    "            batch_size=6\n",
    "        )\n",
    "        n+=1\n",
    "        e+=1\n",
    "\n",
    "    if i%5==0 and bo:\n",
    "        model.save('D:/decoder/models/m'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/decoder/models/m4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_position_encoding(\n",
    "    length, hidden_size, min_timescale=1.0, max_timescale=1.0e4):\n",
    "  \"\"\"Return positional encoding.\n",
    "  Calculates the position encoding as a mix of sine and cosine functions with\n",
    "  geometrically increasing wavelengths.\n",
    "  Defined and formulized in Attention is All You Need, section 3.5.\n",
    "  Args:\n",
    "    length: Sequence length.\n",
    "    hidden_size: Size of the\n",
    "    min_timescale: Minimum scale that will be applied at each position\n",
    "    max_timescale: Maximum scale that will be applied at each position\n",
    "  Returns:\n",
    "    Tensor with shape [length, hidden_size]\n",
    "  \"\"\"\n",
    "  # We compute the positional encoding in float32 even if the model uses\n",
    "  # float16, as many of the ops used, like log and exp, are numerically unstable\n",
    "  # in float16.\n",
    "  position = tf.cast(tf.range(length), tf.float32)\n",
    "  num_timescales = hidden_size // 2\n",
    "  log_timescale_increment = (\n",
    "      math.log(float(max_timescale) / float(min_timescale)) /\n",
    "      (tf.cast(num_timescales, tf.float32) - 1))\n",
    "  inv_timescales = min_timescale * tf.exp(\n",
    "      tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n",
    "  scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "  return signal\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
