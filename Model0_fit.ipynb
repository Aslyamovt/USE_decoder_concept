{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras_transformer import *\n",
    "\n",
    "import numpy as np\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "import keras\n",
    "\n",
    "def get_m(token_num,\n",
    "              embed_dim,\n",
    "              decoder_num,\n",
    "              head_num,\n",
    "              hidden_dim,\n",
    "              attention_activation=None,\n",
    "              feed_forward_activation='relu',\n",
    "              dropout_rate=0.0,\n",
    "              embed_weights =None,\n",
    "              embed_trainable=None,\n",
    "              trainable=True,\n",
    "              use_adapter=False,\n",
    "              adapter_units=None,\n",
    "              adapter_activation='relu'):\n",
    "    \"\"\"Get full model without compilation.\n",
    "    :param token_num: Number of distinct tokens.\n",
    "    :param embed_dim: Dimension of token embedding.\n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n",
    "                           `embed_trainable` should be lists of two elements if it is False.\n",
    "    :param embed_weights: Initial weights of token embedding.\n",
    "    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n",
    "                            value is None when embedding weights has been provided.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :param use_adapter: Whether to use feed-forward adapters before each residual connections.\n",
    "    :param adapter_units: The dimension of the first transformation in feed-forward adapter.\n",
    "    :param adapter_activation: The activation after the first transformation in feed-forward adapter.\n",
    "    :return: Keras model.\n",
    "    \"\"\"\n",
    "    decoder_token_num = token_num\n",
    "\n",
    "    decoder_embed_weights = embed_weights\n",
    "\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    "\n",
    "    decoder_embed_trainable = embed_trainable\n",
    "\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    "\n",
    "\n",
    "    decoder_embed_layer = EmbeddingRet(\n",
    "        input_dim=decoder_token_num,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        weights=decoder_embed_weights,\n",
    "        trainable=decoder_embed_trainable,\n",
    "        name='Decoder-Token-Embedding',\n",
    "    )\n",
    "    encoder_output = keras.layers.Input(shape=(510,512), name='Encoder-Output')\n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input')\n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=encoder_output ,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    dense_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    return keras.models.Model(inputs=[encoder_output, decoder_input], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecesextender(data):\n",
    "    out = np.zeros(shape=(data.shape[0],510,512))  \n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(1,511):\n",
    "            for pk in range(512-k):\n",
    "                out[g][k-1][pk] += data[g][k+pk]\n",
    "    return out\n",
    "\n",
    "def vectorsextender(data):\n",
    "    data = data.reshape(data.shape[0],1,512)\n",
    "    out = np.zeros(shape=(data.shape[0],510,512))\n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(510):\n",
    "            out[g][k]=out[g][k]+data[g]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Token-Embedding (Embedd [(None, None, 512),  5121536     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 512)    0           Decoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (InputLayer)     (None, 510, 512)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output[0][0]             \n",
      "                                                                 Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 512)    123512      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 512)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 512)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output[0][0]             \n",
      "                                                                 Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 512)    123512      Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 512)    0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 512)    0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output[0][0]             \n",
      "                                                                 Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward (FeedForw (None, None, 512)    123512      Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Dropout ( (None, None, 512)    0           Decoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Add (Add) (None, None, 512)    0           Decoder-3-MultiHeadQueryAttention\n",
      "                                                                 Decoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output[0][0]             \n",
      "                                                                 Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward (FeedForw (None, None, 512)    123512      Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Dropout ( (None, None, 512)    0           Decoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Add (Add) (None, None, 512)    0           Decoder-4-MultiHeadQueryAttention\n",
      "                                                                 Decoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 10003)  10003       Decoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-Token-Embedding[0][1]    \n",
      "==================================================================================================\n",
      "Total params: 14,042,867\n",
      "Trainable params: 8,921,331\n",
      "Non-trainable params: 5,121,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "model = get_m(\n",
    "    token_num=10003,\n",
    "    embed_dim=512,\n",
    "    decoder_num=4,\n",
    "    head_num=4,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((10003, 512)),\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (999951, 512) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3d393bf43c6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mshifting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpieces_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mvectors_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectors_train\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshifting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mpieces_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpieces_train\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshifting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mpieces_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpieces_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (999951, 512) and data type float64"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(10800) \n",
    "\n",
    "n=0\n",
    "e=1\n",
    "vectors_train = np.load('D:/decoder/news_vectors0.npy')\n",
    "pieces_train = np.load('D:/decoder/news_pieces0.npy')\n",
    "\n",
    "shifting = np.ones(shape=(pieces_train.shape[0],512))\n",
    "vectors_train = vectors_train + shifting\n",
    "pieces_train = pieces_train + shifting\n",
    "pieces_output = np.copy(pieces_train)\n",
    "pieces_output=shift(pieces_output, (0,-1)).reshape(pieces_output.shape[0],510,512,1)\n",
    "\n",
    "b=0\n",
    "for j in range(pieces_train.shape[0]//10000-1):\n",
    "\n",
    "    vtt = vectorsextender(vectors_train[0+j*10000:(1+j)*10000])\n",
    "    ptt = piecesextender(pieces_train[0+j*10000:(1+j)*10000])\n",
    "    pot = piecesextender(pieces_output[0+j*10000:(1+j)*10000])\n",
    "\n",
    "    model.fit(\n",
    "        x=[vtt, ptt],\n",
    "        y=pot,\n",
    "        epochs=e, initial_epoch=n,\n",
    "        validation_data=0.05, batch_size=4\n",
    "    )\n",
    "    n+=1\n",
    "    e+=1\n",
    "    b=j\n",
    "    model.save('D:/decoder/models/m'+str(e)+'.h5')\n",
    "\n",
    "vtt = vectorsextender(vectors_train[b:])\n",
    "ptt = piecesextender(pieces_train[b:])\n",
    "pot = piecesextender(pieces_output[b:])\n",
    "\n",
    "model.fit(\n",
    "    x=[vtt, ptt],\n",
    "    y=pot,\n",
    "    epochs=e, initial_epoch=n,\n",
    "    validation_data=0.05, batch_size=4\n",
    ")\n",
    "n+=1\n",
    "e+=1\n",
    "model.save('D:/decoder/models/m'+str(e)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1022755.  1042131.  1078044.  1082064.  1115036.  1161369.  1260059.\n",
      "  1270255.  1279637.  1286955.  1388041.  1396189.  1401422.  1401663.\n",
      "  1415694.  1415696.  1415698.  1416900.  1419849.  1420031.  1440518.\n",
      "  1457122.  1507462.  1520227.  1521007.  1529788.  1608574.  1622669.\n",
      "  1625725.  1646831.  1646833.  1654706.  1704910.  1708546.  1718634.\n",
      "  1719483.  1722989.  1754747.  1793042.  1845374.  1912779.  1912795.\n",
      "  1922676.  1928482.  1942566.  1950455.  1967802.  1977786.  1991170.\n",
      "  2038347.  2039692.  2051912.  2062370.  2068460.  2068998.  2089804.\n",
      "  2098758.  2111489.  2114189.  2130516.  2134042.  2134693.  2141407.\n",
      "  2148918.  2161945.  2177482.  2178546.  2189332.  2224814.  2231754.\n",
      "  2270801.  2311762.  2313154.  2372946.  2431071.  2436406.  2444467.\n",
      "  2446077.  2465667.  2480937.  2482131.  2567922.  2601638.  2604393.\n",
      "  2604406.  2604435.  2606385.  2633741.  2637454.  2661026.  2662490.\n",
      "  2668090.  2668113.  2712238.  2714823.  2765661.  2771962.  2839466.\n",
      "  2843523.  2845574.  2860075.  2871190.  2908916.  2908918.  2909108.\n",
      "  2909110.  2934885.  2938442.  2979177.  2987292.  2991227.  2995333.\n",
      "  2997208.  3008765.  3041584.  3051460.  3072141.  3081027.  3120177.\n",
      "  3134161.  3144866.  3152246.  3165046.  3182902.  3207858.  3216916.\n",
      "  3230504.  3264286.  3333356.  3334790.  3362956.  3377122.  3380679.\n",
      "  3441067.  3442009.  3456891.  3486089.  3496496.  3513876.  3516880.\n",
      "  3552331.  3552339.  3552348.  3588924.  3588926.  3588929.  3607679.\n",
      "  3617863.  3693299.  3705215.  3708020.  3716957.  3726741.  3735793.\n",
      "  3791644.  3795545.  3796986.  3802649.  3825665.  3826472.  3830177.\n",
      "  3831662.  3909116.  3909907.  3919758.  3924546.  3929028.  3956604.\n",
      "  3958081.  3978095.  4022306.  4040234.  4061953.  4095607.  4102639.\n",
      "  4108618.  4124481.  4134659.  4166168.  4178980.  4248988.  4255346.\n",
      "  4269669.  4288236.  4300017.  4307353.  4317286.  4330823.  4376878.\n",
      "  4403842.  4407089.  4431251.  4460601.  4476003.  4504363.  4527320.\n",
      "  4554921.  4578387.  4580759.  4585481.  4595399.  4613762.  4630076.\n",
      "  4648388.  4666818.  4667460.  4671662.  4676048.  4677937.  4720209.\n",
      "  4732359.  4742362.  4754562.  4755122.  4757760.  4765709.  4775810.\n",
      "  4791875.  4806688.  4825479.  4825694.  4826237.  4827335.  4854078.\n",
      "  4893739.  4895984.  4972934.  5001572.  5053590.  5058486.  5089903.\n",
      "  5138352.  5158602.  5168745.  5178159.  5216671.  5228827.  5251177.\n",
      "  5268468.  5275222.  5289841.  5300806.  5302903.  5321938.  5339215.\n",
      "  5341376.  5344620.  5353208.  5393614.  5442210.  5444058.  5445074.\n",
      "  5453038.  5461582.  5463754.  5466336.  5472513.  5479802.  5479866.\n",
      "  5517726.  5523955.  5528163.  5528179.  5552774.  5571959.  5633063.\n",
      "  5656031.  5696721.  5761280.  5766151.  5798809.  5801120.  5850381.\n",
      "  5850389.  5858361.  5884549.  5884693.  5891079.  5891151.  5901755.\n",
      "  5924351.  5924357.  5924359.  5926666.  5926741.  5928311.  5966310.\n",
      "  5983568.  5983939.  5985756.  5990069.  6012978.  6091505.  6093547.\n",
      "  6100904.  6103996.  6108444.  6129195.  6141896.  6163947.  6171627.\n",
      "  6171633.  6171635.  6171637.  6171638.  6171640.  6171643.  6171646.\n",
      "  6171650.  6171652.  6171655.  6171659.  6171663.  6171665.  6183846.\n",
      "  6183933.  6237516.  6305421.  6310428.  6318827.  6318841.  6323673.\n",
      "  6406122.  6464888.  6464892.  6488374.  6557936.  6661477.  6680090.\n",
      "  6787276.  6794005.  6797591.  6856104.  6894258.  6975481.  6989060.\n",
      "  7081202.  7082845.  7093742.  7093888.  7114327.  7125733.  7135949.\n",
      "  7148592.  7172784.  7172971.  7187851.  7208638.  7212521.  7231882.\n",
      "  7240303.  7255683.  7298694.  7312884.  7312911.  7364058.  7417269.\n",
      "  7417548.  7422730.  7424337.  7439999.  7440211.  7521931.  7532998.\n",
      "  7554289.  7601122.  7629098.  7631908.  7634602.  7638766.  7639568.\n",
      "  7657864.  7691005.  7709346.  7735413.  7750439.  7771525.  7813673.\n",
      "  7819614.  7838952.  7849259.  7895561.  7899318.  7953098.  7957290.\n",
      "  7962508.  7975000.  8030705.  8059258.  8079849.  8110172.  8201517.\n",
      "  8206182.  8267579.  8279234.  8284048.  8288469.  8292501.  8293203.\n",
      "  8333555.  8341452.  8349292.  8349448.  8404187.  8464471.  8474522.\n",
      "  8493473.  8494341.  8497252.  8507717.  8530488.  8585724.  8628321.\n",
      "  8631425.  8641415.  8649095.  8657328.  8689088.  8693803.  8702273.\n",
      "  8711209.  8719363.  8724554.  8754869.  8755484.  8755997.  8756043.\n",
      "  8757011.  8757909.  8757910.  8759659.  8763298.  8770329.  8818032.\n",
      "  8847695.  8860756.  8894578.  8995924.  9046634.  9055210.  9061053.\n",
      "  9062157.  9087773.  9090992.  9099046.  9104249.  9214271.  9314640.\n",
      "  9323344.  9337859.  9360557.  9366710.  9411268.  9419494.  9427792.\n",
      "  9645710.  9682891.  9714669.  9716441.  9792016.  9817259.  9868929.\n",
      "  9886040.  9922451.  9934494. 10080148. 10134312. 10145453. 10153245.\n",
      " 10169460. 10251975. 10254798. 10260607. 10277347. 10280894. 10353205.\n",
      " 10353207. 10353208. 10353209. 10363228. 10369524. 10378091. 10409267.\n",
      " 10409513. 10421816. 10426884. 10435594. 10460979. 10462844. 10463332.\n",
      " 10477695. 10483832. 10511115. 10513512. 10517187. 10518321. 10528796.\n",
      " 10544466. 10554421. 10555592. 10579496. 10588422. 10628900. 10673378.\n",
      " 10673495. 10673496. 10675629. 10731145. 10766864. 10772763. 10782192.\n",
      " 10783451. 10790781. 10802944. 10824902. 10896191. 10913978. 10913979.\n",
      " 10945178. 10992077. 11003184. 11007323. 11031022. 11036017. 11057108.\n",
      " 11065040. 11102499. 11102502. 11102506. 11102509. 11126655. 11150768.\n",
      " 11190218. 11278509. 11296082. 11302875. 11347329. 11347869. 11359959.\n",
      " 11373849. 11381450. 11383478. 11389402. 11402515. 11414414. 11414418.\n",
      " 11416218. 11433727. 11443174. 11445223. 11457242. 11513846. 11517602.\n",
      " 11518920. 11535420. 11539739. 11641723. 11652304. 11664284. 11678017.\n",
      " 11680116. 11695279. 11697926. 11714721. 11727646. 11729438. 11756145.\n",
      " 11766122. 11783554. 11828345. 11862644. 11886751. 11888387. 11905179.\n",
      " 11973756. 11988099. 11993093. 12056467. 12056559. 12085888. 12147958.\n",
      " 12164529. 12166864. 12186907. 12192026. 12195734. 12195830. 12196942.\n",
      " 12201539. 12204967. 12231180. 12242472. 12243703. 12264767. 12273135.\n",
      " 12324343. 12325856. 12336768. 12336942. 12402083. 12402537. 12423297.\n",
      " 12426525. 12500554. 12535812. 12537457. 12544605. 12571276. 12594133.\n",
      " 12626051. 12645331. 12648519. 12690452. 12706634. 12709380. 12712233.\n",
      " 12717720. 12717721. 12723150. 12723151. 12727588. 12728109. 12728406.\n",
      " 12733886. 12741863. 12772845. 12779014. 12837597. 12841043. 12872457.\n",
      " 12875213. 12963465. 13093153. 13105431. 13158351. 13192585. 13245953.\n",
      " 13245997. 13293514. 13316091. 13328398. 13338513. 13339107. 13353731.\n",
      " 13358424. 13385320. 13487516. 13535018. 13541258. 13547839. 13570055.\n",
      " 13585161. 13587903. 13587904. 13590457. 13607948. 13616181. 13616961.\n",
      " 13619898. 13622495. 13622496. 13622498. 13624172. 13716183. 13756957.\n",
      " 13760362. 13760531. 13767355. 13795094. 13845040. 13845995. 13864018.\n",
      " 13872225. 13893263. 13897163. 13897169. 13912157. 13917343. 13922433.\n",
      " 13938985. 13947153. 13948260. 13949661. 13969472. 13999633.]\n"
     ]
    }
   ],
   "source": [
    "ids = np.load('D:/decoder/ids.npy')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
