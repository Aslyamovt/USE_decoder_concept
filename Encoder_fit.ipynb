{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y):\n",
    "    from keras import backend as k\n",
    "    return k.one_hot(y, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\DNS\\Anaconda3\\envs\\decoder\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DNS\\Anaconda3\\envs\\decoder\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DNS\\Anaconda3\\envs\\decoder\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DNS\\Anaconda3\\envs\\decoder\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DNS\\Anaconda3\\envs\\decoder\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DNS\\Anaconda3\\envs\\decoder\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Concatenate,Lambda,Reshape,Dropout\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "vector = Input(shape=(512,), name='Vectors-Input',dtype='float32')\n",
    "vector_shiter = Dense(512)(vector)\n",
    "position = Input(shape=(1,), name='Positions-Input',dtype='int32')\n",
    "categorical_position=Lambda(to_categorical, name='Positional_encoding')(position)\n",
    "reshaped_categorical_position=Reshape((512,))(categorical_position)\n",
    "concatenated_encoder_input=Concatenate()([vector,reshaped_categorical_position])\n",
    "encoder_input_divider1 = Dense(1024, name='Encoder-Output-Divider-1',activation='selu')(concatenated_encoder_input)\n",
    "encoder_input_dropout1=Dropout(0.1, name='Encoder-Output-Dropout-1')(encoder_input_divider1)\n",
    "encoder_input_divider2 = Dense(512, name='Encoder-Output-Divider-2',activation='selu')(encoder_input_dropout1)\n",
    "encoder_input_dropout2=Dropout(0.1, name='Encoder-Output-Dropout-2')(encoder_input_divider2)\n",
    "concatenated_encoder_input1=Concatenate()([vector_shiter,encoder_input_dropout2])\n",
    "encoder_input_divider3 = Dense(256, name='Encoder-Output-Divider-3',activation='selu')(concatenated_encoder_input1)\n",
    "encoder_input_dropout3=Dropout(0.1, name='Encoder-Output-Dropout-3')(encoder_input_divider3)\n",
    "output_vector = Dense(100, name='Encoder-Output',activation='selu')(encoder_input_dropout3)\n",
    "output_vector2 = Dense(100, name='Encoder-Garbedge',activation='selu')(encoder_input_dropout3)\n",
    "\n",
    "model = Model(inputs=[vector,position],outputs=[output_vector,output_vector2])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_absolute_error',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_ru = BPEmb(lang=\"ru\", dim=100, vs=3000)\n",
    "\n",
    "def textreader(st):\n",
    "    texts = []\n",
    "    with open(\"C:/Users/DNS/Desktop/Progi/Python/NeuralNetworks/Курсач/data/np_rev.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        i=0\n",
    "        for line in fr:\n",
    "            i+=1\n",
    "            if i<st:      \n",
    "                continue\n",
    "            texts.append(line.replace('\\n',''))\n",
    "            if i==st+499:\n",
    "                break\n",
    "    return texts\n",
    "\n",
    "def word_vectors_creator(data):\n",
    "    kol= np.zeros(shape=(len(data)))\n",
    "    pieces=[]\n",
    "    for i in range(len(data)):\n",
    "        a = bpemb_ru.encode(data[i])\n",
    "        s = bpemb_ru.encode_ids(data[i])\n",
    "        b =[]\n",
    "        for f in range(len(a)):\n",
    "            if s[f] != 0:\n",
    "                b.append(a[f])\n",
    "        kol[i]+=len(b)\n",
    "        pieces.append(b)\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(np.sum(kol),100))\n",
    "    out2 = np.zeros(shape=(np.sum(kol),100))\n",
    "    buf = 0\n",
    "    for g in range(len(data)):        \n",
    "        for k in range(0,kol[g]):\n",
    "            out[buf]+=np.asarray(bpemb_ru.emb.get_vector(pieces[g][k]))\n",
    "            for m in range(kol[g]):\n",
    "                if m!=g:\n",
    "                    out2[buf]+=(np.asarray(bpemb_ru.emb.get_vector(pieces[g][m]))/kol[g])\n",
    "            buf+=1\n",
    "    return out, kol, out2\n",
    "\n",
    "def vectorsextender(data,kol):\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    out2 = np.zeros(shape=(np.sum(kol)))\n",
    "    buf=0\n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(kol[g]):\n",
    "            out[buf]+=data[g]\n",
    "            #out2[buf]+=np.sum(data[g])/512\n",
    "            buf+=1\n",
    "    return out#, out2\n",
    "\n",
    "def positioncreator(kol):\n",
    "    out = np.zeros(shape=(np.sum(kol)))\n",
    "    buf=0\n",
    "    for g in range(len(kol)):\n",
    "        out[buf:buf+kol[g]]+=np.arange(1,kol[g]+1)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'D:/decoder/models/sen_to_word0.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-26d8fc1a2792>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'D:/decoder/models/sen_to_word0.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'D:/decoder/models/sen_to_word0.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model ('D:/decoder/models/sen_to_word0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "**********************************\n",
      "\n",
      "Train on 1803 samples, validate on 95 samples\n",
      "Epoch 1/1\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.3509 - Encoder-Output_loss: 0.2642 - Encoder-Garbedge_loss: 0.0867 - val_loss: 0.3242 - val_Encoder-Output_loss: 0.2470 - val_Encoder-Garbedge_loss: 0.0771\n",
      "Train on 2162 samples, validate on 114 samples\n",
      "Epoch 21/21\n",
      "2162/2162 [==============================] - 17s 8ms/step - loss: 0.2743 - Encoder-Output_loss: 0.2425 - Encoder-Garbedge_loss: 0.0319 - val_loss: 0.2901 - val_Encoder-Output_loss: 0.2400 - val_Encoder-Garbedge_loss: 0.0500\n",
      "Train on 2220 samples, validate on 117 samples\n",
      "Epoch 41/41\n",
      "2220/2220 [==============================] - 18s 8ms/step - loss: 0.2667 - Encoder-Output_loss: 0.2384 - Encoder-Garbedge_loss: 0.0283 - val_loss: 0.2649 - val_Encoder-Output_loss: 0.2242 - val_Encoder-Garbedge_loss: 0.0408\n",
      "Train on 2319 samples, validate on 123 samples\n",
      "Epoch 61/61\n",
      "2319/2319 [==============================] - 18s 8ms/step - loss: 0.2846 - Encoder-Output_loss: 0.2504 - Encoder-Garbedge_loss: 0.0343 - val_loss: 0.2716 - val_Encoder-Output_loss: 0.2503 - val_Encoder-Garbedge_loss: 0.0213\n",
      "Train on 2729 samples, validate on 144 samples\n",
      "Epoch 81/81\n",
      "2729/2729 [==============================] - 22s 8ms/step - loss: 0.2643 - Encoder-Output_loss: 0.2375 - Encoder-Garbedge_loss: 0.0267 - val_loss: 0.2951 - val_Encoder-Output_loss: 0.2312 - val_Encoder-Garbedge_loss: 0.0639\n",
      "Train on 2505 samples, validate on 132 samples\n",
      "Epoch 101/101\n",
      "2505/2505 [==============================] - 20s 8ms/step - loss: 0.2687 - Encoder-Output_loss: 0.2404 - Encoder-Garbedge_loss: 0.0283 - val_loss: 0.3169 - val_Encoder-Output_loss: 0.2605 - val_Encoder-Garbedge_loss: 0.0564\n",
      "Train on 2816 samples, validate on 149 samples\n",
      "Epoch 121/121\n",
      "2816/2816 [==============================] - 23s 8ms/step - loss: 0.2650 - Encoder-Output_loss: 0.2391 - Encoder-Garbedge_loss: 0.0259 - val_loss: 0.3043 - val_Encoder-Output_loss: 0.2432 - val_Encoder-Garbedge_loss: 0.0611\n",
      "Train on 2195 samples, validate on 116 samples\n",
      "Epoch 141/141\n",
      "2195/2195 [==============================] - 17s 8ms/step - loss: 0.2722 - Encoder-Output_loss: 0.2413 - Encoder-Garbedge_loss: 0.0309 - val_loss: 0.3022 - val_Encoder-Output_loss: 0.2354 - val_Encoder-Garbedge_loss: 0.0668\n",
      "Train on 3196 samples, validate on 169 samples\n",
      "Epoch 161/161\n",
      "3196/3196 [==============================] - 26s 8ms/step - loss: 0.2724 - Encoder-Output_loss: 0.2462 - Encoder-Garbedge_loss: 0.0262 - val_loss: 0.2880 - val_Encoder-Output_loss: 0.2367 - val_Encoder-Garbedge_loss: 0.0513\n",
      "Train on 2221 samples, validate on 117 samples\n",
      "Epoch 181/181\n",
      "2221/2221 [==============================] - 18s 8ms/step - loss: 0.2675 - Encoder-Output_loss: 0.2372 - Encoder-Garbedge_loss: 0.0303 - val_loss: 0.3120 - val_Encoder-Output_loss: 0.2425 - val_Encoder-Garbedge_loss: 0.0694\n",
      "Train on 2000 samples, validate on 106 samples\n",
      "Epoch 201/201\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.2775 - Encoder-Output_loss: 0.2456 - Encoder-Garbedge_loss: 0.0319 - val_loss: 0.3042 - val_Encoder-Output_loss: 0.2455 - val_Encoder-Garbedge_loss: 0.0587\n",
      "Train on 1843 samples, validate on 98 samples\n",
      "Epoch 221/221\n",
      "1843/1843 [==============================] - 15s 8ms/step - loss: 0.2897 - Encoder-Output_loss: 0.2473 - Encoder-Garbedge_loss: 0.0424 - val_loss: 0.3000 - val_Encoder-Output_loss: 0.2310 - val_Encoder-Garbedge_loss: 0.0690\n",
      "Train on 2356 samples, validate on 124 samples\n",
      "Epoch 241/241\n",
      "2356/2356 [==============================] - 19s 8ms/step - loss: 0.2629 - Encoder-Output_loss: 0.2345 - Encoder-Garbedge_loss: 0.0284 - val_loss: 0.2652 - val_Encoder-Output_loss: 0.2220 - val_Encoder-Garbedge_loss: 0.0432\n",
      "Train on 2555 samples, validate on 135 samples\n",
      "Epoch 261/261\n",
      "2555/2555 [==============================] - 20s 8ms/step - loss: 0.2658 - Encoder-Output_loss: 0.2373 - Encoder-Garbedge_loss: 0.0285 - val_loss: 0.3092 - val_Encoder-Output_loss: 0.2534 - val_Encoder-Garbedge_loss: 0.0557\n",
      "Train on 2393 samples, validate on 126 samples\n",
      "Epoch 281/281\n",
      "2393/2393 [==============================] - 19s 8ms/step - loss: 0.2827 - Encoder-Output_loss: 0.2495 - Encoder-Garbedge_loss: 0.0332 - val_loss: 0.3199 - val_Encoder-Output_loss: 0.2461 - val_Encoder-Garbedge_loss: 0.0738\n",
      "Train on 2530 samples, validate on 134 samples\n",
      "Epoch 301/301\n",
      "2530/2530 [==============================] - 20s 8ms/step - loss: 0.2733 - Encoder-Output_loss: 0.2446 - Encoder-Garbedge_loss: 0.0288 - val_loss: 0.3290 - val_Encoder-Output_loss: 0.2616 - val_Encoder-Garbedge_loss: 0.0674\n",
      "Train on 1472 samples, validate on 78 samples\n",
      "Epoch 321/321\n",
      "1472/1472 [==============================] - 12s 8ms/step - loss: 0.2719 - Encoder-Output_loss: 0.2336 - Encoder-Garbedge_loss: 0.0383 - val_loss: 0.2657 - val_Encoder-Output_loss: 0.2226 - val_Encoder-Garbedge_loss: 0.0431\n",
      "Train on 2656 samples, validate on 140 samples\n",
      "Epoch 341/341\n",
      "2656/2656 [==============================] - 21s 8ms/step - loss: 0.2663 - Encoder-Output_loss: 0.2378 - Encoder-Garbedge_loss: 0.0285 - val_loss: 0.2818 - val_Encoder-Output_loss: 0.2387 - val_Encoder-Garbedge_loss: 0.0431\n",
      "Train on 2120 samples, validate on 112 samples\n",
      "Epoch 361/361\n",
      "2120/2120 [==============================] - 17s 8ms/step - loss: 0.2604 - Encoder-Output_loss: 0.2328 - Encoder-Garbedge_loss: 0.0276 - val_loss: 0.3195 - val_Encoder-Output_loss: 0.2673 - val_Encoder-Garbedge_loss: 0.0523\n",
      "Train on 1504 samples, validate on 80 samples\n",
      "Epoch 381/381\n",
      "1504/1504 [==============================] - 12s 8ms/step - loss: 0.2683 - Encoder-Output_loss: 0.2333 - Encoder-Garbedge_loss: 0.0350 - val_loss: 0.3167 - val_Encoder-Output_loss: 0.2463 - val_Encoder-Garbedge_loss: 0.0704\n",
      "Train on 2270 samples, validate on 120 samples\n",
      "Epoch 401/401\n",
      "2270/2270 [==============================] - 18s 8ms/step - loss: 0.2607 - Encoder-Output_loss: 0.2340 - Encoder-Garbedge_loss: 0.0267 - val_loss: 0.2755 - val_Encoder-Output_loss: 0.2289 - val_Encoder-Garbedge_loss: 0.0465\n",
      "Train on 1643 samples, validate on 87 samples\n",
      "Epoch 421/421\n",
      "1643/1643 [==============================] - 13s 8ms/step - loss: 0.2772 - Encoder-Output_loss: 0.2427 - Encoder-Garbedge_loss: 0.0345 - val_loss: 0.3224 - val_Encoder-Output_loss: 0.2526 - val_Encoder-Garbedge_loss: 0.0698\n",
      "Train on 2098 samples, validate on 111 samples\n",
      "Epoch 441/441\n",
      "2098/2098 [==============================] - 17s 8ms/step - loss: 0.2726 - Encoder-Output_loss: 0.2404 - Encoder-Garbedge_loss: 0.0322 - val_loss: 0.3136 - val_Encoder-Output_loss: 0.2434 - val_Encoder-Garbedge_loss: 0.0702\n",
      "Train on 2533 samples, validate on 134 samples\n",
      "Epoch 461/461\n",
      "2533/2533 [==============================] - 20s 8ms/step - loss: 0.2681 - Encoder-Output_loss: 0.2398 - Encoder-Garbedge_loss: 0.0283 - val_loss: 0.2929 - val_Encoder-Output_loss: 0.2446 - val_Encoder-Garbedge_loss: 0.0483\n",
      "Train on 1990 samples, validate on 105 samples\n",
      "Epoch 481/481\n",
      "1990/1990 [==============================] - 16s 8ms/step - loss: 0.2756 - Encoder-Output_loss: 0.2428 - Encoder-Garbedge_loss: 0.0328 - val_loss: 0.2775 - val_Encoder-Output_loss: 0.2402 - val_Encoder-Garbedge_loss: 0.0372\n",
      "Train on 1981 samples, validate on 105 samples\n",
      "Epoch 501/501\n",
      "1981/1981 [==============================] - 16s 8ms/step - loss: 0.2791 - Encoder-Output_loss: 0.2442 - Encoder-Garbedge_loss: 0.0349 - val_loss: 0.3052 - val_Encoder-Output_loss: 0.2447 - val_Encoder-Garbedge_loss: 0.0604\n",
      "Train on 2589 samples, validate on 137 samples\n",
      "Epoch 521/521\n",
      "2589/2589 [==============================] - 21s 8ms/step - loss: 0.2711 - Encoder-Output_loss: 0.2425 - Encoder-Garbedge_loss: 0.0286 - val_loss: 0.2782 - val_Encoder-Output_loss: 0.2296 - val_Encoder-Garbedge_loss: 0.0486\n",
      "Train on 2711 samples, validate on 143 samples\n",
      "Epoch 541/541\n",
      "2711/2711 [==============================] - 21s 8ms/step - loss: 0.2751 - Encoder-Output_loss: 0.2439 - Encoder-Garbedge_loss: 0.0313 - val_loss: 0.2955 - val_Encoder-Output_loss: 0.2403 - val_Encoder-Garbedge_loss: 0.0552\n",
      "Train on 2249 samples, validate on 119 samples\n",
      "Epoch 561/561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2249/2249 [==============================] - 18s 8ms/step - loss: 0.2706 - Encoder-Output_loss: 0.2407 - Encoder-Garbedge_loss: 0.0299 - val_loss: 0.3054 - val_Encoder-Output_loss: 0.2415 - val_Encoder-Garbedge_loss: 0.0639\n",
      "Train on 1990 samples, validate on 105 samples\n",
      "Epoch 581/581\n",
      "1990/1990 [==============================] - 16s 8ms/step - loss: 0.2776 - Encoder-Output_loss: 0.2429 - Encoder-Garbedge_loss: 0.0347 - val_loss: 0.2974 - val_Encoder-Output_loss: 0.2457 - val_Encoder-Garbedge_loss: 0.0518\n",
      "Train on 3137 samples, validate on 166 samples\n",
      "Epoch 601/601\n",
      "3137/3137 [==============================] - 25s 8ms/step - loss: 0.2661 - Encoder-Output_loss: 0.2401 - Encoder-Garbedge_loss: 0.0259 - val_loss: 0.2959 - val_Encoder-Output_loss: 0.2601 - val_Encoder-Garbedge_loss: 0.0358\n",
      "Train on 1945 samples, validate on 103 samples\n",
      "Epoch 621/621\n",
      "1945/1945 [==============================] - 16s 8ms/step - loss: 0.2716 - Encoder-Output_loss: 0.2385 - Encoder-Garbedge_loss: 0.0331 - val_loss: 0.3264 - val_Encoder-Output_loss: 0.2598 - val_Encoder-Garbedge_loss: 0.0665\n",
      "Train on 2172 samples, validate on 115 samples\n",
      "Epoch 641/641\n",
      "2172/2172 [==============================] - 17s 8ms/step - loss: 0.2797 - Encoder-Output_loss: 0.2469 - Encoder-Garbedge_loss: 0.0328 - val_loss: 0.2998 - val_Encoder-Output_loss: 0.2397 - val_Encoder-Garbedge_loss: 0.0601\n",
      "Train on 2550 samples, validate on 135 samples\n",
      "Epoch 661/661\n",
      "2550/2550 [==============================] - 20s 8ms/step - loss: 0.2668 - Encoder-Output_loss: 0.2401 - Encoder-Garbedge_loss: 0.0266 - val_loss: 0.3118 - val_Encoder-Output_loss: 0.2484 - val_Encoder-Garbedge_loss: 0.0634\n",
      "Train on 2698 samples, validate on 142 samples\n",
      "Epoch 681/681\n",
      "2698/2698 [==============================] - 22s 8ms/step - loss: 0.2595 - Encoder-Output_loss: 0.2332 - Encoder-Garbedge_loss: 0.0263 - val_loss: 0.2523 - val_Encoder-Output_loss: 0.2379 - val_Encoder-Garbedge_loss: 0.0144\n",
      "Train on 1845 samples, validate on 98 samples\n",
      "Epoch 701/701\n",
      "1845/1845 [==============================] - 15s 8ms/step - loss: 0.2813 - Encoder-Output_loss: 0.2374 - Encoder-Garbedge_loss: 0.0439 - val_loss: 0.2833 - val_Encoder-Output_loss: 0.2317 - val_Encoder-Garbedge_loss: 0.0516\n",
      "Train on 2012 samples, validate on 106 samples\n",
      "Epoch 721/721\n",
      "2012/2012 [==============================] - 21s 10ms/step - loss: 0.2761 - Encoder-Output_loss: 0.2419 - Encoder-Garbedge_loss: 0.0342 - val_loss: 0.3091 - val_Encoder-Output_loss: 0.2414 - val_Encoder-Garbedge_loss: 0.0677\n",
      "Train on 2540 samples, validate on 134 samples\n",
      "Epoch 741/741\n",
      "2540/2540 [==============================] - 21s 8ms/step - loss: 0.2629 - Encoder-Output_loss: 0.2378 - Encoder-Garbedge_loss: 0.0251 - val_loss: 0.3105 - val_Encoder-Output_loss: 0.2438 - val_Encoder-Garbedge_loss: 0.0667\n",
      "Train on 3125 samples, validate on 165 samples\n",
      "Epoch 761/761\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 0.2695 - Encoder-Output_loss: 0.2417 - Encoder-Garbedge_loss: 0.0278 - val_loss: 0.2799 - val_Encoder-Output_loss: 0.2415 - val_Encoder-Garbedge_loss: 0.0384\n",
      "Train on 2779 samples, validate on 147 samples\n",
      "Epoch 781/781\n",
      "2779/2779 [==============================] - 22s 8ms/step - loss: 0.2730 - Encoder-Output_loss: 0.2443 - Encoder-Garbedge_loss: 0.0287 - val_loss: 0.2651 - val_Encoder-Output_loss: 0.2231 - val_Encoder-Garbedge_loss: 0.0420\n",
      "Train on 1509 samples, validate on 80 samples\n",
      "Epoch 801/801\n",
      "1509/1509 [==============================] - 12s 8ms/step - loss: 0.2679 - Encoder-Output_loss: 0.2339 - Encoder-Garbedge_loss: 0.0340 - val_loss: 0.3169 - val_Encoder-Output_loss: 0.2454 - val_Encoder-Garbedge_loss: 0.0715\n",
      "Train on 2528 samples, validate on 134 samples\n",
      "Epoch 821/821\n",
      "2528/2528 [==============================] - 20s 8ms/step - loss: 0.2697 - Encoder-Output_loss: 0.2387 - Encoder-Garbedge_loss: 0.0309 - val_loss: 0.3264 - val_Encoder-Output_loss: 0.2579 - val_Encoder-Garbedge_loss: 0.0685\n",
      "Train on 2438 samples, validate on 129 samples\n",
      "Epoch 841/841\n",
      "2438/2438 [==============================] - 20s 8ms/step - loss: 0.2720 - Encoder-Output_loss: 0.2424 - Encoder-Garbedge_loss: 0.0296 - val_loss: 0.3063 - val_Encoder-Output_loss: 0.2483 - val_Encoder-Garbedge_loss: 0.0580\n",
      "Train on 2700 samples, validate on 143 samples\n",
      "Epoch 861/861\n",
      "2700/2700 [==============================] - 22s 8ms/step - loss: 0.2674 - Encoder-Output_loss: 0.2400 - Encoder-Garbedge_loss: 0.0274 - val_loss: 0.2985 - val_Encoder-Output_loss: 0.2496 - val_Encoder-Garbedge_loss: 0.0489\n",
      "Train on 1264 samples, validate on 67 samples\n",
      "Epoch 881/881\n",
      "1264/1264 [==============================] - 10s 8ms/step - loss: 0.2615 - Encoder-Output_loss: 0.2270 - Encoder-Garbedge_loss: 0.0345 - val_loss: 0.3086 - val_Encoder-Output_loss: 0.2455 - val_Encoder-Garbedge_loss: 0.0631\n",
      "Train on 1817 samples, validate on 96 samples\n",
      "Epoch 901/901\n",
      "1817/1817 [==============================] - 15s 8ms/step - loss: 0.2587 - Encoder-Output_loss: 0.2293 - Encoder-Garbedge_loss: 0.0294 - val_loss: 0.2504 - val_Encoder-Output_loss: 0.2158 - val_Encoder-Garbedge_loss: 0.0347\n",
      "Train on 2131 samples, validate on 113 samples\n",
      "Epoch 921/921\n",
      "2131/2131 [==============================] - 17s 8ms/step - loss: 0.2760 - Encoder-Output_loss: 0.2430 - Encoder-Garbedge_loss: 0.0330 - val_loss: 0.3068 - val_Encoder-Output_loss: 0.2461 - val_Encoder-Garbedge_loss: 0.0606\n",
      "Train on 3011 samples, validate on 159 samples\n",
      "Epoch 941/941\n",
      "3011/3011 [==============================] - 23s 8ms/step - loss: 0.2611 - Encoder-Output_loss: 0.2376 - Encoder-Garbedge_loss: 0.0235 - val_loss: 0.2963 - val_Encoder-Output_loss: 0.2387 - val_Encoder-Garbedge_loss: 0.0577\n",
      "Train on 1772 samples, validate on 94 samples\n",
      "Epoch 961/961\n",
      "1772/1772 [==============================] - 14s 8ms/step - loss: 0.2824 - Encoder-Output_loss: 0.2428 - Encoder-Garbedge_loss: 0.0396 - val_loss: 0.2900 - val_Encoder-Output_loss: 0.2553 - val_Encoder-Garbedge_loss: 0.0347\n",
      "Train on 1755 samples, validate on 93 samples\n",
      "Epoch 981/981\n",
      "1755/1755 [==============================] - 14s 8ms/step - loss: 0.2702 - Encoder-Output_loss: 0.2378 - Encoder-Garbedge_loss: 0.0324 - val_loss: 0.2865 - val_Encoder-Output_loss: 0.2277 - val_Encoder-Garbedge_loss: 0.0588\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "**********************************\n",
      "\n",
      "Train on 2021 samples, validate on 107 samples\n",
      "Epoch 1001/1001\n",
      "2021/2021 [==============================] - 16s 8ms/step - loss: 0.2731 - Encoder-Output_loss: 0.2422 - Encoder-Garbedge_loss: 0.0309 - val_loss: 0.2859 - val_Encoder-Output_loss: 0.2312 - val_Encoder-Garbedge_loss: 0.0547\n",
      "Train on 2352 samples, validate on 124 samples\n",
      "Epoch 1021/1021\n",
      "2352/2352 [==============================] - 18s 8ms/step - loss: 0.2695 - Encoder-Output_loss: 0.2404 - Encoder-Garbedge_loss: 0.0291 - val_loss: 0.2975 - val_Encoder-Output_loss: 0.2334 - val_Encoder-Garbedge_loss: 0.0641\n",
      "Train on 2427 samples, validate on 128 samples\n",
      "Epoch 1041/1041\n",
      "2427/2427 [==============================] - 19s 8ms/step - loss: 0.2674 - Encoder-Output_loss: 0.2378 - Encoder-Garbedge_loss: 0.0296 - val_loss: 0.2703 - val_Encoder-Output_loss: 0.2358 - val_Encoder-Garbedge_loss: 0.0345\n",
      "Train on 2433 samples, validate on 129 samples\n",
      "Epoch 1061/1061\n",
      "2433/2433 [==============================] - 19s 8ms/step - loss: 0.2678 - Encoder-Output_loss: 0.2403 - Encoder-Garbedge_loss: 0.0276 - val_loss: 0.2956 - val_Encoder-Output_loss: 0.2388 - val_Encoder-Garbedge_loss: 0.0569\n",
      "Train on 2404 samples, validate on 127 samples\n",
      "Epoch 1081/1081\n",
      "2404/2404 [==============================] - 19s 8ms/step - loss: 0.2605 - Encoder-Output_loss: 0.2342 - Encoder-Garbedge_loss: 0.0264 - val_loss: 0.3224 - val_Encoder-Output_loss: 0.2516 - val_Encoder-Garbedge_loss: 0.0708coder-Ga\n",
      "Train on 2591 samples, validate on 137 samples\n",
      "Epoch 1101/1101\n",
      "2591/2591 [==============================] - 20s 8ms/step - loss: 0.2650 - Encoder-Output_loss: 0.2394 - Encoder-Garbedge_loss: 0.0256 - val_loss: 0.3119 - val_Encoder-Output_loss: 0.2526 - val_Encoder-Garbedge_loss: 0.0593\n",
      "Train on 2432 samples, validate on 129 samples\n",
      "Epoch 1121/1121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2432/2432 [==============================] - 19s 8ms/step - loss: 0.2639 - Encoder-Output_loss: 0.2375 - Encoder-Garbedge_loss: 0.0264 - val_loss: 0.2597 - val_Encoder-Output_loss: 0.2336 - val_Encoder-Garbedge_loss: 0.0261\n",
      "Train on 2475 samples, validate on 131 samples\n",
      "Epoch 1141/1141\n",
      "2475/2475 [==============================] - 19s 8ms/step - loss: 0.2692 - Encoder-Output_loss: 0.2399 - Encoder-Garbedge_loss: 0.0293 - val_loss: 0.3076 - val_Encoder-Output_loss: 0.2490 - val_Encoder-Garbedge_loss: 0.0586\n",
      "Train on 2011 samples, validate on 106 samples\n",
      "Epoch 1161/1161\n",
      "2011/2011 [==============================] - 16s 8ms/step - loss: 0.2673 - Encoder-Output_loss: 0.2391 - Encoder-Garbedge_loss: 0.0282 - val_loss: 0.3051 - val_Encoder-Output_loss: 0.2417 - val_Encoder-Garbedge_loss: 0.0634\n",
      "Train on 2217 samples, validate on 117 samples\n",
      "Epoch 1181/1181\n",
      "2217/2217 [==============================] - 17s 8ms/step - loss: 0.2856 - Encoder-Output_loss: 0.2480 - Encoder-Garbedge_loss: 0.0376 - val_loss: 0.3078 - val_Encoder-Output_loss: 0.2394 - val_Encoder-Garbedge_loss: 0.0684\n",
      "Train on 2636 samples, validate on 139 samples\n",
      "Epoch 1201/1201\n",
      "2636/2636 [==============================] - 21s 8ms/step - loss: 0.2701 - Encoder-Output_loss: 0.2393 - Encoder-Garbedge_loss: 0.0309 - val_loss: 0.2791 - val_Encoder-Output_loss: 0.2359 - val_Encoder-Garbedge_loss: 0.0432\n",
      "Train on 2353 samples, validate on 124 samples\n",
      "Epoch 1221/1221\n",
      "2353/2353 [==============================] - 18s 8ms/step - loss: 0.2634 - Encoder-Output_loss: 0.2357 - Encoder-Garbedge_loss: 0.0277 - val_loss: 0.3154 - val_Encoder-Output_loss: 0.2580 - val_Encoder-Garbedge_loss: 0.0574\n",
      "Train on 1920 samples, validate on 102 samples\n",
      "Epoch 1241/1241\n",
      "1920/1920 [==============================] - 15s 8ms/step - loss: 0.2756 - Encoder-Output_loss: 0.2426 - Encoder-Garbedge_loss: 0.0330 - val_loss: 0.2820 - val_Encoder-Output_loss: 0.2339 - val_Encoder-Garbedge_loss: 0.0481\n",
      "Train on 2401 samples, validate on 127 samples\n",
      "Epoch 1261/1261\n",
      "2401/2401 [==============================] - 19s 8ms/step - loss: 0.2675 - Encoder-Output_loss: 0.2401 - Encoder-Garbedge_loss: 0.0273 - val_loss: 0.3029 - val_Encoder-Output_loss: 0.2582 - val_Encoder-Garbedge_loss: 0.0447\n",
      "Train on 2113 samples, validate on 112 samples\n",
      "Epoch 1281/1281\n",
      "2113/2113 [==============================] - 16s 8ms/step - loss: 0.2611 - Encoder-Output_loss: 0.2337 - Encoder-Garbedge_loss: 0.0274 - val_loss: 0.3036 - val_Encoder-Output_loss: 0.2443 - val_Encoder-Garbedge_loss: 0.0593\n",
      "Train on 2303 samples, validate on 122 samples\n",
      "Epoch 1301/1301\n",
      "2303/2303 [==============================] - 18s 8ms/step - loss: 0.2601 - Encoder-Output_loss: 0.2333 - Encoder-Garbedge_loss: 0.0269 - val_loss: 0.3082 - val_Encoder-Output_loss: 0.2435 - val_Encoder-Garbedge_loss: 0.0647\n",
      "Train on 2066 samples, validate on 109 samples\n",
      "Epoch 1321/1321\n",
      "2066/2066 [==============================] - 16s 8ms/step - loss: 0.2726 - Encoder-Output_loss: 0.2410 - Encoder-Garbedge_loss: 0.0317 - val_loss: 0.2532 - val_Encoder-Output_loss: 0.2309 - val_Encoder-Garbedge_loss: 0.0223\n",
      "Train on 1626 samples, validate on 86 samples\n",
      "Epoch 1341/1341\n",
      "1626/1626 [==============================] - 13s 8ms/step - loss: 0.2737 - Encoder-Output_loss: 0.2373 - Encoder-Garbedge_loss: 0.0363 - val_loss: 0.2975 - val_Encoder-Output_loss: 0.2303 - val_Encoder-Garbedge_loss: 0.0672\n",
      "Train on 1430 samples, validate on 76 samples\n",
      "Epoch 1361/1361\n",
      "1430/1430 [==============================] - 11s 8ms/step - loss: 0.2734 - Encoder-Output_loss: 0.2345 - Encoder-Garbedge_loss: 0.0389 - val_loss: 0.3803 - val_Encoder-Output_loss: 0.2748 - val_Encoder-Garbedge_loss: 0.1054\n",
      "Train on 1995 samples, validate on 105 samples\n",
      "Epoch 1381/1381\n",
      "1995/1995 [==============================] - 16s 8ms/step - loss: 0.2623 - Encoder-Output_loss: 0.2322 - Encoder-Garbedge_loss: 0.0301 - val_loss: 0.2987 - val_Encoder-Output_loss: 0.2422 - val_Encoder-Garbedge_loss: 0.0565\n",
      "Train on 2110 samples, validate on 112 samples\n",
      "Epoch 1401/1401\n",
      "2110/2110 [==============================] - 16s 8ms/step - loss: 0.2597 - Encoder-Output_loss: 0.2325 - Encoder-Garbedge_loss: 0.0272 - val_loss: 0.2733 - val_Encoder-Output_loss: 0.2306 - val_Encoder-Garbedge_loss: 0.0427\n",
      "Train on 1798 samples, validate on 95 samples\n",
      "Epoch 1421/1421\n",
      "1798/1798 [==============================] - 14s 8ms/step - loss: 0.2710 - Encoder-Output_loss: 0.2376 - Encoder-Garbedge_loss: 0.0334 - val_loss: 0.2871 - val_Encoder-Output_loss: 0.2246 - val_Encoder-Garbedge_loss: 0.0624\n",
      "Train on 1748 samples, validate on 93 samples\n",
      "Epoch 1441/1441\n",
      "1748/1748 [==============================] - 14s 8ms/step - loss: 0.2612 - Encoder-Output_loss: 0.2301 - Encoder-Garbedge_loss: 0.0311 - val_loss: 0.2793 - val_Encoder-Output_loss: 0.2375 - val_Encoder-Garbedge_loss: 0.0418\n",
      "Train on 2394 samples, validate on 126 samples\n",
      "Epoch 1461/1461\n",
      "2394/2394 [==============================] - 19s 8ms/step - loss: 0.2576 - Encoder-Output_loss: 0.2318 - Encoder-Garbedge_loss: 0.0259 - val_loss: 0.2925 - val_Encoder-Output_loss: 0.2324 - val_Encoder-Garbedge_loss: 0.0601\n",
      "Train on 2182 samples, validate on 115 samples\n",
      "Epoch 1481/1481\n",
      "2182/2182 [==============================] - 17s 8ms/step - loss: 0.2782 - Encoder-Output_loss: 0.2413 - Encoder-Garbedge_loss: 0.0369 - val_loss: 0.3288 - val_Encoder-Output_loss: 0.2525 - val_Encoder-Garbedge_loss: 0.0763\n",
      "Train on 1859 samples, validate on 98 samples\n",
      "Epoch 1501/1501\n",
      "1859/1859 [==============================] - 14s 8ms/step - loss: 0.2779 - Encoder-Output_loss: 0.2443 - Encoder-Garbedge_loss: 0.0336 - val_loss: 0.3051 - val_Encoder-Output_loss: 0.2347 - val_Encoder-Garbedge_loss: 0.0704\n",
      "Train on 1958 samples, validate on 104 samples\n",
      "Epoch 1521/1521\n",
      "1958/1958 [==============================] - 15s 8ms/step - loss: 0.2717 - Encoder-Output_loss: 0.2399 - Encoder-Garbedge_loss: 0.0317 - val_loss: 0.2910 - val_Encoder-Output_loss: 0.2356 - val_Encoder-Garbedge_loss: 0.0554\n",
      "Train on 1109 samples, validate on 59 samples\n",
      "Epoch 1541/1541\n",
      "1109/1109 [==============================] - 9s 8ms/step - loss: 0.2896 - Encoder-Output_loss: 0.2422 - Encoder-Garbedge_loss: 0.0474 - val_loss: 0.3202 - val_Encoder-Output_loss: 0.2464 - val_Encoder-Garbedge_loss: 0.0738\n",
      "Train on 1943 samples, validate on 103 samples\n",
      "Epoch 1561/1561\n",
      "1943/1943 [==============================] - 15s 8ms/step - loss: 0.2667 - Encoder-Output_loss: 0.2373 - Encoder-Garbedge_loss: 0.0294 - val_loss: 0.3208 - val_Encoder-Output_loss: 0.2557 - val_Encoder-Garbedge_loss: 0.0652\n",
      "Train on 2150 samples, validate on 114 samples\n",
      "Epoch 1581/1581\n",
      "2150/2150 [==============================] - 17s 8ms/step - loss: 0.2718 - Encoder-Output_loss: 0.2411 - Encoder-Garbedge_loss: 0.0307 - val_loss: 0.3276 - val_Encoder-Output_loss: 0.2727 - val_Encoder-Garbedge_loss: 0.0549\n",
      "Train on 2395 samples, validate on 127 samples\n",
      "Epoch 1601/1601\n",
      "2395/2395 [==============================] - 19s 8ms/step - loss: 0.2836 - Encoder-Output_loss: 0.2464 - Encoder-Garbedge_loss: 0.0372 - val_loss: 0.3007 - val_Encoder-Output_loss: 0.2438 - val_Encoder-Garbedge_loss: 0.0569\n",
      "Train on 2972 samples, validate on 157 samples\n",
      "Epoch 1621/1621\n",
      "2972/2972 [==============================] - 23s 8ms/step - loss: 0.2668 - Encoder-Output_loss: 0.2412 - Encoder-Garbedge_loss: 0.0256 - val_loss: 0.2966 - val_Encoder-Output_loss: 0.2391 - val_Encoder-Garbedge_loss: 0.0574\n",
      "Train on 2280 samples, validate on 120 samples\n",
      "Epoch 1641/1641\n",
      "2280/2280 [==============================] - 18s 8ms/step - loss: 0.2683 - Encoder-Output_loss: 0.2403 - Encoder-Garbedge_loss: 0.0280 - val_loss: 0.3195 - val_Encoder-Output_loss: 0.2515 - val_Encoder-Garbedge_loss: 0.0680\n",
      "Train on 2714 samples, validate on 143 samples\n",
      "Epoch 1661/1661\n",
      "2714/2714 [==============================] - 21s 8ms/step - loss: 0.2596 - Encoder-Output_loss: 0.2319 - Encoder-Garbedge_loss: 0.0277 - val_loss: 0.2786 - val_Encoder-Output_loss: 0.2466 - val_Encoder-Garbedge_loss: 0.0319\n",
      "Train on 2672 samples, validate on 141 samples\n",
      "Epoch 1681/1681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2672/2672 [==============================] - 21s 8ms/step - loss: 0.2625 - Encoder-Output_loss: 0.2357 - Encoder-Garbedge_loss: 0.0268 - val_loss: 0.2720 - val_Encoder-Output_loss: 0.2458 - val_Encoder-Garbedge_loss: 0.0262\n",
      "Train on 1444 samples, validate on 76 samples\n",
      "Epoch 1701/1701\n",
      "1444/1444 [==============================] - 11s 8ms/step - loss: 0.2659 - Encoder-Output_loss: 0.2298 - Encoder-Garbedge_loss: 0.0361 - val_loss: 0.2772 - val_Encoder-Output_loss: 0.2193 - val_Encoder-Garbedge_loss: 0.0578\n",
      "Train on 1895 samples, validate on 100 samples\n",
      "Epoch 1721/1721\n",
      "1895/1895 [==============================] - 15s 8ms/step - loss: 0.2675 - Encoder-Output_loss: 0.2346 - Encoder-Garbedge_loss: 0.0329 - val_loss: 0.2840 - val_Encoder-Output_loss: 0.2231 - val_Encoder-Garbedge_loss: 0.0610\n",
      "Train on 2675 samples, validate on 141 samples\n",
      "Epoch 1741/1741\n",
      "2675/2675 [==============================] - 21s 8ms/step - loss: 0.2557 - Encoder-Output_loss: 0.2319 - Encoder-Garbedge_loss: 0.0239 - val_loss: 0.2737 - val_Encoder-Output_loss: 0.2328 - val_Encoder-Garbedge_loss: 0.0409\n",
      "Train on 3059 samples, validate on 161 samples\n",
      "Epoch 1761/1761\n",
      "3059/3059 [==============================] - 24s 8ms/step - loss: 0.2666 - Encoder-Output_loss: 0.2413 - Encoder-Garbedge_loss: 0.0253 - val_loss: 0.3322 - val_Encoder-Output_loss: 0.2575 - val_Encoder-Garbedge_loss: 0.0748\n",
      "Train on 2345 samples, validate on 124 samples\n",
      "Epoch 1781/1781\n",
      "2345/2345 [==============================] - 18s 8ms/step - loss: 0.2935 - Encoder-Output_loss: 0.2525 - Encoder-Garbedge_loss: 0.0410 - val_loss: 0.3069 - val_Encoder-Output_loss: 0.2383 - val_Encoder-Garbedge_loss: 0.0686\n",
      "Train on 2058 samples, validate on 109 samples\n",
      "Epoch 1801/1801\n",
      "2058/2058 [==============================] - 16s 8ms/step - loss: 0.2811 - Encoder-Output_loss: 0.2470 - Encoder-Garbedge_loss: 0.0341 - val_loss: 0.3323 - val_Encoder-Output_loss: 0.2576 - val_Encoder-Garbedge_loss: 0.0746\n",
      "Train on 2320 samples, validate on 123 samples\n",
      "Epoch 1821/1821\n",
      "2320/2320 [==============================] - 18s 8ms/step - loss: 0.2578 - Encoder-Output_loss: 0.2292 - Encoder-Garbedge_loss: 0.0286 - val_loss: 0.3066 - val_Encoder-Output_loss: 0.2500 - val_Encoder-Garbedge_loss: 0.0566\n",
      "Train on 2602 samples, validate on 137 samples\n",
      "Epoch 1841/1841\n",
      "2602/2602 [==============================] - 20s 8ms/step - loss: 0.2664 - Encoder-Output_loss: 0.2402 - Encoder-Garbedge_loss: 0.0261 - val_loss: 0.2798 - val_Encoder-Output_loss: 0.2334 - val_Encoder-Garbedge_loss: 0.0464\n",
      "Train on 1972 samples, validate on 104 samples\n",
      "Epoch 1861/1861\n",
      "1972/1972 [==============================] - 15s 8ms/step - loss: 0.2600 - Encoder-Output_loss: 0.2289 - Encoder-Garbedge_loss: 0.0311 - val_loss: 0.2691 - val_Encoder-Output_loss: 0.2214 - val_Encoder-Garbedge_loss: 0.0476\n",
      "Train on 1450 samples, validate on 77 samples\n",
      "Epoch 1881/1881\n",
      "1450/1450 [==============================] - 11s 8ms/step - loss: 0.2731 - Encoder-Output_loss: 0.2352 - Encoder-Garbedge_loss: 0.0379 - val_loss: 0.3219 - val_Encoder-Output_loss: 0.2544 - val_Encoder-Garbedge_loss: 0.0674\n",
      "Train on 1824 samples, validate on 96 samples\n",
      "Epoch 1901/1901\n",
      "1824/1824 [==============================] - 14s 8ms/step - loss: 0.2715 - Encoder-Output_loss: 0.2393 - Encoder-Garbedge_loss: 0.0322 - val_loss: 0.2693 - val_Encoder-Output_loss: 0.2294 - val_Encoder-Garbedge_loss: 0.0399\n",
      "Train on 2832 samples, validate on 150 samples\n",
      "Epoch 1921/1921\n",
      "2832/2832 [==============================] - 22s 8ms/step - loss: 0.2751 - Encoder-Output_loss: 0.2430 - Encoder-Garbedge_loss: 0.0321 - val_loss: 0.2752 - val_Encoder-Output_loss: 0.2302 - val_Encoder-Garbedge_loss: 0.0450\n",
      "Train on 3107 samples, validate on 164 samples\n",
      "Epoch 1941/1941\n",
      "3107/3107 [==============================] - 24s 8ms/step - loss: 0.2527 - Encoder-Output_loss: 0.2312 - Encoder-Garbedge_loss: 0.0215 - val_loss: 0.2500 - val_Encoder-Output_loss: 0.2241 - val_Encoder-Garbedge_loss: 0.0258\n",
      "Train on 2063 samples, validate on 109 samples\n",
      "Epoch 1961/1961\n",
      "2063/2063 [==============================] - 16s 8ms/step - loss: 0.2723 - Encoder-Output_loss: 0.2405 - Encoder-Garbedge_loss: 0.0318 - val_loss: 0.2693 - val_Encoder-Output_loss: 0.2139 - val_Encoder-Garbedge_loss: 0.0554\n",
      "Train on 2462 samples, validate on 130 samples\n",
      "Epoch 1981/1981\n",
      "2462/2462 [==============================] - 19s 8ms/step - loss: 0.2743 - Encoder-Output_loss: 0.2438 - Encoder-Garbedge_loss: 0.0305 - val_loss: 0.2779 - val_Encoder-Output_loss: 0.2408 - val_Encoder-Garbedge_loss: 0.0371\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "**********************************\n",
      "\n",
      "Train on 1929 samples, validate on 102 samples\n",
      "Epoch 2001/2001\n",
      "1929/1929 [==============================] - 15s 8ms/step - loss: 0.2706 - Encoder-Output_loss: 0.2383 - Encoder-Garbedge_loss: 0.0324 - val_loss: 0.3155 - val_Encoder-Output_loss: 0.2450 - val_Encoder-Garbedge_loss: 0.0705\n",
      "Train on 1514 samples, validate on 80 samples\n",
      "Epoch 2021/2021\n",
      "1514/1514 [==============================] - 12s 8ms/step - loss: 0.2565 - Encoder-Output_loss: 0.2248 - Encoder-Garbedge_loss: 0.0317 - val_loss: 0.2661 - val_Encoder-Output_loss: 0.2201 - val_Encoder-Garbedge_loss: 0.0459\n",
      "Train on 1960 samples, validate on 104 samples\n",
      "Epoch 2041/2041\n",
      "1960/1960 [==============================] - 15s 8ms/step - loss: 0.2745 - Encoder-Output_loss: 0.2443 - Encoder-Garbedge_loss: 0.0302 - val_loss: 0.2687 - val_Encoder-Output_loss: 0.2232 - val_Encoder-Garbedge_loss: 0.0454\n",
      "Train on 2235 samples, validate on 118 samples\n",
      "Epoch 2061/2061\n",
      "2235/2235 [==============================] - 17s 8ms/step - loss: 0.2642 - Encoder-Output_loss: 0.2347 - Encoder-Garbedge_loss: 0.0295 - val_loss: 0.2662 - val_Encoder-Output_loss: 0.2135 - val_Encoder-Garbedge_loss: 0.0527\n",
      "Train on 2309 samples, validate on 122 samples\n",
      "Epoch 2081/2081\n",
      "2309/2309 [==============================] - 18s 8ms/step - loss: 0.2744 - Encoder-Output_loss: 0.2414 - Encoder-Garbedge_loss: 0.0330 - val_loss: 0.3560 - val_Encoder-Output_loss: 0.2807 - val_Encoder-Garbedge_loss: 0.0752\n",
      "Train on 1598 samples, validate on 85 samples\n",
      "Epoch 2101/2101\n",
      "1598/1598 [==============================] - 12s 8ms/step - loss: 0.2646 - Encoder-Output_loss: 0.2308 - Encoder-Garbedge_loss: 0.0338 - val_loss: 0.2766 - val_Encoder-Output_loss: 0.2232 - val_Encoder-Garbedge_loss: 0.0534\n",
      "Train on 2353 samples, validate on 124 samples\n",
      "Epoch 2121/2121\n",
      "2353/2353 [==============================] - 18s 8ms/step - loss: 0.2778 - Encoder-Output_loss: 0.2455 - Encoder-Garbedge_loss: 0.0323 - val_loss: 0.3055 - val_Encoder-Output_loss: 0.2491 - val_Encoder-Garbedge_loss: 0.0565-Output_loss\n",
      "Train on 2691 samples, validate on 142 samples\n",
      "Epoch 2141/2141\n",
      "2691/2691 [==============================] - 21s 8ms/step - loss: 0.2700 - Encoder-Output_loss: 0.2419 - Encoder-Garbedge_loss: 0.0281 - val_loss: 0.2889 - val_Encoder-Output_loss: 0.2367 - val_Encoder-Garbedge_loss: 0.0522\n",
      "Train on 2625 samples, validate on 139 samples\n",
      "Epoch 2161/2161\n",
      "2625/2625 [==============================] - 20s 8ms/step - loss: 0.2688 - Encoder-Output_loss: 0.2409 - Encoder-Garbedge_loss: 0.0280 - val_loss: 0.3039 - val_Encoder-Output_loss: 0.2440 - val_Encoder-Garbedge_loss: 0.0599\n",
      "Train on 2679 samples, validate on 141 samples\n",
      "Epoch 2181/2181\n",
      "2679/2679 [==============================] - 21s 8ms/step - loss: 0.2631 - Encoder-Output_loss: 0.2384 - Encoder-Garbedge_loss: 0.0247 - val_loss: 0.3159 - val_Encoder-Output_loss: 0.2484 - val_Encoder-Garbedge_loss: 0.0675\n",
      "Train on 3332 samples, validate on 176 samples\n",
      "Epoch 2201/2201\n",
      "3332/3332 [==============================] - 26s 8ms/step - loss: 0.2606 - Encoder-Output_loss: 0.2389 - Encoder-Garbedge_loss: 0.0216 - val_loss: 0.2963 - val_Encoder-Output_loss: 0.2289 - val_Encoder-Garbedge_loss: 0.0674\n",
      "Train on 2508 samples, validate on 133 samples\n",
      "Epoch 2221/2221\n",
      "2508/2508 [==============================] - 20s 8ms/step - loss: 0.2682 - Encoder-Output_loss: 0.2422 - Encoder-Garbedge_loss: 0.0260 - val_loss: 0.2833 - val_Encoder-Output_loss: 0.2324 - val_Encoder-Garbedge_loss: 0.0509\n",
      "Train on 2713 samples, validate on 143 samples\n",
      "Epoch 2241/2241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2713/2713 [==============================] - 21s 8ms/step - loss: 0.2713 - Encoder-Output_loss: 0.2407 - Encoder-Garbedge_loss: 0.0306 - val_loss: 0.2819 - val_Encoder-Output_loss: 0.2219 - val_Encoder-Garbedge_loss: 0.0600\n",
      "Train on 3341 samples, validate on 176 samples\n",
      "Epoch 2261/2261\n",
      "3341/3341 [==============================] - 26s 8ms/step - loss: 0.2511 - Encoder-Output_loss: 0.2302 - Encoder-Garbedge_loss: 0.0209 - val_loss: 0.2727 - val_Encoder-Output_loss: 0.2278 - val_Encoder-Garbedge_loss: 0.0449\n",
      "Train on 2346 samples, validate on 124 samples\n",
      "Epoch 2281/2281\n",
      "2346/2346 [==============================] - 18s 8ms/step - loss: 0.2722 - Encoder-Output_loss: 0.2414 - Encoder-Garbedge_loss: 0.0308 - val_loss: 0.3150 - val_Encoder-Output_loss: 0.2522 - val_Encoder-Garbedge_loss: 0.0628\n",
      "Train on 1875 samples, validate on 99 samples\n",
      "Epoch 2301/2301\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2848 - Encoder-Output_loss: 0.2458 - Encoder-Garbedge_loss: 0.0390 - val_loss: 0.2574 - val_Encoder-Output_loss: 0.2273 - val_Encoder-Garbedge_loss: 0.0300\n"
     ]
    }
   ],
   "source": [
    "start=1\n",
    "n=0\n",
    "e=1\n",
    "for d in range(12):\n",
    "    print('Epoch: '+str(d)+'\\n'+\"**********************************\\n\")\n",
    "    for i in range(200):\n",
    "        vectors_train = np.load('C:/Users/DNS/Desktop/Progi/Python/NeuralNetworks/Курсач/data/data3000/news_vectors'+str(999-i)+'.npy')\n",
    "        #print(start)\n",
    "        texts = textreader(start)\n",
    "        start+=500\n",
    "        #print(len(texts))\n",
    "\n",
    "        for j in range(5):\n",
    "            tt, kol, garb = word_vectors_creator(texts[0+j*100:(1+j)*100])\n",
    "            vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "            pos = positioncreator(kol)\n",
    "\n",
    "            if n%20==0:\n",
    "                model.fit(\n",
    "                    x=[vtt,\n",
    "                       pos],\n",
    "                    y = [tt,\n",
    "                        garb],\n",
    "                    epochs=e, initial_epoch=n,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=1,\n",
    "                    batch_size=4\n",
    "                )\n",
    "                if n%100==0:\n",
    "                    model.save('C:/Users/DNS/Desktop/Progi/Python/NeuralNetworks/Курсач/models/sen_to_word'+str(d)+'_'+str(i)+'.h5')\n",
    "            else:\n",
    "                model.fit(\n",
    "                    x=[vtt,\n",
    "                       pos],\n",
    "                    y = [tt,\n",
    "                        garb],\n",
    "                    epochs=e, initial_epoch=n,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=0,\n",
    "                    batch_size=4\n",
    "                )\n",
    "            n+=1\n",
    "            e+=1\n",
    "    model.save('C:/Users/DNS/Desktop/Progi/Python/NeuralNetworks/Курсач/models/sen_to_word'+str(d)+'.h5')\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/decoder/models/sen_to_word'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=1\n",
    "n=0\n",
    "e=1\n",
    "for d in range(11,20):\n",
    "    print('Epoch: '+str(d)+'\\n'+\"**********************************\\n\")\n",
    "    for i in range(200):\n",
    "        vectors_train = np.load('D:/decoder/data/data3000/news_vectors'+str(999-i)+'.npy')\n",
    "        #print(start)\n",
    "        texts = textreader(start)\n",
    "        start+=500\n",
    "        #print(len(texts))\n",
    "\n",
    "        for j in range(5):\n",
    "            tt, kol = word_vectors_creator(texts[0+j*100:(1+j)*100])\n",
    "            vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "            pos = positioncreator(kol)\n",
    "\n",
    "            if n%100==0:\n",
    "                model.fit(\n",
    "                    x=[vtt,\n",
    "                       pos],\n",
    "                    y = tt,\n",
    "                    epochs=e, initial_epoch=n,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=1\n",
    "                    #batch_size=6\n",
    "                )\n",
    "            else:\n",
    "                model.fit(\n",
    "                    x=[vtt,\n",
    "                       pos],\n",
    "                    y = tt,\n",
    "                    epochs=e, initial_epoch=n,\n",
    "                    validation_split=0.05,\n",
    "                    verbose=0\n",
    "                    #batch_size=6\n",
    "                )\n",
    "            n+=1\n",
    "            e+=1\n",
    "    model.save('D:/decoder/models/sen_to_word'+str(d)+'.h5')\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
