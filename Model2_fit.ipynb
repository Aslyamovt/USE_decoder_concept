{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Lambda,RepeatVector,Dense,Reshape,Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "def repeat_vector(args):\n",
    "        layer_to_repeat = args[0]\n",
    "        sequence_layer = args[1]\n",
    "        return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "    \n",
    "def concatenate_vectors(args):\n",
    "    return K.concatenate([args[0],args[1]],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_transformer import *\n",
    "\n",
    "import numpy as np\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "\n",
    "\n",
    "def get_m(token_num,\n",
    "              embed_dim,\n",
    "              encoder_num,\n",
    "              decoder_num,\n",
    "              head_num,\n",
    "              hidden_dim,\n",
    "              attention_activation=None,\n",
    "              feed_forward_activation='relu',\n",
    "              dropout_rate=0.0,\n",
    "              embed_weights =None,\n",
    "              embed_trainable=None,\n",
    "              trainable=True,\n",
    "              use_adapter=False,\n",
    "              adapter_units=None,\n",
    "              adapter_activation='relu'):\n",
    "    \"\"\"Get full model without compilation.\n",
    "    :param token_num: Number of distinct tokens.\n",
    "    :param embed_dim: Dimension of token embedding.\n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n",
    "                           `embed_trainable` should be lists of two elements if it is False.\n",
    "    :param embed_weights: Initial weights of token embedding.\n",
    "    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n",
    "                            value is None when embedding weights has been provided.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :param use_adapter: Whether to use feed-forward adapters before each residual connections.\n",
    "    :param adapter_units: The dimension of the first transformation in feed-forward adapter.\n",
    "    :param adapter_activation: The activation after the first transformation in feed-forward adapter.\n",
    "    :return: Keras model.\n",
    "    \"\"\"\n",
    "    decoder_token_num = token_num\n",
    "\n",
    "    decoder_embed_weights = embed_weights\n",
    "\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    "\n",
    "    decoder_embed_trainable = embed_trainable\n",
    "\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    "\n",
    "\n",
    "    decoder_embed_layer = EmbeddingRet(\n",
    "        input_dim=decoder_token_num,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        weights=decoder_embed_weights,\n",
    "        trainable=decoder_embed_trainable,\n",
    "        name='Decoder-Token-Embedding',\n",
    "    )\n",
    "    \n",
    "    vector = Input(shape=(512,), name='Vectors-Input',dtype='float32')\n",
    "    positions = Input(shape=(None,10), name='Positions-Input',dtype='float32')\n",
    "    vectors_repeated = Lambda(repeat_vector, output_shape=(None, 512), name='Vectors-Repeater') ([vector, positions])\n",
    "    encoded_inputs_concatenated = Lambda(concatenate_vectors, output_shape=(None, 522), name='Encoded-Inputs-Concatenator') ([vectors_repeated, positions])\n",
    "    encoder_input_divider1 = Dense(embed_dim*4, name='Encoder-Output-Divider-1',activation='selu')(encoded_inputs_concatenated)\n",
    "    encoder_input_dropout1=Dropout(0.1, name='Encoder-Output-Dropout-1')(encoder_input_divider1)\n",
    "    encoder_input_divider2 = Dense(embed_dim*2, name='Encoder-Output-Divider-2',activation='selu')(encoder_input_dropout1)\n",
    "    encoder_input_dropout2=Dropout(0.1, name='Encoder-Output-Dropout-2')(encoder_input_divider2)\n",
    "    encoder_input_divider3 = Dense(embed_dim, name='Encoder-Output-Divider-3',activation='selu')( encoder_input_dropout2)\n",
    "    encoder_input_dropout3=Dropout(0.1, name='Encoder-Output-Dropout-3')(encoder_input_divider3)\n",
    "    encoder_input_divider4 = Dense(embed_dim, name='Encoder-Output-Divider-4',activation='selu')(encoder_input_dropout3)\n",
    "    \n",
    "    encoded_layer = get_encoders(\n",
    "        encoder_num=encoder_num,\n",
    "        input_layer=encoder_input_divider4,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input') \n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=encoded_layer,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    dense_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    return keras.models.Model(inputs=[vector,positions,decoder_input], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecesextender(data):\n",
    "    kol= np.zeros(shape=(data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        kol[i]=np.count_nonzero(data[i])\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    buf = 0\n",
    "    for g in range(data.shape[0]):        \n",
    "        for k in range(1,kol[g]):\n",
    "            out[buf][0]=3000\n",
    "            out[buf][1:513-k]+=data[g][k:]\n",
    "            buf+=1\n",
    "    return out, kol\n",
    "\n",
    "def vectorsextender(data,kol):\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    buf=0\n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(kol[g]-1):\n",
    "            out[buf]+=data[g]\n",
    "            buf+=1\n",
    "    return out\n",
    "\n",
    "def positionencoder(kol):\n",
    "    out = np.zeros(shape=(np.sum(kol),512,10))\n",
    "    buf=0\n",
    "    for g in range(len(kol)):\n",
    "        for k in range(kol[g]-1):\n",
    "            #out[buf]+=binarycalculator(k+1)\n",
    "            out[buf]+=binarycalculator(512)\n",
    "            buf+=1\n",
    "    return out\n",
    "            \n",
    "def binarycalculator(k):\n",
    "    buf=np.zeros(shape=(10))\n",
    "    out=np.zeros(shape=(512,10))\n",
    "    for i in range(k):\n",
    "        buf=numbertoarray(i+1)\n",
    "        out[i]=out[i]+buf\n",
    "    for i in range(512-k):\n",
    "        out[i+k]=out[i+k]+buf\n",
    "    return out\n",
    "    \n",
    "    \n",
    "def numbertoarray(m):\n",
    "    out = np.zeros(shape=(10))\n",
    "    powers=[512,256,128,64,32,16,8,4,2,1]\n",
    "    for i in range(10):\n",
    "        a=m//powers[i]\n",
    "        if a>=1 and a<2:\n",
    "            out[i] = 1 \n",
    "            m-=powers[i]\n",
    "        if m == 0:\n",
    "            break\n",
    "    return out           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Vectors-Input (InputLayer)      (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Positions-Input (InputLayer)    (None, None, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Vectors-Repeater (Lambda)       (None, None, 512)    0           Vectors-Input[0][0]              \n",
      "                                                                 Positions-Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoded-Inputs-Concatenator (La (None, None, 522)    0           Vectors-Repeater[0][0]           \n",
      "                                                                 Positions-Input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Divider-1 (Dense (None, None, 256)    133888      Encoded-Inputs-Concatenator[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Dropout-1 (Dropo (None, None, 256)    0           Encoder-Output-Divider-1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Divider-2 (Dense (None, None, 128)    32896       Encoder-Output-Dropout-1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Dropout-2 (Dropo (None, None, 128)    0           Encoder-Output-Divider-2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Divider-3 (Dense (None, None, 64)     8256        Encoder-Output-Dropout-2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Dropout-3 (Dropo (None, None, 64)     0           Encoder-Output-Divider-3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Divider-4 (Dense (None, None, 64)     4160        Encoder-Output-Dropout-3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 64)     16640       Encoder-Output-Divider-4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 64)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 64)     0           Encoder-Output-Divider-4[0][0]   \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 64)     128         Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 64)     15544       Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 64)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 64)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 64)     128         Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 64)     16640       Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 64)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 64)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 64)     128         Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 64)     15544       Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 64)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 64)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 64)     128         Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 64)     16640       Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 64)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Token-Embedding (Embedd [(None, None, 64), ( 192192      Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 64)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 64)     0           Decoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 64)     128         Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 64)     15544       Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 64)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 64)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 64)     128         Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 64)     15544       Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 64)     0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 64)     0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 64)     15544       Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 64)     0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 64)     0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward (FeedForw (None, None, 64)     15544       Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Dropout ( (None, None, 64)     0           Decoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Add (Add) (None, None, 64)     0           Decoder-3-MultiHeadQueryAttention\n",
      "                                                                 Decoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward (FeedForw (None, None, 64)     15544       Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Dropout ( (None, None, 64)     0           Decoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Add (Add) (None, None, 64)     0           Decoder-4-MultiHeadQueryAttention\n",
      "                                                                 Decoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward (FeedForw (None, None, 64)     15544       Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward-Dropout ( (None, None, 64)     0           Decoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward-Add (Add) (None, None, 64)     0           Decoder-5-MultiHeadQueryAttention\n",
      "                                                                 Decoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward (FeedForw (None, None, 64)     15544       Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward-Dropout ( (None, None, 64)     0           Decoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward-Add (Add) (None, None, 64)     0           Decoder-6-MultiHeadQueryAttention\n",
      "                                                                 Decoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-7-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-7-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-7-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-FeedForward (FeedForw (None, None, 64)     15544       Decoder-7-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-FeedForward-Dropout ( (None, None, 64)     0           Decoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-FeedForward-Add (Add) (None, None, 64)     0           Decoder-7-MultiHeadQueryAttention\n",
      "                                                                 Decoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-7-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadSelfAttentio (None, None, 64)     16640       Decoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadSelfAttentio (None, None, 64)     0           Decoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadSelfAttentio (None, None, 64)     128         Decoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadQueryAttenti (None, None, 64)     16640       Decoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-8-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadQueryAttenti (None, None, 64)     0           Decoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-8-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-MultiHeadQueryAttenti (None, None, 64)     128         Decoder-8-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-FeedForward (FeedForw (None, None, 64)     15544       Decoder-8-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-FeedForward-Dropout ( (None, None, 64)     0           Decoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-FeedForward-Add (Add) (None, None, 64)     0           Decoder-8-MultiHeadQueryAttention\n",
      "                                                                 Decoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-8-FeedForward-Norm (Lay (None, None, 64)     128         Decoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 3003)   3003        Decoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-Token-Embedding[0][1]    \n",
      "==================================================================================================\n",
      "Total params: 865,379\n",
      "Trainable params: 673,187\n",
      "Non-trainable params: 192,192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_m(\n",
    "    token_num=3003,\n",
    "    embed_dim=64,\n",
    "    encoder_num=3,\n",
    "    decoder_num=8,\n",
    "    head_num=4,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((3003, 64)),\n",
    "    use_adapter=True,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adagrad',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "0.001\n",
      "0.0002\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model ('D:/decoder/models/m5.h5' , custom_objects = get_custom_objects ())\n",
    "print(K.get_value(model.optimizer.lr))\n",
    "K.set_value(model.optimizer.lr, .0002)\n",
    "print(K.get_value(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed\n",
      "Train on 5453 samples, validate on 287 samples\n",
      "Epoch 1/1\n",
      "5453/5453 [==============================] - 433s 79ms/step - loss: 7.1631 - val_loss: 3.8508\n",
      "Train on 4615 samples, validate on 243 samples\n",
      "Epoch 2/2\n",
      "4615/4615 [==============================] - 355s 77ms/step - loss: 5.7817 - val_loss: 3.7864\n",
      "Train on 4569 samples, validate on 241 samples\n",
      "Epoch 3/3\n",
      "4569/4569 [==============================] - 351s 77ms/step - loss: 5.5983 - val_loss: 3.4127\n",
      "Train on 5488 samples, validate on 289 samples\n",
      "Epoch 4/4\n",
      "5488/5488 [==============================] - 423s 77ms/step - loss: 5.6053 - val_loss: 3.6657\n",
      "Train on 4372 samples, validate on 231 samples\n",
      "Epoch 5/5\n",
      "4372/4372 [==============================] - 337s 77ms/step - loss: 5.2134 - val_loss: 3.1306\n",
      "Readed\n",
      "Train on 5344 samples, validate on 282 samples\n",
      "Epoch 6/6\n",
      "5344/5344 [==============================] - 412s 77ms/step - loss: 5.5773 - val_loss: 4.2312\n",
      "Train on 5092 samples, validate on 268 samples\n",
      "Epoch 7/7\n",
      "5092/5092 [==============================] - 391s 77ms/step - loss: 5.5280 - val_loss: 3.1595\n",
      "Train on 5563 samples, validate on 293 samples\n",
      "Epoch 8/8\n",
      "5563/5563 [==============================] - 429s 77ms/step - loss: 5.4457 - val_loss: 3.3203\n",
      "Train on 4247 samples, validate on 224 samples\n",
      "Epoch 9/9\n",
      "4247/4247 [==============================] - 326s 77ms/step - loss: 5.3103 - val_loss: 3.7233\n",
      "Train on 5357 samples, validate on 282 samples\n",
      "Epoch 10/10\n",
      "5357/5357 [==============================] - 411s 77ms/step - loss: 5.4983 - val_loss: 4.7454\n",
      "Readed\n",
      "Train on 5131 samples, validate on 271 samples\n",
      "Epoch 11/11\n",
      "5131/5131 [==============================] - 401s 78ms/step - loss: 5.5298 - val_loss: 3.0636\n",
      "Train on 5918 samples, validate on 312 samples\n",
      "Epoch 12/12\n",
      "5918/5918 [==============================] - 455s 77ms/step - loss: 5.8007 - val_loss: 3.9049\n",
      "Train on 4452 samples, validate on 235 samples\n",
      "Epoch 13/13\n",
      "4452/4452 [==============================] - 343s 77ms/step - loss: 5.3438 - val_loss: 3.2720\n",
      "Train on 4645 samples, validate on 245 samples\n",
      "Epoch 14/14\n",
      "4645/4645 [==============================] - 357s 77ms/step - loss: 5.1749 - val_loss: 2.5946\n",
      "Train on 2836 samples, validate on 150 samples\n",
      "Epoch 15/15\n",
      "2836/2836 [==============================] - 219s 77ms/step - loss: 5.2063 - val_loss: 1.2184\n",
      "Readed\n",
      "Train on 2452 samples, validate on 130 samples\n",
      "Epoch 16/16\n",
      "2452/2452 [==============================] - 189s 77ms/step - loss: 4.9928 - val_loss: 0.8456\n",
      "Train on 2401 samples, validate on 127 samples\n",
      "Epoch 17/17\n",
      "2401/2401 [==============================] - 185s 77ms/step - loss: 4.9948 - val_loss: 0.9626\n",
      "Train on 2326 samples, validate on 123 samples\n",
      "Epoch 18/18\n",
      "2326/2326 [==============================] - 179s 77ms/step - loss: 4.6987 - val_loss: 0.7286\n",
      "Train on 2794 samples, validate on 148 samples\n",
      "Epoch 19/19\n",
      "2794/2794 [==============================] - 216s 77ms/step - loss: 5.0200 - val_loss: 1.2616\n",
      "Train on 4109 samples, validate on 217 samples\n",
      "Epoch 20/20\n",
      "4109/4109 [==============================] - 317s 77ms/step - loss: 5.5679 - val_loss: 3.1079\n",
      "Readed\n",
      "Train on 5033 samples, validate on 265 samples\n",
      "Epoch 21/21\n",
      "5033/5033 [==============================] - 392s 78ms/step - loss: 5.5645 - val_loss: 3.3683\n",
      "Train on 4720 samples, validate on 249 samples\n",
      "Epoch 22/22\n",
      "4720/4720 [==============================] - 363s 77ms/step - loss: 5.3993 - val_loss: 3.2118\n",
      "Train on 5108 samples, validate on 269 samples\n",
      "Epoch 23/23\n",
      "5108/5108 [==============================] - 395s 77ms/step - loss: 5.3485 - val_loss: 3.6247\n",
      "Train on 5851 samples, validate on 308 samples\n",
      "Epoch 24/24\n",
      "5851/5851 [==============================] - 452s 77ms/step - loss: 5.7880 - val_loss: 4.1121\n",
      "Train on 5430 samples, validate on 286 samples\n",
      "Epoch 25/25\n",
      "5430/5430 [==============================] - 419s 77ms/step - loss: 5.5839 - val_loss: 3.6536\n",
      "Readed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2f299f7ad13d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mpot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mvtt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorsextender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositionencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-428e74472fe8>\u001b[0m in \u001b[0;36mpositionencoder\u001b[1;34m(kol)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m#out[buf]+=binarycalculator(k+1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mbinarycalculator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mbuf\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-428e74472fe8>\u001b[0m in \u001b[0;36mbinarycalculator\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbinarycalculator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mbuf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumbertoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "n=0\n",
    "e=1\n",
    "bo = True\n",
    "\n",
    "for i in range(1000):\n",
    "    vectors_train = np.load('D:/decoder/data/data3000/news_vectors'+str(i)+'.npy')\n",
    "    pieces_train = np.load('D:/decoder/data/data3000/news_pieces'+str(i)+'.npy')\n",
    "    shifting = np.ones(shape=(pieces_train.shape[0],512))\n",
    "    pieces_train = pieces_train+shifting\n",
    "    \n",
    "    print('Readed')\n",
    "    \n",
    "    '''\n",
    "    if i == 32:\n",
    "        K.set_value(model.optimizer.lr, .005)\n",
    "        print(K.get_value(model.optimizer.lr))\n",
    "        model.save('/home/shared/decoder/models/m'+str(i)+'.h5')\n",
    "        \n",
    "    if i == 38:\n",
    "        K.set_value(model.optimizer.lr, .002)\n",
    "        print(K.get_value(model.optimizer.lr)) \n",
    "        model.save('/home/shared/decoder/models/m'+str(i)+'.h5')\n",
    "        \n",
    "    if i == 42:\n",
    "        K.set_value(model.optimizer.lr, .001)\n",
    "        print(K.get_value(model.optimizer.lr)) \n",
    "        model.save('/home/shared/decoder/models/m'+str(i)+'.h5')\n",
    "        \n",
    "    if i == 47:\n",
    "        K.set_value(model.optimizer.lr, .0005)\n",
    "        print(K.get_value(model.optimizer.lr))\n",
    "        model.save('/home/shared/decoder/models/m'+str(i)+'.h5')\n",
    "        bo =True\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    for j in range(5):\n",
    "        ptt, kol = piecesextender(pieces_train[0+j*100:(1+j)*100])\n",
    "        pot = np.copy(ptt)\n",
    "        pot = shift(pot, (0,-1))\n",
    "        pot = pot.reshape(pot.shape[0],512,1)\n",
    "        vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "        pos = positionencoder(kol)\n",
    "        \n",
    "        \n",
    "        model.fit(\n",
    "            x=[vtt,\n",
    "               pos,\n",
    "               ptt],\n",
    "            y = pot,\n",
    "            epochs=e, initial_epoch=n,\n",
    "            validation_split=0.05,\n",
    "            batch_size=6\n",
    "        )\n",
    "        n+=1\n",
    "        e+=1\n",
    "\n",
    "    if i%5==0 and bo:\n",
    "        model.save('D:/decoder/models/m'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/decoder/models/m4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_position_encoding(\n",
    "    length, hidden_size, min_timescale=1.0, max_timescale=1.0e4):\n",
    "  \"\"\"Return positional encoding.\n",
    "  Calculates the position encoding as a mix of sine and cosine functions with\n",
    "  geometrically increasing wavelengths.\n",
    "  Defined and formulized in Attention is All You Need, section 3.5.\n",
    "  Args:\n",
    "    length: Sequence length.\n",
    "    hidden_size: Size of the\n",
    "    min_timescale: Minimum scale that will be applied at each position\n",
    "    max_timescale: Maximum scale that will be applied at each position\n",
    "  Returns:\n",
    "    Tensor with shape [length, hidden_size]\n",
    "  \"\"\"\n",
    "  # We compute the positional encoding in float32 even if the model uses\n",
    "  # float16, as many of the ops used, like log and exp, are numerically unstable\n",
    "  # in float16.\n",
    "  position = tf.cast(tf.range(length), tf.float32)\n",
    "  num_timescales = hidden_size // 2\n",
    "  log_timescale_increment = (\n",
    "      math.log(float(max_timescale) / float(min_timescale)) /\n",
    "      (tf.cast(num_timescales, tf.float32) - 1))\n",
    "  inv_timescales = min_timescale * tf.exp(\n",
    "      tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n",
    "  scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "  return signal\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
