{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras_transformer import *\n",
    "\n",
    "import numpy as np\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "import keras\n",
    "from keras.layers import Dense,Reshape\n",
    "\n",
    "def get_m(token_num,\n",
    "              embed_dim,\n",
    "              decoder_num,\n",
    "              head_num,\n",
    "              hidden_dim,\n",
    "              attention_activation=None,\n",
    "              feed_forward_activation='relu',\n",
    "              dropout_rate=0.0,\n",
    "              embed_weights =None,\n",
    "              embed_trainable=None,\n",
    "              trainable=True,\n",
    "              use_adapter=False,\n",
    "              adapter_units=None,\n",
    "              adapter_activation='relu'):\n",
    "    \"\"\"Get full model without compilation.\n",
    "    :param token_num: Number of distinct tokens.\n",
    "    :param embed_dim: Dimension of token embedding.\n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n",
    "                           `embed_trainable` should be lists of two elements if it is False.\n",
    "    :param embed_weights: Initial weights of token embedding.\n",
    "    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n",
    "                            value is None when embedding weights has been provided.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :param use_adapter: Whether to use feed-forward adapters before each residual connections.\n",
    "    :param adapter_units: The dimension of the first transformation in feed-forward adapter.\n",
    "    :param adapter_activation: The activation after the first transformation in feed-forward adapter.\n",
    "    :return: Keras model.\n",
    "    \"\"\"\n",
    "    decoder_token_num = token_num\n",
    "\n",
    "    decoder_embed_weights = embed_weights\n",
    "\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    "\n",
    "    decoder_embed_trainable = embed_trainable\n",
    "\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    "\n",
    "\n",
    "    decoder_embed_layer = EmbeddingRet(\n",
    "        input_dim=decoder_token_num,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        weights=decoder_embed_weights,\n",
    "        trainable=decoder_embed_trainable,\n",
    "        name='Decoder-Token-Embedding',\n",
    "    )\n",
    "    encoder_output = keras.layers.Input(shape=(1,512), name='Encoder-Output',dtype='float32')\n",
    "    encoder_output_divider1 = Dense(512,input_shape=(None,1,512), name='Encoder-Output-Divider-1',activation='tanh')(encoder_output)\n",
    "    #divider_reshape1=Reshape(target_shape=(1,512),name='Divider-Reshape-1')(encoder_output_divider1)\n",
    "    encoder_output_divider2 = Dense(512,input_shape=(None,1,512), name='Encoder-Output-Divider-2',activation='relu')(encoder_output_divider1)#(divider_reshape1)\n",
    "    #divider_reshape2=Reshape(target_shape=(1,512),name='Divider-Reshape-2')(encoder_output_divider2)\n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input')\n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=encoder_output_divider2,#divider_reshape2,#encoder_output,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "        use_adapter=use_adapter,\n",
    "        adapter_units=adapter_units,\n",
    "        adapter_activation=adapter_activation,\n",
    "    )\n",
    "    dense_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    return keras.models.Model(inputs=[encoder_output, decoder_input], outputs=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecesextender(data):\n",
    "    kol= np.zeros(shape=(data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        kol[i]=np.count_nonzero(data[i])\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    buf = 0\n",
    "    for g in range(data.shape[0]):        \n",
    "        for k in range(1,kol[g]):\n",
    "            out[buf][0]=3000\n",
    "            out[buf][1:513-k]+=data[g][k:]\n",
    "            buf+=1\n",
    "    return out, kol\n",
    "\n",
    "def vectorsextender(data,kol):\n",
    "    out = np.zeros(shape=(np.sum(kol),512))\n",
    "    buf=0\n",
    "    for g in range(data.shape[0]):\n",
    "        for k in range(kol[g]-1):\n",
    "            out[buf]+=data[g]\n",
    "            buf+=1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Token-Embedding (Embedd [(None, None, 512),  1537536     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 512)    0           Decoder-Token-Embedding[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (InputLayer)     (None, 1, 512)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Divider-1 (Dense (None, 1, 512)       262656      Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output-Divider-2 (Dense (None, 1, 512)       262656      Encoder-Output-Divider-1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 512)    123512      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 512)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 512)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 512)    123512      Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 512)    0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 512)    0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward (FeedForw (None, None, 512)    123512      Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Dropout ( (None, None, 512)    0           Decoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Add (Add) (None, None, 512)    0           Decoder-3-MultiHeadQueryAttention\n",
      "                                                                 Decoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1050624     Decoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    0           Decoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 512)    1024        Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1050624     Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "                                                                 Encoder-Output-Divider-2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    0           Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 512)    1024        Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward (FeedForw (None, None, 512)    123512      Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Dropout ( (None, None, 512)    0           Decoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Add (Add) (None, None, 512)    0           Decoder-4-MultiHeadQueryAttention\n",
      "                                                                 Decoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Norm (Lay (None, None, 512)    1024        Decoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 3003)   3003        Decoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-Token-Embedding[0][1]    \n",
      "==================================================================================================\n",
      "Total params: 10,977,179\n",
      "Trainable params: 9,439,643\n",
      "Non-trainable params: 1,537,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "model = get_m(\n",
    "    token_num=3003,\n",
    "    embed_dim=512,\n",
    "    decoder_num=4,\n",
    "    head_num=4,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((3003, 512)),\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model ('D:/decoder/models/m5.h5' , custom_objects = get_custom_objects ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 26/26\n",
      "5318/5318 [==============================] - 316s 59ms/step - loss: 2.0357 - val_loss: 3.9100\n",
      "Train on 5083 samples, validate on 268 samples\n",
      "Epoch 27/27\n",
      "5083/5083 [==============================] - 295s 58ms/step - loss: 1.8933 - val_loss: 1.6678\n",
      "Train on 5615 samples, validate on 296 samples\n",
      "Epoch 28/28\n",
      "5615/5615 [==============================] - 327s 58ms/step - loss: 1.7650 - val_loss: 2.4289\n",
      "Train on 4158 samples, validate on 219 samples\n",
      "Epoch 29/29\n",
      "4158/4158 [==============================] - 240s 58ms/step - loss: 1.9302 - val_loss: 2.3757\n",
      "Train on 5320 samples, validate on 281 samples\n",
      "Epoch 30/30\n",
      "5320/5320 [==============================] - 306s 58ms/step - loss: 1.9759 - val_loss: 5.6564\n",
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 31/31\n",
      "5318/5318 [==============================] - 307s 58ms/step - loss: 1.7515 - val_loss: 3.7778\n",
      "Train on 5083 samples, validate on 268 samples\n",
      "Epoch 32/32\n",
      "5083/5083 [==============================] - 295s 58ms/step - loss: 1.7489 - val_loss: 1.7792\n",
      "Train on 5615 samples, validate on 296 samples\n",
      "Epoch 33/33\n",
      "5615/5615 [==============================] - 329s 59ms/step - loss: 1.7528 - val_loss: 2.1311\n",
      "Train on 4158 samples, validate on 219 samples\n",
      "Epoch 34/34\n",
      "4158/4158 [==============================] - 244s 59ms/step - loss: 1.9353 - val_loss: 2.1772\n",
      "Train on 5320 samples, validate on 281 samples\n",
      "Epoch 35/35\n",
      "5320/5320 [==============================] - 312s 59ms/step - loss: 1.7722 - val_loss: 5.6165\n",
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 36/36\n",
      "5318/5318 [==============================] - 311s 58ms/step - loss: 1.6832 - val_loss: 3.9625\n",
      "Train on 5083 samples, validate on 268 samples\n",
      "Epoch 37/37\n",
      "5083/5083 [==============================] - 296s 58ms/step - loss: 1.5730 - val_loss: 1.6736\n",
      "Train on 5615 samples, validate on 296 samples\n",
      "Epoch 38/38\n",
      "5615/5615 [==============================] - 327s 58ms/step - loss: 1.5838 - val_loss: 2.4075\n",
      "Train on 4158 samples, validate on 219 samples\n",
      "Epoch 39/39\n",
      "4158/4158 [==============================] - 243s 58ms/step - loss: 1.8230 - val_loss: 2.3191\n",
      "Train on 5320 samples, validate on 281 samples\n",
      "Epoch 40/40\n",
      "5320/5320 [==============================] - 308s 58ms/step - loss: 1.6911 - val_loss: 5.6456\n",
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 41/41\n",
      "5318/5318 [==============================] - 308s 58ms/step - loss: 1.5668 - val_loss: 3.9836\n",
      "Train on 5083 samples, validate on 268 samples\n",
      "Epoch 42/42\n",
      "5083/5083 [==============================] - 295s 58ms/step - loss: 1.5403 - val_loss: 1.8919\n",
      "Train on 5615 samples, validate on 296 samples\n",
      "Epoch 43/43\n",
      "5615/5615 [==============================] - 331s 59ms/step - loss: 1.5530 - val_loss: 2.4638\n",
      "Train on 4158 samples, validate on 219 samples\n",
      "Epoch 44/44\n",
      "4158/4158 [==============================] - 241s 58ms/step - loss: 1.7090 - val_loss: 2.4375\n",
      "Train on 5320 samples, validate on 281 samples\n",
      "Epoch 45/45\n",
      "5320/5320 [==============================] - 307s 58ms/step - loss: 1.6974 - val_loss: 5.5811\n",
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 46/46\n",
      "5318/5318 [==============================] - 307s 58ms/step - loss: 1.7285 - val_loss: 4.0313\n",
      "Train on 5083 samples, validate on 268 samples\n",
      "Epoch 47/47\n",
      "5083/5083 [==============================] - 294s 58ms/step - loss: 1.5273 - val_loss: 1.9815\n",
      "Train on 5615 samples, validate on 296 samples\n",
      "Epoch 48/48\n",
      "5615/5615 [==============================] - 324s 58ms/step - loss: 1.5244 - val_loss: 2.7166\n",
      "Train on 4158 samples, validate on 219 samples\n",
      "Epoch 49/49\n",
      "4158/4158 [==============================] - 240s 58ms/step - loss: 1.6272 - val_loss: 2.3343\n",
      "Train on 5320 samples, validate on 281 samples\n",
      "Epoch 50/50\n",
      "5320/5320 [==============================] - 307s 58ms/step - loss: 1.7170 - val_loss: 5.6440\n",
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 51/51\n",
      "5318/5318 [==============================] - 309s 58ms/step - loss: 1.6155 - val_loss: 3.9886\n",
      "Train on 5083 samples, validate on 268 samples\n",
      "Epoch 52/52\n",
      "5083/5083 [==============================] - 296s 58ms/step - loss: 1.4895 - val_loss: 1.7587\n",
      "Train on 5615 samples, validate on 296 samples\n",
      "Epoch 53/53\n",
      "5615/5615 [==============================] - 325s 58ms/step - loss: 1.5201 - val_loss: 2.5237\n",
      "Train on 4158 samples, validate on 219 samples\n",
      "Epoch 54/54\n",
      "4158/4158 [==============================] - 240s 58ms/step - loss: 1.7352 - val_loss: 2.2756\n",
      "Train on 5320 samples, validate on 281 samples\n",
      "Epoch 55/55\n",
      "5320/5320 [==============================] - 308s 58ms/step - loss: 1.6126 - val_loss: 5.4222\n",
      "Readed\n",
      "Train on 5318 samples, validate on 280 samples\n",
      "Epoch 56/56\n",
      " 200/5318 [>.............................] - ETA: 4:54 - loss: 4.8859"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-082466492b10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         )\n\u001b[0;32m     55\u001b[0m         \u001b[0mn\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "n=25\n",
    "e=26\n",
    "\n",
    "for i in range(5,1000):\n",
    "    vectors_train = np.load('D:\\\\decoder\\\\data\\\\data3000\\\\news_vectors'+str(i)+'.npy')\n",
    "    pieces_train = np.load('D:\\\\decoder\\\\data\\\\data3000\\\\news_pieces'+str(i)+'.npy')\n",
    "    shifting = np.ones(shape=(pieces_train.shape[0],512))\n",
    "    pieces_train = pieces_train+shifting\n",
    "    \n",
    "    print('Readed')\n",
    "    '''\n",
    "    ptt, kol = piecesextender(pieces_train)\n",
    "    shifting = np.ones(shape=(ptt.shape[0],512))\n",
    "    ptt = ptt + shifting\n",
    "    print('Pt converted')\n",
    "    pot = np.copy(ptt)\n",
    "    pot = shift(pot, (0,-1))\n",
    "    pot = pot.reshape(pot.shape[0],512,1)\n",
    "    print('Po converted')\n",
    "    vtt = vectorsextender(vectors_train, kol)\n",
    "    print('V converted')\n",
    "\n",
    "    model.fit(\n",
    "        x=[vtt.reshape(vtt.shape[0],1,512), \n",
    "           ptt],\n",
    "        y = pot,\n",
    "        epochs=i, initial_epoch=i+1,\n",
    "        validation_split=0.05,\n",
    "        #batch_size=1\n",
    "    )\n",
    "    '''\n",
    "        \n",
    "    for j in range(5):\n",
    "        ptt, kol = piecesextender(pieces_train[0+j*100:(1+j)*100])\n",
    "        #shifting = np.ones(shape=(ptt.shape[0],512))\n",
    "        #ptt = ptt + shifting\n",
    "        #print('Pt converted')\n",
    "        pot = np.copy(ptt)\n",
    "        pot = shift(pot, (0,-1))\n",
    "        pot = pot.reshape(pot.shape[0],512,1)\n",
    "        #print('Po converted')\n",
    "        vtt = vectorsextender(vectors_train[0+j*100:(1+j)*100], kol)\n",
    "        #print('V converted')\n",
    "        \n",
    "        \n",
    "        model.fit(\n",
    "            x=[vtt.reshape(vtt.shape[0],1,512), \n",
    "               ptt],\n",
    "            y = pot,\n",
    "            epochs=e, initial_epoch=n,\n",
    "            validation_split=0.05,\n",
    "            batch_size=4\n",
    "        )\n",
    "        n+=1\n",
    "        e+=1\n",
    "\n",
    "    if i%10==0:\n",
    "        model.save('D:\\\\decoder\\\\models\\\\m'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('D:\\\\decoder\\\\models\\\\m5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\DNS\\\\Desktop\\\\Progi\\\\Python\\\\NeuralNetworks\\\\Курсач\\\\models\\\\m5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
