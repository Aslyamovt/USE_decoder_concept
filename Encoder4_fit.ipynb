{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\dec2\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Vectors-Input (InputLayer)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Input-Dense (Dense)          (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "Vectors-Input-Reshaped (Resh (None, 2, 512, 1)         0         \n",
      "_________________________________________________________________\n",
      "Deconvolution1 (Conv2DTransp (None, 3, 512, 64)        192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution2 (Conv2DTransp (None, 4, 512, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution3 (Conv2DTransp (None, 5, 512, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution4 (Conv2DTransp (None, 6, 512, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution5 (Conv2DTransp (None, 7, 512, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution6 (Conv2DTransp (None, 8, 512, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution7 (Conv2DTransp (None, 9, 512, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 9, 512, 64)        256       \n",
      "_________________________________________________________________\n",
      "Deconvolution8 (Conv2DTransp (None, 10, 512, 64)       8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 10, 512, 64)       256       \n",
      "_________________________________________________________________\n",
      "Deconvolution9 (Conv2DTransp (None, 12, 512, 60)       11580     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 512, 60)       240       \n",
      "_________________________________________________________________\n",
      "Deconvolution10 (Conv2DTrans (None, 14, 512, 60)       10860     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 14, 512, 60)       240       \n",
      "_________________________________________________________________\n",
      "Deconvolution11 (Conv2DTrans (None, 16, 512, 60)       10860     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 512, 60)       240       \n",
      "_________________________________________________________________\n",
      "Deconvolution12 (Conv2DTrans (None, 18, 512, 60)       10860     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 18, 512, 60)       240       \n",
      "_________________________________________________________________\n",
      "Deconvolution13 (Conv2DTrans (None, 20, 512, 60)       10860     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 20, 512, 60)       240       \n",
      "_________________________________________________________________\n",
      "Deconvolution14 (Conv2DTrans (None, 23, 512, 55)       13255     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 23, 512, 55)       220       \n",
      "_________________________________________________________________\n",
      "Deconvolution15 (Conv2DTrans (None, 26, 512, 55)       12155     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 26, 512, 55)       220       \n",
      "_________________________________________________________________\n",
      "Deconvolution16 (Conv2DTrans (None, 29, 512, 55)       12155     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 29, 512, 55)       220       \n",
      "_________________________________________________________________\n",
      "Deconvolution17 (Conv2DTrans (None, 32, 512, 55)       12155     \n",
      "_________________________________________________________________\n",
      "Depadding1 (Conv2DTranspose) (None, 64, 512, 55)       12155     \n",
      "_________________________________________________________________\n",
      "Deconvolution18 (Conv2DTrans (None, 68, 512, 50)       13800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 68, 512, 50)       200       \n",
      "_________________________________________________________________\n",
      "Deconvolution19 (Conv2DTrans (None, 72, 512, 50)       12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 72, 512, 50)       200       \n",
      "_________________________________________________________________\n",
      "Deconvolution20 (Conv2DTrans (None, 76, 512, 50)       12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 76, 512, 50)       200       \n",
      "_________________________________________________________________\n",
      "Deconvolution21 (Conv2DTrans (None, 80, 512, 50)       12550     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 80, 512, 50)       200       \n",
      "_________________________________________________________________\n",
      "Deconvolution22 (Conv2DTrans (None, 85, 512, 45)       13545     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 85, 512, 45)       180       \n",
      "_________________________________________________________________\n",
      "Deconvolution23 (Conv2DTrans (None, 90, 512, 45)       12195     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 90, 512, 45)       180       \n",
      "_________________________________________________________________\n",
      "Deconvolution24 (Conv2DTrans (None, 95, 512, 45)       12195     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 95, 512, 45)       180       \n",
      "_________________________________________________________________\n",
      "Deconvolution25 (Conv2DTrans (None, 100, 512, 45)      12195     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 100, 512, 45)      180       \n",
      "_________________________________________________________________\n",
      "Deconvolution26 (Conv2DTrans (None, 107, 512, 40)      14440     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 107, 512, 40)      160       \n",
      "_________________________________________________________________\n",
      "Deconvolution27 (Conv2DTrans (None, 114, 512, 40)      12840     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 114, 512, 40)      160       \n",
      "_________________________________________________________________\n",
      "Deconvolution28 (Conv2DTrans (None, 121, 512, 40)      12840     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 121, 512, 40)      160       \n",
      "_________________________________________________________________\n",
      "Deconvolution29 (Conv2DTrans (None, 128, 512, 40)      12840     \n",
      "_________________________________________________________________\n",
      "Depadding2 (Conv2DTranspose) (None, 256, 512, 40)      12840     \n",
      "_________________________________________________________________\n",
      "Deconvolution30 (Conv2DTrans (None, 276, 512, 20)      16820     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 276, 512, 20)      80        \n",
      "_________________________________________________________________\n",
      "Deconvolution31 (Conv2DTrans (None, 296, 512, 20)      8420      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 296, 512, 20)      80        \n",
      "_________________________________________________________________\n",
      "Deconvolution32 (Conv2DTrans (None, 316, 512, 20)      8420      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 316, 512, 20)      80        \n",
      "_________________________________________________________________\n",
      "Deconvolution33 (Conv2DTrans (None, 336, 512, 20)      8420      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 336, 512, 20)      80        \n",
      "_________________________________________________________________\n",
      "Deconvolution34 (Conv2DTrans (None, 356, 512, 20)      8420      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 356, 512, 20)      80        \n",
      "_________________________________________________________________\n",
      "Deconvolution35 (Conv2DTrans (None, 396, 512, 15)      12315     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 396, 512, 15)      60        \n",
      "_________________________________________________________________\n",
      "Deconvolution36 (Conv2DTrans (None, 436, 512, 15)      9240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 436, 512, 15)      60        \n",
      "_________________________________________________________________\n",
      "Deconvolution37 (Conv2DTrans (None, 476, 512, 15)      9240      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 476, 512, 15)      60        \n",
      "_________________________________________________________________\n",
      "Deconvolution38 (Conv2DTrans (None, 512, 512, 1)       556       \n",
      "_________________________________________________________________\n",
      "Output-Reshaped (Reshape)    (None, 512, 512)          0         \n",
      "=================================================================\n",
      "Total params: 960,006\n",
      "Trainable params: 954,714\n",
      "Non-trainable params: 5,292\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Reshape, BatchNormalization, Conv2DTranspose, UpSampling2D, Activation\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vector = Input(shape=(512,), name='Vectors-Input',dtype='float32')\n",
    "inputDense=Dense(1024,name='Input-Dense')(vector)\n",
    "norm_input=BatchNormalization()(inputDense)\n",
    "activ_input1=Activation('relu')(norm_input)\n",
    "vectorReshaped = Reshape((2,512,1),name='Vectors-Input-Reshaped')(activ_input1)\n",
    "deconv1 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution1',activation='relu')(vectorReshaped)\n",
    "norm_deconv1=BatchNormalization()(deconv1)\n",
    "deconv2 = Conv2DTranspose(64, (2,1), strides=(1,1), name='Deconvolution2',activation='relu')(norm_deconv1)\n",
    "norm_deconv2=BatchNormalization()(deconv2)\n",
    "deconv3 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution3',activation='relu')(norm_deconv2)\n",
    "norm_deconv3=BatchNormalization()(deconv3)\n",
    "deconv4 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution4',activation='relu')(norm_deconv3)\n",
    "norm_deconv4=BatchNormalization()(deconv4)\n",
    "deconv5 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution5',activation='relu')(norm_deconv4)\n",
    "norm_deconv5=BatchNormalization()(deconv5)\n",
    "deconv6 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution6',activation='relu')(norm_deconv5)\n",
    "norm_deconv6=BatchNormalization()(deconv6)\n",
    "deconv7 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution7',activation='relu')(norm_deconv6)\n",
    "norm_deconv7=BatchNormalization()(deconv7)\n",
    "deconv8 = Conv2DTranspose(64, (2,1), strides=(1,1),name='Deconvolution8',activation='relu')(norm_deconv7)\n",
    "norm_deconv8=BatchNormalization()(deconv8)\n",
    "deconv9 = Conv2DTranspose(60, (3,1), strides=(1,1),name='Deconvolution9',activation='relu')(norm_deconv8)\n",
    "\n",
    "norm_deconv9=BatchNormalization()(deconv9)\n",
    "deconv10 = Conv2DTranspose(60, (3,1), strides=(1,1), name='Deconvolution10',activation='relu')(norm_deconv9)\n",
    "norm_deconv10=BatchNormalization()(deconv10)\n",
    "deconv11 = Conv2DTranspose(60, (3,1), strides=(1,1),name='Deconvolution11',activation='relu')(norm_deconv10)\n",
    "norm_deconv11=BatchNormalization()(deconv11)\n",
    "deconv12 = Conv2DTranspose(60, (3,1), strides=(1,1),name='Deconvolution12',activation='relu')(norm_deconv11)\n",
    "norm_deconv12=BatchNormalization()(deconv12)\n",
    "deconv13 = Conv2DTranspose(60, (3,1), strides=(1,1),name='Deconvolution13',activation='relu')(norm_deconv12)\n",
    "norm_deconv13=BatchNormalization()(deconv13)\n",
    "deconv14 = Conv2DTranspose(55, (4,1), strides=(1,1),name='Deconvolution14',activation='relu')(norm_deconv13)\n",
    "norm_deconv14=BatchNormalization()(deconv14)\n",
    "deconv15 = Conv2DTranspose(55, (4,1), strides=(1,1),name='Deconvolution15',activation='relu')(norm_deconv14)\n",
    "norm_deconv15=BatchNormalization()(deconv15)\n",
    "deconv16 = Conv2DTranspose(55, (4,1), strides=(1,1),name='Deconvolution16',activation='relu')(norm_deconv15)\n",
    "norm_deconv16=BatchNormalization()(deconv16)\n",
    "deconv17 = Conv2DTranspose(55, (4,1), strides=(1,1),name='Deconvolution17',activation='relu')(norm_deconv16)\n",
    "depad1=Conv2DTranspose(55, (4,1),padding='same', strides=(2,1),name='Depadding1',activation='relu')(deconv17)\n",
    "\n",
    "deconv18 = Conv2DTranspose(50, (5,1), strides=(1,1), name='Deconvolution18',activation='relu')(depad1)\n",
    "norm_deconv17=BatchNormalization()(deconv18)\n",
    "deconv19 = Conv2DTranspose(50, (5,1), strides=(1,1),name='Deconvolution19',activation='relu')(norm_deconv17)\n",
    "norm_deconv18=BatchNormalization()(deconv19)\n",
    "deconv20 = Conv2DTranspose(50, (5,1), strides=(1,1),name='Deconvolution20',activation='relu')(norm_deconv18)\n",
    "norm_deconv19=BatchNormalization()(deconv20)\n",
    "deconv21 = Conv2DTranspose(50, (5,1), strides=(1,1),name='Deconvolution21',activation='relu')(norm_deconv19)\n",
    "norm_deconv20=BatchNormalization()(deconv21)\n",
    "deconv22 = Conv2DTranspose(45, (6,1), strides=(1,1),name='Deconvolution22',activation='relu')(norm_deconv20)\n",
    "norm_deconv21=BatchNormalization()(deconv22)\n",
    "deconv23 = Conv2DTranspose(45, (6,1), strides=(1,1),name='Deconvolution23',activation='relu')(norm_deconv21)\n",
    "norm_deconv22=BatchNormalization()(deconv23)\n",
    "deconv24 = Conv2DTranspose(45, (6,1), strides=(1,1),name='Deconvolution24',activation='relu')(norm_deconv22)\n",
    "norm_deconv23=BatchNormalization()(deconv24)\n",
    "deconv25 = Conv2DTranspose(45, (6,1), strides=(1,1),name='Deconvolution25',activation='relu')(norm_deconv23)\n",
    "norm_deconv24=BatchNormalization()(deconv25)\n",
    "deconv26 = Conv2DTranspose(40, (8,1), strides=(1,1), name='Deconvolution26',activation='relu')(norm_deconv24)\n",
    "norm_deconv25=BatchNormalization()(deconv26)\n",
    "deconv27 = Conv2DTranspose(40, (8,1), strides=(1,1),name='Deconvolution27',activation='relu')(norm_deconv25)\n",
    "norm_deconv26=BatchNormalization()(deconv27)\n",
    "deconv28 = Conv2DTranspose(40, (8,1), strides=(1,1),name='Deconvolution28',activation='relu')(norm_deconv26)\n",
    "norm_deconv27=BatchNormalization()(deconv28)\n",
    "deconv29 = Conv2DTranspose(40, (8,1), strides=(1,1),name='Deconvolution29',activation='relu')(norm_deconv27)\n",
    "depad2=Conv2DTranspose(40, (8,1),padding='same', strides=(2,1),name='Depadding2',activation='relu')(deconv29)\n",
    "\n",
    "\n",
    "deconv30 = Conv2DTranspose(20, (21,1), strides=(1,1),name='Deconvolution30',activation='relu')(depad2)\n",
    "norm_deconv28=BatchNormalization()(deconv30)\n",
    "deconv31 = Conv2DTranspose(20, (21,1), strides=(1,1), name='Deconvolution31',activation='relu')(norm_deconv28)\n",
    "norm_deconv29=BatchNormalization()(deconv31)\n",
    "deconv32 = Conv2DTranspose(20, (21,1), strides=(1,1),name='Deconvolution32',activation='relu')(norm_deconv29)\n",
    "norm_deconv30=BatchNormalization()(deconv32)\n",
    "deconv33 = Conv2DTranspose(20, (21,1), strides=(1,1),name='Deconvolution33',activation='relu')(norm_deconv30)\n",
    "norm_deconv31=BatchNormalization()(deconv33)\n",
    "deconv34 = Conv2DTranspose(20, (21,1), strides=(1,1),name='Deconvolution34',activation='relu')(norm_deconv31)\n",
    "norm_deconv32=BatchNormalization()(deconv34)\n",
    "deconv35 = Conv2DTranspose(15, (41,1), strides=(1,1),name='Deconvolution35',activation='relu')(norm_deconv32)\n",
    "norm_deconv33=BatchNormalization()(deconv35)\n",
    "deconv36 = Conv2DTranspose(15, (41,1), strides=(1,1),name='Deconvolution36',activation='relu')(norm_deconv33)\n",
    "norm_deconv34=BatchNormalization()(deconv36)\n",
    "deconv37 = Conv2DTranspose(15, (41,1), strides=(1,1),name='Deconvolution37',activation='relu')(norm_deconv34)\n",
    "norm_deconv35=BatchNormalization()(deconv37)\n",
    "deconv38 = Conv2DTranspose(1, (37,1), strides=(1,1),name='Deconvolution38',activation='relu')(norm_deconv35)\n",
    "\n",
    "output = Reshape((512,512),name='Output-Reshaped')(deconv38)\n",
    "\n",
    "\n",
    "model = Model(inputs=vector,outputs=output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "from collections import Iterable\n",
    "import numpy as np\n",
    "\n",
    "bpemb_ru = BPEmb(lang=\"ru\", dim=100, vs=3000)\n",
    "embed = np.load('D:/112/decoder/embeddings.npy')\n",
    "\n",
    "def textreader(st,limit):\n",
    "    texts = []\n",
    "    with open(\"D:/112/decoder/data/data3000/np_rev2.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        i=0\n",
    "        for line in fr:\n",
    "            i+=1\n",
    "            if i<st:      \n",
    "                continue\n",
    "            texts.append(line.replace('\\n',''))\n",
    "            if i==st+limit-1:\n",
    "                break\n",
    "    return texts\n",
    "\n",
    "def word_vectors_creator(data):\n",
    "    l=len(data)\n",
    "    kol= np.zeros(shape=(l))\n",
    "    pieces=[]\n",
    "    for i in range(l):\n",
    "        s = bpemb_ru.encode_ids(data[i])\n",
    "        kol[i]+=len(s)\n",
    "        pieces.append(s)\n",
    "    kol = kol.astype(np.int)\n",
    "    out = np.zeros(shape=(l,512,512))\n",
    "    for g in range(l):\n",
    "        out[g][kol[g]] = np.ones(512)\n",
    "        for k in range(1,kol[g]-1):\n",
    "            out[g][k+1]+=embed[pieces[g][k]]\n",
    "        out[g][kol[g]+2:]-= np.ones(shape=(512-kol[g]-2,512))\n",
    "        out[g][kol[g]+2:]-= np.ones(shape=(512-kol[g]-2,512))    \n",
    "    return out, kol       #, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "**********************************\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1/1\n",
      "475/475 [==============================] - 46s 97ms/step - loss: 3.8634 - val_loss: 3.6591\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 21/21\n",
      "475/475 [==============================] - 31s 65ms/step - loss: 3.6351 - val_loss: 3.7040\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 41/41\n",
      "475/475 [==============================] - 30s 64ms/step - loss: 3.6162 - val_loss: 3.6402\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 61/61\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5676 - val_loss: 3.6287\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 81/81\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6206 - val_loss: 3.7233\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 101/101\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6104 - val_loss: 3.5903\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 121/121\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6519 - val_loss: 3.6918\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 141/141\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6891 - val_loss: 3.6505\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 161/161\n",
      "474/474 [==============================] - 27s 57ms/step - loss: 3.6217 - val_loss: 3.5940\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 181/181\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6522 - val_loss: 3.5978\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 201/201\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5875 - val_loss: 3.6218\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 221/221\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6257 - val_loss: 3.6599\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 241/241\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5618 - val_loss: 3.5578\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 261/261\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5736 - val_loss: 3.6534\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 281/281\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5532 - val_loss: 3.5237\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 301/301\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6338 - val_loss: 3.5831\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 321/321\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5532 - val_loss: 3.6618\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 341/341\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6083 - val_loss: 3.5493\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 361/361\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5780 - val_loss: 3.5806\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 381/381\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.6032 - val_loss: 3.6134\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 401/401\n",
      "474/474 [==============================] - 27s 58ms/step - loss: 3.5801 - val_loss: 3.5725\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 421/421\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5932 - val_loss: 3.5412\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 441/441\n",
      "474/474 [==============================] - 27s 58ms/step - loss: 3.6031 - val_loss: 3.6218\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 461/461\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5998 - val_loss: 3.5865\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 481/481\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5975 - val_loss: 3.6540\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 501/501\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5978 - val_loss: 3.7236\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 521/521\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6048 - val_loss: 3.5890\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 541/541\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5997 - val_loss: 3.6933\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 561/561\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.5924 - val_loss: 3.6452\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 581/581\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6284 - val_loss: 3.7477\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 601/601\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6062 - val_loss: 3.6321\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 621/621\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5805 - val_loss: 3.6012\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 641/641\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6208 - val_loss: 3.6565\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 661/661\n",
      "475/475 [==============================] - 27s 56ms/step - loss: 3.6188 - val_loss: 3.6265\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 681/681\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5705 - val_loss: 3.6103\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 701/701\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.6060 - val_loss: 3.6187\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 721/721\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5873 - val_loss: 3.5250\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 741/741\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5958 - val_loss: 3.5915\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 761/761\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6190 - val_loss: 3.5706\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 781/781\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6486 - val_loss: 3.5618\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "**********************************\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 801/801\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6015 - val_loss: 3.6046\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 821/821\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6351 - val_loss: 3.7040\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 841/841\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6162 - val_loss: 3.6402\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 861/861\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.5676 - val_loss: 3.6287\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 881/881\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6206 - val_loss: 3.7233\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 901/901\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6104 - val_loss: 3.5903\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 921/921\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6519 - val_loss: 3.6918\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 941/941\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6891 - val_loss: 3.6505\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 961/961\n",
      "474/474 [==============================] - 28s 59ms/step - loss: 3.6217 - val_loss: 3.5940\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 981/981\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.6522 - val_loss: 3.5978\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1001/1001\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5875 - val_loss: 3.6218\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1021/1021\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.6257 - val_loss: 3.6599\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1041/1041\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5618 - val_loss: 3.5578\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1061/1061\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5736 - val_loss: 3.6534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1081/1081\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5532 - val_loss: 3.5237\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1101/1101\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6338 - val_loss: 3.5831\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1121/1121\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5532 - val_loss: 3.6618\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1141/1141\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.6083 - val_loss: 3.5493\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1161/1161\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.5780 - val_loss: 3.5806\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1181/1181\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.6032 - val_loss: 3.6134\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 1201/1201\n",
      "474/474 [==============================] - 28s 59ms/step - loss: 3.5801 - val_loss: 3.5725\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1221/1221\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5932 - val_loss: 3.5412\n",
      "Train on 474 samples, validate on 25 samples\n",
      "Epoch 1241/1241\n",
      "474/474 [==============================] - 27s 58ms/step - loss: 3.6031 - val_loss: 3.6218\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1261/1261\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5998 - val_loss: 3.5865\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1281/1281\n",
      "475/475 [==============================] - 28s 59ms/step - loss: 3.5975 - val_loss: 3.6540\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1301/1301\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5978 - val_loss: 3.7236\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1321/1321\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.6048 - val_loss: 3.5890\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1341/1341\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.5997 - val_loss: 3.6933\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1361/1361\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5924 - val_loss: 3.6452\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1381/1381\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6284 - val_loss: 3.7477\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1401/1401\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6062 - val_loss: 3.6321\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1421/1421\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5805 - val_loss: 3.6012\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1441/1441\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6208 - val_loss: 3.6565\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1461/1461\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6188 - val_loss: 3.6265\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1481/1481\n",
      "475/475 [==============================] - 28s 58ms/step - loss: 3.5705 - val_loss: 3.6103\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1501/1501\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6060 - val_loss: 3.6187\n",
      "\n",
      "\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1521/1521\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5873 - val_loss: 3.5250\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1541/1541\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.5958 - val_loss: 3.5915\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1561/1561\n",
      "475/475 [==============================] - 27s 57ms/step - loss: 3.6190 - val_loss: 3.5706\n",
      "Train on 475 samples, validate on 25 samples\n",
      "Epoch 1581/1581\n",
      "475/475 [==============================] - 27s 58ms/step - loss: 3.6486 - val_loss: 3.5618\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "n=0\n",
    "e=1\n",
    "for d in range(2):\n",
    "    start=1\n",
    "    print('Epoch: '+str(d)+'\\n'+\"**********************************\\n\")\n",
    "    for i in range(800):\n",
    "        vectors_train = np.load('D:/112/decoder/data/data3000/news_vectors'+str(999-i)+'.npy')\n",
    "        #print(start)\n",
    "        texts = textreader(start,vectors_train.shape[0])\n",
    "        start+=vectors_train.shape[0]\n",
    "        #print(len(texts))\n",
    "\n",
    "        #for j in range(5):\n",
    "        tt, kol  = word_vectors_creator(texts)\n",
    "        #vtt = vectors_train[0+j*100:(1+j)*100]\n",
    "        tt, vectors_train = shuffle(tt, vectors_train, random_state=0)\n",
    "\n",
    "        if n%20==0:\n",
    "            model.fit(\n",
    "                x=vectors_train,\n",
    "                y =tt,\n",
    "                epochs=e, initial_epoch=n,\n",
    "                validation_split=0.05,\n",
    "                verbose=1,\n",
    "                batch_size=8\n",
    "            )\n",
    "            if n%500==0:\n",
    "                model.save('D:/112/decoder/models/conv2_mse_sen_to_word'+str(d)+'_'+str(i)+'.h5')\n",
    "                print('\\n')\n",
    "        else:\n",
    "            model.fit(\n",
    "                x=vectors_train,\n",
    "                y =tt,\n",
    "                epochs=e, initial_epoch=n,\n",
    "                validation_split=0.05,\n",
    "                verbose=0,\n",
    "                batch_size=8\n",
    "            )\n",
    "        n+=1\n",
    "        e+=1\n",
    "    model.save('D:/112/decoder/models/conv2_mse_sen_to_word'+str(d)+'.h5')\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
